{"meta":{"version":1,"warehouse":"4.0.2"},"models":{"Asset":[{"_id":"themes/butterfly/source/css/background.css","path":"css/background.css","modified":0,"renderable":1},{"_id":"themes/butterfly/source/css/var.styl","path":"css/var.styl","modified":0,"renderable":1},{"_id":"themes/butterfly/source/css/index.styl","path":"css/index.styl","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/404.jpg","path":"img/404.jpg","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/archive_img.jpg","path":"img/archive_img.jpg","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/avater.png","path":"img/avater.png","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/favicon.png","path":"img/favicon.png","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/favicon1.png","path":"img/favicon1.png","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/friend_404.gif","path":"img/friend_404.gif","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/index_img1.jpg","path":"img/index_img1.jpg","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/index_img.jpg","path":"img/index_img.jpg","modified":0,"renderable":1},{"_id":"themes/butterfly/source/js/main.js","path":"js/main.js","modified":0,"renderable":1},{"_id":"themes/butterfly/source/js/tw_cn.js","path":"js/tw_cn.js","modified":0,"renderable":1},{"_id":"themes/butterfly/source/js/utils.js","path":"js/utils.js","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/covers/1.jpg","path":"img/covers/1.jpg","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/covers/11.jpg","path":"img/covers/11.jpg","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/covers/10.jpg","path":"img/covers/10.jpg","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/covers/2.jpg","path":"img/covers/2.jpg","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/covers/4.jpg","path":"img/covers/4.jpg","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/covers/5.jpg","path":"img/covers/5.jpg","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/covers/6.jpg","path":"img/covers/6.jpg","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/covers/3.jpg","path":"img/covers/3.jpg","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/covers/7.jpg","path":"img/covers/7.jpg","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/covers/8.jpg","path":"img/covers/8.jpg","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/covers/9.jpg","path":"img/covers/9.jpg","modified":0,"renderable":1},{"_id":"themes/butterfly/source/js/search/local-search.js","path":"js/search/local-search.js","modified":0,"renderable":1},{"_id":"themes/butterfly/source/js/search/algolia.js","path":"js/search/algolia.js","modified":0,"renderable":1}],"Cache":[{"_id":"source/Music/index.md","hash":"6289a5601782729fc1243fad854af9fa9488645f","modified":1679416167504},{"_id":"source/_drafts/微积分.md","hash":"5ca568eba3843c42b2cc816dc4425227acecaa2b","modified":1679472141387},{"_id":"source/_drafts/数据预处理.md","hash":"b452bb29cb25c16aaa569d2036b9fd38b2a219d8","modified":1679471610692},{"_id":"source/_drafts/数据操作.md","hash":"f4068c988fb26ca82917c914ed9ea189d16ecc6a","modified":1679471527194},{"_id":"source/_drafts/线性代数.md","hash":"d45081685ae72759b6a763c2b93aaea9572926d1","modified":1679472824223},{"_id":"source/_drafts/自动微分.md","hash":"7bf4cd386383eb5066584f01319a49d147be865b","modified":1679472545273},{"_id":"source/tags/index.md","hash":"ebc141fe64f57bcf212607dde1cc1ca3ca145737","modified":1679392369760},{"_id":"source/categories/index.md","hash":"a983b40e83f96c6b0f40624fa9a7e2e5999a91ee","modified":1679375364137},{"_id":"source/_drafts/微积分/output_7_0.svg","hash":"94a763b1f112bbd036258b596e3f9cf62441f3db","modified":1679471997615},{"_id":"source/_posts/学习记录/考研/计算机网络.md","hash":"9bc62280e3e441fbb00884ab45557a6e0db2fbc5","modified":1679397227614},{"_id":"source/_posts/学习记录/考研/微机原理及接口技术.md","hash":"2ce52af49dde80db6cc6e1623159260beb643a32","modified":1679397227610},{"_id":"source/_posts/学习记录/DataWhale/.~D2L_Ch01_深度学习的相关概念.md","hash":"f3727dcf2b4422f350b03f4b3b4694b469f3cceb","modified":1679473523957},{"_id":"source/_posts/学习记录/DataWhale/D2L_ch02_预备知识.md","hash":"a42dfe4916bf0fb83fe298809e83df5ccc825589","modified":1679475125435},{"_id":"source/_posts/学习记录/DataWhale/D2L_Ch01_深度学习的相关概念.md","hash":"f3727dcf2b4422f350b03f4b3b4694b469f3cceb","modified":1679473523962},{"_id":"source/_posts/踩坑日记/Tips/7x24-Hours-无人监守直播间.md","hash":"b91d13e07f2a0ecbbb3de270603ee0ff15aed87f","modified":1679397227616},{"_id":"source/_posts/学习记录/DataWhale/DataWhale学习记录.md","hash":"143a3bd424679e051ba126f5aa64e2712932ed83","modified":1679473711949},{"_id":"source/_posts/踩坑日记/环境配置/Windows下D2L-Pytorch环境配置-CPU版.md","hash":"6cb709ab2de4888103a834f14d9d9300602f2d1b","modified":1679473731649},{"_id":"source/_posts/踩坑日记/Tips/Git-Bash秒变Linux.md","hash":"919c811db1623bc52ce739c3cc3fd6927c279389","modified":1679397227620},{"_id":"source/_posts/踩坑日记/Tips/hello-world.md","hash":"cb8fae7a4ca4a15e573cfd166e5a9d36dee133d4","modified":1679398455919},{"_id":"source/_posts/学习记录/考研/微机原理及接口技术/image-20230308232215011-16782889368582.png","hash":"92e9d68096b222dacc409d572e2a00bf3513940d","modified":1678288935000},{"_id":"source/_posts/学习记录/考研/微机原理及接口技术/image-20230308232215011.png","hash":"92e9d68096b222dacc409d572e2a00bf3513940d","modified":1678288935000},{"_id":"source/_posts/学习记录/考研/微机原理及接口技术/image-20230308223135359.png","hash":"b41a3a08fa315cc9568144e0d8b7dfd60d97f104","modified":1678286050000},{"_id":"source/_posts/学习记录/考研/微机原理及接口技术/微机原理及接口技术.jpg","hash":"faac49fb751999f721a318a563a4cad1413a858f","modified":1679234568350},{"_id":"source/_posts/学习记录/DataWhale/D2L_Ch01_深度学习的相关概念/image-20230319170611582.png","hash":"f0aeb558b47d6ad6a70fbf94724e31315d9aefb2","modified":1679473356935},{"_id":"source/_posts/学习记录/DataWhale/D2L_Ch01_深度学习的相关概念/image-20230319174247282.png","hash":"40387970af363f240d787edf2468fc343ae47f5c","modified":1679473356935},{"_id":"source/_posts/学习记录/DataWhale/D2L_Ch01_深度学习的相关概念/image-20230319174557369.png","hash":"3d0aa98dad6dc865f7caf71b66e4fe818f3b5b64","modified":1679219157000},{"_id":"source/_posts/学习记录/DataWhale/D2L_ch02_预备知识/output_7_0-16794727634302.svg","hash":"94a763b1f112bbd036258b596e3f9cf62441f3db","modified":1679472763435},{"_id":"source/_posts/学习记录/DataWhale/D2L_ch02_预备知识/output_7_0.svg","hash":"94a763b1f112bbd036258b596e3f9cf62441f3db","modified":1679472758789},{"_id":"source/_posts/学习记录/DataWhale/D2L_Ch01_深度学习的相关概念/image-20230319192839845.png","hash":"fa8b873f2196066f7c7840e3ea7fae99a768677a","modified":1679473356945},{"_id":"source/_posts/踩坑日记/Tips/Git-Bash秒变Linux/image-20230320230913167-16793685665802.png","hash":"614b36d944bad55509c5ce983bfd5099e589cdd0","modified":1679368566587},{"_id":"source/_posts/踩坑日记/Tips/Git-Bash秒变Linux/image-20230320230913167.png","hash":"614b36d944bad55509c5ce983bfd5099e589cdd0","modified":1679368561857},{"_id":"source/_posts/踩坑日记/Tips/Git-Bash秒变Linux/image-20230321164332935-16793882362101.png","hash":"7629f9f4e32023525b02b4c54f1177950b05c229","modified":1679388236221},{"_id":"source/_posts/踩坑日记/Tips/Git-Bash秒变Linux/image-20230321164332935.png","hash":"7629f9f4e32023525b02b4c54f1177950b05c229","modified":1679388212976},{"_id":"source/_posts/学习记录/DataWhale/DataWhale学习记录/深度学习小技巧4.jpg","hash":"4379527940028695566105f491e8c15bde3b8a32","modified":1679148421000},{"_id":"themes/butterfly/README.md","hash":"15e7f6386b4fd3fa4a6c179ddf641e1ae57eafa2","modified":1678084194546},{"_id":"themes/butterfly/LICENSE","hash":"c8bc7df08db9dd3b39c2c2259a163a36cf2f6808","modified":1678084194545},{"_id":"themes/butterfly/README_CN.md","hash":"d0f7a24732855ae340f94e8286085d5387850cde","modified":1678084194546},{"_id":"themes/butterfly/package.json","hash":"861ab8b6d38e2ffb37f224342f1123d5f37b87e0","modified":1678084194664},{"_id":"themes/butterfly/_config.yml","hash":"7549b155880905b348c027a54e7071fc8445042a","modified":1678084194548},{"_id":"themes/butterfly/plugins.yml","hash":"589f1446c5ef64f77bd7bfb466743a8ddb59d2c9","modified":1678084194665},{"_id":"themes/butterfly/layout/category.pug","hash":"bf979aec88d78b644fc5d31518f8679ad7625792","modified":1678084194553},{"_id":"themes/butterfly/layout/archive.pug","hash":"115fa5ee8864e5c97549eff91a17c66101d724ab","modified":1678084194552},{"_id":"themes/butterfly/layout/index.pug","hash":"648dcbdb3d145a710de81c909e000e8664d2ac9c","modified":1678084194660},{"_id":"themes/butterfly/layout/page.pug","hash":"bf2d6c6d2d156777b55292e51be02b0b3acf0af8","modified":1678084194661},{"_id":"themes/butterfly/languages/default.yml","hash":"a62bfe75c32dd0c2754b6fc70df027947ed1e711","modified":1678084194549},{"_id":"themes/butterfly/layout/post.pug","hash":"fdbb508b5e6dec30fb8753c5a7fdd494410c4fc0","modified":1678084194662},{"_id":"themes/butterfly/layout/tag.pug","hash":"4bb5efc6dabdf1626685bf6771aaa1467155ae86","modified":1678084194663},{"_id":"themes/butterfly/languages/en.yml","hash":"9d83a52e3fe3c086eadcdd9bca50829ca9b3a188","modified":1678084194550},{"_id":"themes/butterfly/languages/zh-CN.yml","hash":"c2f1111d0cffddbf04248a7daa55c7895b9e4cf3","modified":1678084194551},{"_id":"themes/butterfly/languages/zh-TW.yml","hash":"30b69c969394392e4bdd3763873ceed0d473cfcb","modified":1678084194551},{"_id":"themes/butterfly/.github/ISSUE_TEMPLATE/bug_report.yml","hash":"92683cb785a29e5a73701b33dba73d3afac268c3","modified":1678084194540},{"_id":"themes/butterfly/.github/ISSUE_TEMPLATE/config.yml","hash":"d9ff4bad9c9664e12729c7a531169a05698a3fd1","modified":1678084194540},{"_id":"themes/butterfly/.github/ISSUE_TEMPLATE/feature_request.yml","hash":"6e0f9470b18bd37d4891282ac73d61676b040e8c","modified":1678084194541},{"_id":"themes/butterfly/.github/workflows/publish.yml","hash":"e320b40c051bae1549156cd5ea4a51383cf78598","modified":1678084194543},{"_id":"themes/butterfly/.github/workflows/stale.yml","hash":"4040c76547e270aaf184e9b219a44ca41bbb1b9f","modified":1678084194544},{"_id":"themes/butterfly/layout/includes/additional-js.pug","hash":"28b425b486c009e043169c1532a2b62150613c2c","modified":1678084194555},{"_id":"themes/butterfly/layout/includes/404.pug","hash":"aace9ddff469de4226e47a52ede1c81e66d66d5c","modified":1678084194555},{"_id":"themes/butterfly/layout/includes/footer.pug","hash":"8715948b93e7508b84d913be1969b28c6b067b9b","modified":1678084194556},{"_id":"themes/butterfly/layout/includes/head.pug","hash":"ab32b8d4ef682bd1d9b727b9d25a19dc06f3e68e","modified":1678084194556},{"_id":"themes/butterfly/layout/includes/layout.pug","hash":"ab3ff72ecaa7da09fcb7d03a8b341a061e870826","modified":1678084194572},{"_id":"themes/butterfly/layout/includes/pagination.pug","hash":"c5c58714fb3cb839653e5c32e6094784c8662935","modified":1678084194583},{"_id":"themes/butterfly/layout/includes/rightside.pug","hash":"5f96a28a4eac046b68ccc10e8c24ab01525e3245","modified":1678084194586},{"_id":"themes/butterfly/layout/includes/sidebar.pug","hash":"4f41fc46410e1e3018ff87e1d1a5c28be7258119","modified":1678084194587},{"_id":"themes/butterfly/scripts/events/cdn.js","hash":"39a2f0088841abb89b3b801469266cc9568fced0","modified":1678084194669},{"_id":"themes/butterfly/scripts/events/404.js","hash":"f1d1c378356b776e9b2a8411e6dca88dc8c3245c","modified":1678084194668},{"_id":"themes/butterfly/scripts/events/comment.js","hash":"95479790234c291b064d031577d71214cdd1d820","modified":1678084194670},{"_id":"themes/butterfly/scripts/events/init.js","hash":"72605a00bf623b6cbc9fa0f90069ea2ae584c5c0","modified":1678084194671},{"_id":"themes/butterfly/scripts/events/stylus.js","hash":"218add7e9b39b6fb6e69921abb9e44891a6cc3ce","modified":1678084194672},{"_id":"themes/butterfly/scripts/events/welcome.js","hash":"f59e10305fef59ea3e62a7395106c0927582879d","modified":1678084194673},{"_id":"themes/butterfly/scripts/helpers/aside_archives.js","hash":"4f712b4ea383b59a3122683db1d54c04a79ccc5d","modified":1678084194678},{"_id":"themes/butterfly/scripts/helpers/aside_categories.js","hash":"376e1884ea764404c38b1e73b16de0358ece519e","modified":1678084194679},{"_id":"themes/butterfly/scripts/helpers/findArchiveLength.js","hash":"8ff03b35385f9162e1eb54dceb5fa7f781306caf","modified":1678084194680},{"_id":"themes/butterfly/scripts/helpers/inject_head_js.js","hash":"688b424e4fae8db268033dd1f78b999932b22e57","modified":1678084194681},{"_id":"themes/butterfly/scripts/helpers/page.js","hash":"3609660803ee8ff6b662d9326b34cd1ab65ae4d7","modified":1678084194681},{"_id":"themes/butterfly/scripts/helpers/related_post.js","hash":"76343ac8422c9c8539082e77eda6ffee4b877eb2","modified":1678084194683},{"_id":"themes/butterfly/scripts/filters/post_lazyload.js","hash":"5fd6c9659262dc8f61d87866d0417fd534292c88","modified":1678084194675},{"_id":"themes/butterfly/scripts/filters/random_cover.js","hash":"d33f0a055bacaa96ee5b69f821292d87658f4b0c","modified":1678084194677},{"_id":"themes/butterfly/scripts/tag/flink.js","hash":"3ba7677969ff01fab06fc6713455ddc6861f0024","modified":1678084194685},{"_id":"themes/butterfly/scripts/tag/button.js","hash":"44cca49ddc76921bb455465ef912cab46c993cef","modified":1678084194684},{"_id":"themes/butterfly/scripts/tag/gallery.js","hash":"3267d4a7fe849b8e1b4d338670cefc502721e247","modified":1678084194686},{"_id":"themes/butterfly/scripts/tag/hide.js","hash":"e01a3967e5884881bab858b11635457df412de80","modified":1678084194687},{"_id":"themes/butterfly/scripts/tag/inlineImg.js","hash":"7641adb0d520c5ff29dd36fc1fb8617c52ecc9fb","modified":1678084194688},{"_id":"themes/butterfly/scripts/tag/label.js","hash":"551f1b8edc973bd8afc5cce2eae546f002fa84c3","modified":1678084194689},{"_id":"themes/butterfly/scripts/tag/mermaid.js","hash":"fd683ccc090db3122d77c7ee73e8d35dc8735ee3","modified":1678084194690},{"_id":"themes/butterfly/scripts/tag/note.js","hash":"56a5d41487d74654b75305c5325167a116495900","modified":1678084194691},{"_id":"themes/butterfly/scripts/tag/tabs.js","hash":"08ea00791bd4738952234cb5d8360e119df6f875","modified":1678084194693},{"_id":"themes/butterfly/scripts/tag/timeline.js","hash":"4c7056d3cd56f10bd209d2ba4d3cc2027aad1440","modified":1678084194694},{"_id":"themes/butterfly/source/css/background.css","hash":"1d2de6b7b1d86ee3dd711da87e0bc1b1fd848b0b","modified":1679405329821},{"_id":"themes/butterfly/source/css/var.styl","hash":"e1e37a2e932163886789b72624c7348545003f1d","modified":1678084194739},{"_id":"themes/butterfly/source/css/index.styl","hash":"c7924868adcb046b46498626a9223c7a7b3f2e30","modified":1678084194738},{"_id":"themes/butterfly/source/img/404.jpg","hash":"fb4489bc1d30c93d28f7332158c1c6c1416148de","modified":1678084194741},{"_id":"themes/butterfly/source/img/archive_img.jpg","hash":"e986c094e95dcb3360b0b0c5a0e307ba5fd0f7e4","modified":1652626926181},{"_id":"themes/butterfly/source/img/favicon.png","hash":"fc65e76fdb18ff75038f60c0ba50dcbec38c43ef","modified":1679309550144},{"_id":"themes/butterfly/source/img/favicon1.png","hash":"3cf89864b4f6c9b532522a4d260a2e887971c92d","modified":1678084194742},{"_id":"themes/butterfly/source/img/friend_404.gif","hash":"8d2d0ebef70a8eb07329f57e645889b0e420fa48","modified":1678084194744},{"_id":"themes/butterfly/source/js/main.js","hash":"ab1eb2ae2fc7937aa0579d5293432fa986bf49b2","modified":1678084194746},{"_id":"themes/butterfly/source/js/tw_cn.js","hash":"c4dac81869b33fa1590fae60a45b2e915a59c3ec","modified":1678084194749},{"_id":"themes/butterfly/source/js/utils.js","hash":"002c21bdbf1c3f4e02d86ff6927d47fd18c09730","modified":1678084194749},{"_id":"themes/butterfly/layout/includes/head/Open_Graph.pug","hash":"c8dbdfe6145a0bc6f7691c9551be8169a2698f0a","modified":1678084194557},{"_id":"themes/butterfly/layout/includes/head/analytics.pug","hash":"c7666a10448edd93f5ace37296051b7670495f1b","modified":1678084194559},{"_id":"themes/butterfly/layout/includes/head/config_site.pug","hash":"bd5dd5452e28a4fe94c3241a758ec6f4fdb7a149","modified":1678084194561},{"_id":"themes/butterfly/layout/includes/head/config.pug","hash":"f4b80b4a3ff5dd877b447b42d078f01ee3ecab24","modified":1678084194560},{"_id":"themes/butterfly/layout/includes/head/google_adsense.pug","hash":"f29123e603cbbcc6ce277d4e8f600ba67498077c","modified":1678084194562},{"_id":"themes/butterfly/layout/includes/head/preconnect.pug","hash":"fd55f0e09827ef40c53c48d184a553464aac3e03","modified":1678084194564},{"_id":"themes/butterfly/layout/includes/head/pwa.pug","hash":"6dc2c9b85df9ab4f5b554305339fd80a90a6cf43","modified":1678084194565},{"_id":"themes/butterfly/layout/includes/head/noscript.pug","hash":"72efaa09ff60843567458bd54152e06f0cb2757e","modified":1678084194563},{"_id":"themes/butterfly/layout/includes/header/menu_item.pug","hash":"ca8bcd90ad9467819330bfe7c02b76322754bccf","modified":1678084194568},{"_id":"themes/butterfly/layout/includes/head/site_verification.pug","hash":"5168caadc4cf541f5d6676a9c5e8ae47a948f9ad","modified":1678084194565},{"_id":"themes/butterfly/layout/includes/header/index.pug","hash":"e9f952f1b66a0116fccb812b14db0f229666e586","modified":1678084194567},{"_id":"themes/butterfly/layout/includes/header/nav.pug","hash":"962ee70a35e60a13c31eea47d16b9f98069fe417","modified":1678084194568},{"_id":"themes/butterfly/layout/includes/header/post-info.pug","hash":"50b4b9c7f6a4771910be4198639cfcfc6d7b8925","modified":1678084194570},{"_id":"themes/butterfly/layout/includes/header/social.pug","hash":"631ec7000fd4d6cfa2de118ee02ad8a42ffb34f5","modified":1678084194570},{"_id":"themes/butterfly/layout/includes/loading/fullpage-loading.pug","hash":"767ca9b4dad126ba5fbd2f3ea5ca8d07c0d2b32c","modified":1678084194573},{"_id":"themes/butterfly/layout/includes/loading/pace.pug","hash":"39847b8a1f0fad7889954a4ac936b7681949f16c","modified":1678084194575},{"_id":"themes/butterfly/layout/includes/page/categories.pug","hash":"5276a8d2835e05bd535fedc9f593a0ce8c3e8437","modified":1678084194580},{"_id":"themes/butterfly/layout/includes/page/default-page.pug","hash":"e9459f122af7b733398578f9f0f8ab3c5e12a217","modified":1678084194581},{"_id":"themes/butterfly/layout/includes/loading/index.pug","hash":"00ae419f527d8225a2dc03d4f977cec737248423","modified":1678084194574},{"_id":"themes/butterfly/layout/includes/page/flink.pug","hash":"a60d9cc9083142e3d92b618561abd557fcab502b","modified":1678084194581},{"_id":"themes/butterfly/layout/includes/page/tags.pug","hash":"12be059c536490af216a397e8f2a7abbf6d4610e","modified":1678084194582},{"_id":"themes/butterfly/layout/includes/mixins/article-sort.pug","hash":"9155f01d4c644a2e19b2b13b2d3c6d5e34dd0abf","modified":1678084194577},{"_id":"themes/butterfly/layout/includes/mixins/post-ui.pug","hash":"39c205027296dbd9fc9eb3cfd9d033bc5bd031b7","modified":1678084194578},{"_id":"themes/butterfly/layout/includes/post/post-copyright.pug","hash":"cc1f7ae8a7ce5445277215821092e712ec8cc296","modified":1678084194584},{"_id":"themes/butterfly/layout/includes/post/reward.pug","hash":"594626a18b7efbf771232855dfbce143fb244bc6","modified":1678084194585},{"_id":"themes/butterfly/layout/includes/third-party/aplayer.pug","hash":"e939344fd389aeb11864ee697d5fd9b036d8325f","modified":1678084194588},{"_id":"themes/butterfly/layout/includes/third-party/pangu.pug","hash":"f0898509da70388b5c532f19e762756d74080200","modified":1678084194634},{"_id":"themes/butterfly/layout/includes/third-party/effect.pug","hash":"4e37535c63149708ecbedb262336014524ad8723","modified":1678084194619},{"_id":"themes/butterfly/layout/includes/third-party/pjax(backup).pug","hash":"0c1da3564cef7b60b2b8e2ad2aeb2256b7cbbc66","modified":1679412964978},{"_id":"themes/butterfly/layout/includes/third-party/pjax.pug","hash":"7c0c43f5394cb999588e83edb7edd212d587a327","modified":1679482502574},{"_id":"themes/butterfly/layout/includes/third-party/prismjs.pug","hash":"08979afbfecb4476a5ae8e360947b92624d285b8","modified":1678084194636},{"_id":"themes/butterfly/layout/includes/third-party/subtitle.pug","hash":"d4836a2ce1c87a59cfa8e4986346bf353710277f","modified":1678084194646},{"_id":"themes/butterfly/layout/includes/widget/card_ad.pug","hash":"a8312b527493dabbadbb1280760168d3bc909a3b","modified":1678084194647},{"_id":"themes/butterfly/layout/includes/widget/card_announcement.pug","hash":"21e019bdc3b1e796bb00976bb29af2d51f873624","modified":1678084194648},{"_id":"themes/butterfly/layout/includes/widget/card_author.pug","hash":"08641633d38903351a7424baf9893d9038ba057d","modified":1678084194650},{"_id":"themes/butterfly/layout/includes/widget/card_archives.pug","hash":"73d33b6930e7944187a4b3403daf25d27077a2dd","modified":1678084194649},{"_id":"themes/butterfly/layout/includes/widget/card_bottom_self.pug","hash":"1dba77d250eeebfb6e293d504352c7e9ea31980b","modified":1678084194651},{"_id":"themes/butterfly/layout/includes/widget/card_categories.pug","hash":"66e383b4ef374951eb87dd1bf4cdb7a667193fb5","modified":1678084194652},{"_id":"themes/butterfly/layout/includes/widget/card_newest_comment.pug","hash":"c02b1779bd0ebca6749f195be096b6ca574bfa29","modified":1678084194653},{"_id":"themes/butterfly/layout/includes/widget/card_post_toc.pug","hash":"59d979702fa21d960443824198614d63aaf69662","modified":1678084194653},{"_id":"themes/butterfly/layout/includes/widget/card_recent_post.pug","hash":"bb842d2aa6469d65bf06af1372f0a19a9e4ef44c","modified":1678084194654},{"_id":"themes/butterfly/layout/includes/widget/card_tags.pug","hash":"8a8aa922be1824df17f30ba08ee6b55e2af05787","modified":1678084194655},{"_id":"themes/butterfly/layout/includes/widget/card_top_self.pug","hash":"7b5ae404a1205546b7de4be42291315cf918f2b3","modified":1678084194656},{"_id":"themes/butterfly/layout/includes/widget/card_webinfo - 副本.pug","hash":"0d39134669240195a5bed26e1d0aff364f8ae507","modified":1679401981154},{"_id":"themes/butterfly/layout/includes/widget/card_webinfo.pug","hash":"155161dcfe5a4e043cbb3b8376d3bca6c2267249","modified":1679401975135},{"_id":"themes/butterfly/layout/includes/widget/index.pug","hash":"ba94ceb40d7c81372a76103d958dcc2cc6a84f00","modified":1678084194659},{"_id":"themes/butterfly/source/css/_global/index.styl","hash":"fde51780f525b002885c9f189cccc459542f5dc6","modified":1678084194697},{"_id":"themes/butterfly/source/css/_global/function.styl","hash":"6a7a926b72c5083aa14051a0eca8d49e1c6261f1","modified":1678084194696},{"_id":"themes/butterfly/source/css/_highlight/highlight.styl","hash":"a2aa5caf338ff5323b6ff0601ebcc09e710d8398","modified":1678084194698},{"_id":"themes/butterfly/source/css/_highlight/theme.styl","hash":"3c178608406c31d768af355ef1d7326da37cc75f","modified":1678084194704},{"_id":"themes/butterfly/source/css/_layout/aside.styl","hash":"e6107800576bf465dad87494758f6694f0bd27d4","modified":1678084194705},{"_id":"themes/butterfly/source/css/_layout/chat.styl","hash":"792a04d36de32f230ca3256ad87a90fe8392f333","modified":1678084194706},{"_id":"themes/butterfly/source/css/_layout/comments.styl","hash":"0abe05309a186682772a94e5e759b63f8028e61a","modified":1678084194706},{"_id":"themes/butterfly/source/css/_layout/footer.styl","hash":"077ce6932261cf51696834a25d1d988fec6cd7d0","modified":1678084194707},{"_id":"themes/butterfly/source/css/_layout/head.styl","hash":"d33f2d9001a82d9a72d6f4088743762fbd016e4c","modified":1678084194710},{"_id":"themes/butterfly/source/css/_layout/loading.styl","hash":"f0b01bbf321c2c24fdccaee367dd9fd448031a72","modified":1678084194711},{"_id":"themes/butterfly/source/css/_layout/post.styl","hash":"b0ba6f72848bc711bcf534c56b04ae14ab21b320","modified":1678084194713},{"_id":"themes/butterfly/source/css/_layout/pagination.styl","hash":"bd099f7d3adef4b7edd24c0a25a07415b156e587","modified":1678084194712},{"_id":"themes/butterfly/source/css/_layout/relatedposts.styl","hash":"6dcf19c0933c8828a439f801b0f4b256447dec07","modified":1678084194714},{"_id":"themes/butterfly/source/css/_layout/reward.styl","hash":"b5ba2c3339ad406ce611d12d3f8cc84f864fbc03","modified":1678084194715},{"_id":"themes/butterfly/source/css/_layout/rightside.styl","hash":"824b2b2d2ee72583b9e009c0bb13e29e806534cc","modified":1678084194716},{"_id":"themes/butterfly/source/css/_layout/sidebar.styl","hash":"7e9b65dcae7ac54e0183bc841fea0f4bd4d78e5c","modified":1678084194716},{"_id":"themes/butterfly/source/css/_layout/third-party.styl","hash":"956ed11cc5d65941b49192b9be47cecfaed57a91","modified":1678084194717},{"_id":"themes/butterfly/source/css/_mode/darkmode.styl","hash":"01ba993e7bd384827c0f5499ae0c5f82e068d844","modified":1678084194719},{"_id":"themes/butterfly/source/css/_mode/readmode.styl","hash":"d072872714854cdc3d2975fb131ac31cb82b13e3","modified":1678084194720},{"_id":"themes/butterfly/source/css/_page/404.styl","hash":"a7223a8fcc4fa7b81e552c9a2554be7df9de312e","modified":1678084194721},{"_id":"themes/butterfly/source/css/_page/categories.styl","hash":"68bc8cbea25dbb3cdc170f09f9b43ce130547717","modified":1678084194723},{"_id":"themes/butterfly/source/css/_page/archives.styl","hash":"5dd1ba997741d02894ff846eda939ad8051c0bb2","modified":1678084194722},{"_id":"themes/butterfly/source/css/_page/common.styl","hash":"a714776e3d585369f2285b6bb4e1564539c58d8b","modified":1678084194724},{"_id":"themes/butterfly/source/css/_page/tags.styl","hash":"9e35f91847773b915c74a78b8aa66c7bdb950ad0","modified":1678084194727},{"_id":"themes/butterfly/source/css/_page/flink.styl","hash":"ecc2b2e28c179eb9406fc2c6f00e141078249cdd","modified":1678084194725},{"_id":"themes/butterfly/source/css/_page/homepage.styl","hash":"d83389e07f8851f1b9afafe0eacd1bf10ad334ef","modified":1678084194726},{"_id":"themes/butterfly/source/css/_search/algolia.styl","hash":"85f5f2c9150061bae8a5188c10f23a63e7e01a4f","modified":1678084194728},{"_id":"themes/butterfly/source/css/_search/local-search.styl","hash":"f7d3d598a27467d24b0b37f49c012b5759394c0a","modified":1678084194729},{"_id":"themes/butterfly/source/css/_tags/button.styl","hash":"62da1de0d5b8453fcecbfacddb16985265638ba5","modified":1678084194731},{"_id":"themes/butterfly/source/css/_search/index.styl","hash":"cced94e70b4b90130cfa215582be2adbf883efba","modified":1678084194729},{"_id":"themes/butterfly/source/css/_third-party/normalize.min.css","hash":"8549829fb7d3c21cd9e119884962e8c463a4a267","modified":1678084194738},{"_id":"themes/butterfly/source/css/_tags/gallery.styl","hash":"7c463ba25b3b54f0f46dfd75971d792816e942a8","modified":1678084194731},{"_id":"themes/butterfly/source/css/_tags/hexo.styl","hash":"985b183db7b7bfd8f9bdb60494549fb7f850348b","modified":1678084194732},{"_id":"themes/butterfly/source/css/_tags/hide.styl","hash":"b7cf7753479fcf2fe07287ffdb0e568adbba4c18","modified":1678084194733},{"_id":"themes/butterfly/source/css/_tags/label.styl","hash":"2f83bd145b870d80d4b18b0ac603235229a5694e","modified":1678084194734},{"_id":"themes/butterfly/source/css/_tags/inlineImg.styl","hash":"5a873d01fabebcf7ddf7a6b1c2e2e5e2714097f4","modified":1678084194733},{"_id":"themes/butterfly/source/css/_tags/tabs.styl","hash":"ec81ea316c82b83d6aee31e52f248d329559d5d3","modified":1678084194735},{"_id":"themes/butterfly/source/css/_tags/note.styl","hash":"331c89ecfb79fab68466944a43e9e3d0ff49c646","modified":1678084194735},{"_id":"themes/butterfly/source/css/_tags/timeline.styl","hash":"07ea7134db7a66c87658116f089fb1a2a6906563","modified":1678084194736},{"_id":"themes/butterfly/source/js/search/local-search.js","hash":"1ada177b1635bd7d1b6f02b33f7a81823d8bce0c","modified":1678084194748},{"_id":"themes/butterfly/source/js/search/algolia.js","hash":"73027a5e8b49d62b5b5a4d51663d5189f8eb7b4c","modified":1678084194747},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/artalk.pug","hash":"645087801624a73cbee63269cfd399e9da39c663","modified":1678084194589},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/disqus.pug","hash":"f4d21dcbc3b00eed9b1f604e132c4c6811a0a059","modified":1678084194590},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/index.pug","hash":"846cabae287ae31b3bbfac3da022475713dd5ecc","modified":1678084194593},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/fb.pug","hash":"aff5c34b6061b281c66a986cf017e9021dc11ac6","modified":1678084194591},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/remark42.pug","hash":"716dc463fe4ef5112e7018ed60804125fdfa5cad","modified":1678084194594},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/twikoo.pug","hash":"38b85d216d9377ddbaa2e5867e2f03805227237c","modified":1678084194595},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/valine.pug","hash":"cd4fc9c5a61608a5dedf645c1295430a1623040f","modified":1678084194596},{"_id":"themes/butterfly/layout/includes/third-party/chat/daovoice.pug","hash":"e5af55cdb87d1ffd3d8702bc77097159acf95b54","modified":1678084194600},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/waline.pug","hash":"3f59333064c4fc754c638f13a417b7be2569ca09","modified":1678084194597},{"_id":"themes/butterfly/layout/includes/third-party/chat/crisp.pug","hash":"b741b5e942481d779a8a1fe94c45154a62a6b748","modified":1678084194599},{"_id":"themes/butterfly/layout/includes/third-party/chat/chatra.pug","hash":"f3f6eaecbcf9352342e259f4a5a3ad7160f31fc9","modified":1678084194598},{"_id":"themes/butterfly/layout/includes/third-party/chat/gitter.pug","hash":"794ce3911f17d354b7196deb8c36d191afac63fb","modified":1678084194600},{"_id":"themes/butterfly/layout/includes/third-party/chat/messenger.pug","hash":"e703319ae5395273e81de11ba2366f21a104cf47","modified":1678084194603},{"_id":"themes/butterfly/layout/includes/third-party/chat/index.pug","hash":"a5ddcf84ce8855c7801289270009e29c3a103150","modified":1678084194602},{"_id":"themes/butterfly/layout/includes/third-party/chat/tidio.pug","hash":"cd7ab4a776be93eea96a6f6fd0a547977fbe1ea3","modified":1678084194604},{"_id":"themes/butterfly/layout/includes/third-party/comments/disqusjs.pug","hash":"ca1337586aafc85e7f6d730ed0a1984a1a12aba7","modified":1678084194608},{"_id":"themes/butterfly/layout/includes/third-party/comments/facebook_comments.pug","hash":"fe599836225b835bd19fe1a6831fb4d41bcec071","modified":1678084194609},{"_id":"themes/butterfly/layout/includes/third-party/comments/artalk.pug","hash":"d6a2a828689d619943838d42f2c3a45551e2c6c4","modified":1678084194606},{"_id":"themes/butterfly/layout/includes/third-party/comments/disqus.pug","hash":"3b551ab7618b36795480015b5cf565288df5b957","modified":1678084194607},{"_id":"themes/butterfly/layout/includes/third-party/comments/giscus.pug","hash":"319463fc8ff993b798f2293b659b522ad7770cf0","modified":1678084194611},{"_id":"themes/butterfly/layout/includes/third-party/comments/index.pug","hash":"5b44b4e5f2445260811de81edd3eeeb97d8bb583","modified":1678084194613},{"_id":"themes/butterfly/layout/includes/third-party/comments/gitalk.pug","hash":"77729ecfae68ad21ff1d22ff2c8aa212f4d405c6","modified":1678084194612},{"_id":"themes/butterfly/layout/includes/third-party/comments/livere.pug","hash":"589f8503f264d4fda971c8daf2028f45c4f2867b","modified":1678084194615},{"_id":"themes/butterfly/layout/includes/third-party/comments/js.pug","hash":"3abbaaa4ea575c45b3cebffd40bad1acc6ffce84","modified":1678084194614},{"_id":"themes/butterfly/layout/includes/third-party/comments/remark42.pug","hash":"f04263a3bd7efb7f1b250cfee112e82f49805492","modified":1678084194616},{"_id":"themes/butterfly/layout/includes/third-party/comments/twikoo.pug","hash":"ccb5c3f2a821b87986998595743387d7c997c16e","modified":1678084194617},{"_id":"themes/butterfly/layout/includes/third-party/comments/utterances.pug","hash":"d48d59ebf8c0142fb3c4592a0d35874f85e6fd4c","modified":1678084194617},{"_id":"themes/butterfly/layout/includes/third-party/comments/valine.pug","hash":"cba55cbbd0962bf84b8956195e686b0e158ed247","modified":1678084194618},{"_id":"themes/butterfly/layout/includes/third-party/comments/waline.pug","hash":"bf6b1453096dd7f53d81cde078c233d301e14304","modified":1678084194619},{"_id":"themes/butterfly/layout/includes/third-party/math/index.pug","hash":"2afa4c21dd19890f47fb568cfb0d90efb676a253","modified":1678084194621},{"_id":"themes/butterfly/layout/includes/third-party/math/katex.pug","hash":"f0d3eddd2bed68e5517274b3530bfe0fa5057d8e","modified":1678084194622},{"_id":"themes/butterfly/layout/includes/third-party/math/mathjax.pug","hash":"fad70676346f194f166ac4e714ac6ff37cbf50ea","modified":1678084194623},{"_id":"themes/butterfly/layout/includes/third-party/math/mermaid.pug","hash":"297d34d83e7bff8ec3b8bc19bb0f4d901c35fe5a","modified":1678084194623},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/disqus-comment.pug","hash":"cd1ce86441dc508e4c3dbf8b829046455ba8a6b4","modified":1678084194627},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/github-issues.pug","hash":"b2ede1f6b41026ebd233ac076a405889a6eec76b","modified":1678084194628},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/index.pug","hash":"f8b65460c399973090c1fb7ab81e3708c252e7cc","modified":1678084194629},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/artalk.pug","hash":"75ebfe533dd7ee59113b84c2972694cadd92c6a9","modified":1678084194626},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/twikoo-comment.pug","hash":"91a72e94743befa21a7b1c557fbb3751efb87ab0","modified":1678084194631},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/remark42.pug","hash":"bc207dcb771fc2c2c329f29d01708ff6b18443da","modified":1678084194630},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/valine.pug","hash":"33368c0b80e4c4e78f3c7ee9bec0fed70ad838ca","modified":1678084194632},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/waline.pug","hash":"f434dce30f8faee0db5a8e4d2c81196b0e3605f4","modified":1678084194634},{"_id":"themes/butterfly/layout/includes/third-party/share/add-this.pug","hash":"8b4034e53ca5bf85097f681a6e76a53ce685c205","modified":1678084194641},{"_id":"themes/butterfly/layout/includes/third-party/share/addtoany.pug","hash":"1f02a26730e5f36cc2dfec7ff4d5c93a099ed5ba","modified":1678084194643},{"_id":"themes/butterfly/layout/includes/third-party/share/index.pug","hash":"4898a09d8e67fb358ffd74b3a1f0014f555dd856","modified":1678084194644},{"_id":"themes/butterfly/layout/includes/third-party/share/share-js.pug","hash":"8106bd031586f075a994956ee4438eb13be25d7b","modified":1678084194645},{"_id":"themes/butterfly/layout/includes/third-party/search/algolia.pug","hash":"6c90af5e9b4d7f8147f4484f27e1b41e29d5a629","modified":1678084194637},{"_id":"themes/butterfly/layout/includes/third-party/search/docsearch.pug","hash":"ba04174d977da988a1d49c06641262c413352346","modified":1678084194638},{"_id":"themes/butterfly/source/css/_highlight/highlight/diff.styl","hash":"6e77f1ca0cfb0db6b028f5c0238780e66d344f3d","modified":1678084194699},{"_id":"themes/butterfly/layout/includes/third-party/search/local-search.pug","hash":"0785346c1d8c49a10f02481b701138ed5898f4ac","modified":1678084194639},{"_id":"themes/butterfly/source/css/_highlight/highlight/index.styl","hash":"fc702a4614d0562a381907b083f71ba63d301d86","modified":1678084194700},{"_id":"themes/butterfly/source/css/_highlight/prismjs/index.styl","hash":"01ff9e77eb1bd454bec65a6ff5972c8e219bc708","modified":1678084194702},{"_id":"themes/butterfly/layout/includes/third-party/search/index.pug","hash":"3adcf28a8d205ea3ee19828eda0e668702fac07a","modified":1678084194639},{"_id":"themes/butterfly/source/css/_highlight/prismjs/diff.styl","hash":"1309292f1c8c53d96cd7333507b106bcc24ca8fc","modified":1678084194702},{"_id":"themes/butterfly/source/css/_highlight/prismjs/line-number.styl","hash":"0b8aea62d1550113e1fcc237fae1b03743190208","modified":1678084194703},{"_id":"source/_posts/学习记录/DataWhale/D2L_Ch01_深度学习的相关概念/image-20230319212918986.png","hash":"5ab7d037a6129525be1d42270b864f4fa27ca766","modified":1679232559000},{"_id":"source/_posts/踩坑日记/环境配置/Windows下D2L-Pytorch环境配置-CPU版/image-20230320210330101.png","hash":"24702f91df722fee80c43bd8f70cd513ddae2ada","modified":1679319359505},{"_id":"source/_posts/学习记录/DataWhale/DataWhale学习记录/人工智能路线图-16791573386491.jpg","hash":"6d1cc012beb5584051f964023ef24d06d3aba5bb","modified":1679148153000},{"_id":"source/_posts/学习记录/DataWhale/DataWhale学习记录/CV入门-16791573687564.jpg","hash":"fe513be3d0a820213b36a68dcf672c41b101b225","modified":1679148207000},{"_id":"source/_posts/学习记录/DataWhale/DataWhale学习记录/CV入门.jpg","hash":"fe513be3d0a820213b36a68dcf672c41b101b225","modified":1679148207000},{"_id":"source/_posts/学习记录/DataWhale/DataWhale学习记录/人工智能路线图-16791581610352.jpg","hash":"6d1cc012beb5584051f964023ef24d06d3aba5bb","modified":1679148153000},{"_id":"source/_posts/学习记录/DataWhale/DataWhale学习记录/深度学习小技巧.jpg","hash":"89c4705d656a19cc73fd8dbaf41e0e5ec19c5070","modified":1679148409000},{"_id":"source/_posts/学习记录/DataWhale/DataWhale学习记录/人工智能路线图.jpg","hash":"6d1cc012beb5584051f964023ef24d06d3aba5bb","modified":1679148153000},{"_id":"source/_posts/学习记录/DataWhale/DataWhale学习记录/深度学习小技巧-167915739504012.jpg","hash":"89c4705d656a19cc73fd8dbaf41e0e5ec19c5070","modified":1679148409000},{"_id":"source/_posts/学习记录/DataWhale/DataWhale学习记录/深度学习小技巧2.jpg","hash":"bbb692e7852391188d4d7f6ef36ea03942d785f6","modified":1679148413000},{"_id":"source/_posts/学习记录/DataWhale/DataWhale学习记录/深度学习小技巧3-167915739841715.jpg","hash":"aaa4919b1fe65f12a5499d93aced90c2f890b52b","modified":1679148417000},{"_id":"source/_posts/学习记录/DataWhale/DataWhale学习记录/深度学习小技巧3.jpg","hash":"aaa4919b1fe65f12a5499d93aced90c2f890b52b","modified":1679148417000},{"_id":"source/_posts/学习记录/DataWhale/DataWhale学习记录/深度学习小技巧5-167915740039318.jpg","hash":"ad4fa47a1417ffb302504f8b3dc32eea53dffd23","modified":1679148423000},{"_id":"source/_posts/学习记录/DataWhale/DataWhale学习记录/深度学习小技巧5.jpg","hash":"ad4fa47a1417ffb302504f8b3dc32eea53dffd23","modified":1679148423000},{"_id":"source/_posts/学习记录/DataWhale/DataWhale学习记录/科研经验-16791573889377.jpg","hash":"4b2c98f89ef48c18576738a89f1e84e3fae98561","modified":1679148339000},{"_id":"source/_posts/学习记录/DataWhale/DataWhale学习记录/深度学习细分领域-167915739206110.jpg","hash":"d3f8f8e8641fafd4372b541ce05cdd0391d00d50","modified":1679148381000},{"_id":"source/_posts/学习记录/DataWhale/DataWhale学习记录/科研经验.jpg","hash":"4b2c98f89ef48c18576738a89f1e84e3fae98561","modified":1679148339000},{"_id":"source/_posts/学习记录/DataWhale/DataWhale学习记录/深度学习细分领域.jpg","hash":"d3f8f8e8641fafd4372b541ce05cdd0391d00d50","modified":1679148381000},{"_id":"themes/butterfly/source/img/avater.png","hash":"8d31863742617a2383710fac1c70400d9704b691","modified":1678030265391},{"_id":"source/_posts/学习记录/考研/计算机网络/image-20230205152246881.png","hash":"a13fb5852d47bc10158cf185ab118cc413665e01","modified":1678107288000},{"_id":"themes/butterfly/source/img/covers/8.jpg","hash":"4c9641258ca757e1a9bc5a3ebbbd89ecff87bed8","modified":1679479731639},{"_id":"themes/butterfly/source/img/covers/1.jpg","hash":"8aa6de21cf0bc5783466c0eb1a0f7d8d0cd75c90","modified":1679479652387},{"_id":"themes/butterfly/source/img/index_img1.jpg","hash":"8ce7c8f4199f30766adf5e5223fd5191fa6401bb","modified":1678105975213},{"_id":"themes/butterfly/source/img/index_img.jpg","hash":"483ac5cab7508d91d30e0e56423a083ae1bfec0f","modified":1679398067643},{"_id":"themes/butterfly/source/img/covers/4.jpg","hash":"d9b8b71810a38dc9936bc715419387c9c2bbe7d3","modified":1679479684086},{"_id":"themes/butterfly/source/img/covers/3.jpg","hash":"7e77782c501a6377f88b7baa7c1b44f03815ac88","modified":1679479672846},{"_id":"themes/butterfly/source/img/covers/6.jpg","hash":"dd0900ed6ab7f3ff57644a095baba6151dd30618","modified":1679479701571},{"_id":"themes/butterfly/source/img/covers/7.jpg","hash":"d34bbcf57a03f65c9f98dfbb070f4fd7ba5a743f","modified":1679479725516},{"_id":"themes/butterfly/source/img/covers/9.jpg","hash":"0c6b0a5da9d637c4a8114ab655685a7128a892d9","modified":1679479980104},{"_id":"themes/butterfly/source/img/covers/5.jpg","hash":"2e25deda50c281e5d171db3b201aad87b5cedc25","modified":1679479908070},{"_id":"source/_posts/学习记录/考研/计算机网络/image-20230205135027022.png","hash":"8f095e7de7516fdfd929a860dcd0e0f97d88bbb2","modified":1678107315000},{"_id":"themes/butterfly/source/img/covers/2.jpg","hash":"c6c50a2c3d655dc191f0a44dcfc42d5a3964fd59","modified":1679479666843},{"_id":"themes/butterfly/source/img/covers/10.jpg","hash":"48071940c9079cd8d1c2e544ab482753c09828b7","modified":1679479987565},{"_id":"source/_posts/学习记录/DataWhale/DataWhale学习记录/image-20230318220521147.png","hash":"b62ae680b576f3375f36ef4eb66adc22af3180b8","modified":1679148321000},{"_id":"themes/butterfly/source/img/covers/11.jpg","hash":"82282ec71e0f5221b84a530667d6e0ef4563ded5","modified":1679479916044},{"_id":"public/Music/index.html","hash":"345f6d564a66e75f264ab76a295545122855eba3","modified":1680333477067},{"_id":"public/tags/index.html","hash":"79aec0e7e448092b59099f37682f632e66e0f6d0","modified":1680333477067},{"_id":"public/categories/index.html","hash":"8c766e0511b8b5d87201e9293073ac099369139e","modified":1680333477067},{"_id":"public/404.html","hash":"58cb893b1fc806b1d4b23fa2ec418deb678539df","modified":1680333477067},{"_id":"public/2023/03/22/学习记录/DataWhale/D2L_ch02_预备知识/index.html","hash":"8638997a8e6bad161cf3192b61ea13c0d978153f","modified":1680333477067},{"_id":"public/2023/03/21/踩坑日记/Tips/Git-Bash秒变Linux/index.html","hash":"17e1e5ab8af75a4b0ad0a095767dda5641ccdba4","modified":1680333477067},{"_id":"public/2023/03/20/踩坑日记/环境配置/Windows下D2L-Pytorch环境配置-CPU版/index.html","hash":"dfa6530b5eca9804db2530a9c2fb1a951036293d","modified":1680333477067},{"_id":"public/2023/03/19/学习记录/DataWhale/D2L_Ch01_深度学习的相关概念/index.html","hash":"3af2141940a5e49e374eb12218ab39c83d078d6b","modified":1680333477067},{"_id":"public/2023/03/18/学习记录/DataWhale/DataWhale学习记录/index.html","hash":"641a8cb9a3f4766fa7ddbf3ffb26214f3afff5b6","modified":1680333477067},{"_id":"public/2023/03/06/学习记录/考研/微机原理及接口技术/index.html","hash":"746ddfdb2ad5a0a7b1c67a7e54d774e0a0a23357","modified":1680333477067},{"_id":"public/2023/03/03/学习记录/考研/计算机网络/index.html","hash":"2a22f67d9ed5276778ec4dfc8b615ee239473187","modified":1680333477067},{"_id":"public/2023/03/02/踩坑日记/Tips/7x24-Hours-无人监守直播间/index.html","hash":"36be38652f997fc72de1dd0a158d9bdda81869e4","modified":1680333477067},{"_id":"public/archives/index.html","hash":"b6410e16111bea010ed975f6e3fe0ef2e6fe8bf8","modified":1680333477067},{"_id":"public/archives/2023/index.html","hash":"7ea35e472571bb61e155578d67966bb8b95e46f3","modified":1680333477067},{"_id":"public/2023/01/01/踩坑日记/Tips/hello-world/index.html","hash":"e88e7fe8105660e1c9e59bc8504327528c1f6382","modified":1680333477067},{"_id":"public/archives/2023/01/index.html","hash":"d67009e183316075bcc4f60745d1cc3a97979194","modified":1680333477067},{"_id":"public/archives/2023/03/index.html","hash":"8436b3138a68b7b59c6e788ed66ccc2cc02f2321","modified":1680333477067},{"_id":"public/categories/学习记录/index.html","hash":"e881d9efe0360ed1e8528813ccfdd7fbdeb0bb43","modified":1680333477067},{"_id":"public/index.html","hash":"381454754e71c591d6b782c5c134c836aeaf26b7","modified":1680333477067},{"_id":"public/categories/学习记录/DataWhale/index.html","hash":"c1bb6dd8e650ebdd2c2d76936a18aacf6da363cd","modified":1680333477067},{"_id":"public/categories/学习记录/考研/index.html","hash":"5d09f5351cdf132c3745b34fb5810b6e0b3541bb","modified":1680333477067},{"_id":"public/categories/踩坑日记/环境配置/index.html","hash":"0d04776468c9b9035f3293b5a40ccd7530942e32","modified":1680333477067},{"_id":"public/categories/踩坑日记/index.html","hash":"4ed0f863ab0600136190545ca265a1693d30e347","modified":1680333477067},{"_id":"public/categories/踩坑日记/Tips/index.html","hash":"ed67a992fc920df7c5b9a1f757b3e3f5cb08ec74","modified":1680333477067},{"_id":"public/tags/Deep-Learning/index.html","hash":"b469c84524f78c28e5cf566b52995e24c8fadffa","modified":1680333477067},{"_id":"public/tags/DataWhale/index.html","hash":"076baf8f5f769bd474aaeb483ae8ac0815bf603e","modified":1680333477067},{"_id":"public/tags/微机原理及接口技术/index.html","hash":"8a3ef6999d583b403777cb62945396851c44bc24","modified":1680333477067},{"_id":"public/tags/计算机网络/index.html","hash":"3aeac5140142c9d8afb51cc984fe8f1b7695e736","modified":1680333477067},{"_id":"public/tags/自动化/index.html","hash":"b227ce6b1fac6e9799090fdb8b6c2028945c32da","modified":1680333477067},{"_id":"public/tags/bash脚本/index.html","hash":"ff751e4aa7b9b3986c2cc18e748cd9bcfca89806","modified":1680333477067},{"_id":"public/tags/控制科学与工程/index.html","hash":"91797eb3fee1c5f626f8f018bb81fb536c00fc98","modified":1680333477067},{"_id":"public/tags/Pytorch/index.html","hash":"8bb1f69c0b695c77fdb7a1792462ed18e701942b","modified":1680333477067},{"_id":"public/tags/Linux/index.html","hash":"1b828a6bcc8242b23517a37d6dfdd5c7bb3c394e","modified":1680333477067},{"_id":"public/tags/Git-Bush/index.html","hash":"2f384fc1118e597c3aa91248a3d0a623a9c96ede","modified":1680333477067},{"_id":"public/img/archive_img.jpg","hash":"e986c094e95dcb3360b0b0c5a0e307ba5fd0f7e4","modified":1679482514777},{"_id":"public/img/favicon.png","hash":"fc65e76fdb18ff75038f60c0ba50dcbec38c43ef","modified":1679482514777},{"_id":"public/img/404.jpg","hash":"fb4489bc1d30c93d28f7332158c1c6c1416148de","modified":1679482514777},{"_id":"public/img/favicon1.png","hash":"3cf89864b4f6c9b532522a4d260a2e887971c92d","modified":1679482514777},{"_id":"public/img/friend_404.gif","hash":"8d2d0ebef70a8eb07329f57e645889b0e420fa48","modified":1679482514777},{"_id":"public/2023/03/06/学习记录/考研/微机原理及接口技术/image-20230308232215011.png","hash":"92e9d68096b222dacc409d572e2a00bf3513940d","modified":1679482514777},{"_id":"public/2023/03/06/学习记录/考研/微机原理及接口技术/image-20230308223135359.png","hash":"b41a3a08fa315cc9568144e0d8b7dfd60d97f104","modified":1679482514777},{"_id":"public/2023/03/06/学习记录/考研/微机原理及接口技术/微机原理及接口技术.jpg","hash":"faac49fb751999f721a318a563a4cad1413a858f","modified":1679482514777},{"_id":"public/2023/03/06/学习记录/考研/微机原理及接口技术/image-20230308232215011-16782889368582.png","hash":"92e9d68096b222dacc409d572e2a00bf3513940d","modified":1679482514777},{"_id":"public/2023/03/22/学习记录/DataWhale/D2L_ch02_预备知识/output_7_0.svg","hash":"94a763b1f112bbd036258b596e3f9cf62441f3db","modified":1679482514777},{"_id":"public/2023/03/19/学习记录/DataWhale/D2L_Ch01_深度学习的相关概念/image-20230319170611582.png","hash":"f0aeb558b47d6ad6a70fbf94724e31315d9aefb2","modified":1679482514777},{"_id":"public/2023/03/22/学习记录/DataWhale/D2L_ch02_预备知识/output_7_0-16794727634302.svg","hash":"94a763b1f112bbd036258b596e3f9cf62441f3db","modified":1679482514777},{"_id":"public/2023/03/19/学习记录/DataWhale/D2L_Ch01_深度学习的相关概念/image-20230319174557369.png","hash":"3d0aa98dad6dc865f7caf71b66e4fe818f3b5b64","modified":1679482514777},{"_id":"public/2023/03/19/学习记录/DataWhale/D2L_Ch01_深度学习的相关概念/image-20230319192839845.png","hash":"fa8b873f2196066f7c7840e3ea7fae99a768677a","modified":1679482514777},{"_id":"public/2023/03/19/学习记录/DataWhale/D2L_Ch01_深度学习的相关概念/image-20230319174247282.png","hash":"40387970af363f240d787edf2468fc343ae47f5c","modified":1679482514777},{"_id":"public/css/background.css","hash":"e168863f3670bd92ade51faff1b29f598f218a65","modified":1679482514777},{"_id":"public/css/var.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1679482514777},{"_id":"public/js/utils.js","hash":"85bae4adcec638fa3ef1e734414025c6b2c90f3c","modified":1679482514777},{"_id":"public/js/search/local-search.js","hash":"5f2ddfd705a85c6a6d327ffcb51d530d8f167e52","modified":1679482514777},{"_id":"public/js/search/algolia.js","hash":"5f4294337c28324e088908ed0ceb237a2a360ae5","modified":1679482514777},{"_id":"public/css/index.css","hash":"27c4cf36104e18868d733c50843fb64802cdf14f","modified":1679482514777},{"_id":"public/js/main.js","hash":"05c825962e365af62096d3f1b4d7c9ee1b5fc2f5","modified":1679482514777},{"_id":"public/js/tw_cn.js","hash":"76d0c5c172cae44b34b0bd3125fd068b2c3cbd4a","modified":1679482514777},{"_id":"public/2023/03/18/学习记录/DataWhale/DataWhale学习记录/深度学习小技巧4.jpg","hash":"4379527940028695566105f491e8c15bde3b8a32","modified":1679482514777},{"_id":"public/2023/03/21/踩坑日记/Tips/Git-Bash秒变Linux/image-20230320230913167-16793685665802.png","hash":"614b36d944bad55509c5ce983bfd5099e589cdd0","modified":1679482514777},{"_id":"public/2023/03/21/踩坑日记/Tips/Git-Bash秒变Linux/image-20230320230913167.png","hash":"614b36d944bad55509c5ce983bfd5099e589cdd0","modified":1679482514777},{"_id":"public/img/avater.png","hash":"8d31863742617a2383710fac1c70400d9704b691","modified":1679482514777},{"_id":"public/2023/03/21/踩坑日记/Tips/Git-Bash秒变Linux/image-20230321164332935-16793882362101.png","hash":"7629f9f4e32023525b02b4c54f1177950b05c229","modified":1679482514777},{"_id":"public/2023/03/21/踩坑日记/Tips/Git-Bash秒变Linux/image-20230321164332935.png","hash":"7629f9f4e32023525b02b4c54f1177950b05c229","modified":1679482514777},{"_id":"public/2023/03/18/学习记录/DataWhale/DataWhale学习记录/CV入门-16791573687564.jpg","hash":"fe513be3d0a820213b36a68dcf672c41b101b225","modified":1679482514777},{"_id":"public/2023/03/19/学习记录/DataWhale/D2L_Ch01_深度学习的相关概念/image-20230319212918986.png","hash":"5ab7d037a6129525be1d42270b864f4fa27ca766","modified":1679482514777},{"_id":"public/2023/03/18/学习记录/DataWhale/DataWhale学习记录/CV入门.jpg","hash":"fe513be3d0a820213b36a68dcf672c41b101b225","modified":1679482514777},{"_id":"public/2023/03/18/学习记录/DataWhale/DataWhale学习记录/人工智能路线图-16791573386491.jpg","hash":"6d1cc012beb5584051f964023ef24d06d3aba5bb","modified":1679482514777},{"_id":"public/2023/03/18/学习记录/DataWhale/DataWhale学习记录/人工智能路线图-16791581610352.jpg","hash":"6d1cc012beb5584051f964023ef24d06d3aba5bb","modified":1679482514777},{"_id":"public/2023/03/18/学习记录/DataWhale/DataWhale学习记录/人工智能路线图.jpg","hash":"6d1cc012beb5584051f964023ef24d06d3aba5bb","modified":1679482514777},{"_id":"public/2023/03/18/学习记录/DataWhale/DataWhale学习记录/深度学习小技巧-167915739504012.jpg","hash":"89c4705d656a19cc73fd8dbaf41e0e5ec19c5070","modified":1679482514777},{"_id":"public/2023/03/18/学习记录/DataWhale/DataWhale学习记录/深度学习小技巧2.jpg","hash":"bbb692e7852391188d4d7f6ef36ea03942d785f6","modified":1679482514777},{"_id":"public/2023/03/18/学习记录/DataWhale/DataWhale学习记录/深度学习小技巧.jpg","hash":"89c4705d656a19cc73fd8dbaf41e0e5ec19c5070","modified":1679482514777},{"_id":"public/2023/03/18/学习记录/DataWhale/DataWhale学习记录/深度学习小技巧3.jpg","hash":"aaa4919b1fe65f12a5499d93aced90c2f890b52b","modified":1679482514777},{"_id":"public/2023/03/18/学习记录/DataWhale/DataWhale学习记录/深度学习小技巧3-167915739841715.jpg","hash":"aaa4919b1fe65f12a5499d93aced90c2f890b52b","modified":1679482514777},{"_id":"public/2023/03/18/学习记录/DataWhale/DataWhale学习记录/深度学习小技巧5-167915740039318.jpg","hash":"ad4fa47a1417ffb302504f8b3dc32eea53dffd23","modified":1679482514777},{"_id":"public/2023/03/18/学习记录/DataWhale/DataWhale学习记录/深度学习小技巧5.jpg","hash":"ad4fa47a1417ffb302504f8b3dc32eea53dffd23","modified":1679482514777},{"_id":"public/2023/03/18/学习记录/DataWhale/DataWhale学习记录/深度学习细分领域-167915739206110.jpg","hash":"d3f8f8e8641fafd4372b541ce05cdd0391d00d50","modified":1679482514777},{"_id":"public/2023/03/18/学习记录/DataWhale/DataWhale学习记录/深度学习细分领域.jpg","hash":"d3f8f8e8641fafd4372b541ce05cdd0391d00d50","modified":1679482514777},{"_id":"public/2023/03/18/学习记录/DataWhale/DataWhale学习记录/科研经验-16791573889377.jpg","hash":"4b2c98f89ef48c18576738a89f1e84e3fae98561","modified":1679482514777},{"_id":"public/2023/03/18/学习记录/DataWhale/DataWhale学习记录/科研经验.jpg","hash":"4b2c98f89ef48c18576738a89f1e84e3fae98561","modified":1679482514777},{"_id":"public/2023/03/20/踩坑日记/环境配置/Windows下D2L-Pytorch环境配置-CPU版/image-20230320210330101.png","hash":"24702f91df722fee80c43bd8f70cd513ddae2ada","modified":1679482514777},{"_id":"public/2023/03/03/学习记录/考研/计算机网络/image-20230205152246881.png","hash":"a13fb5852d47bc10158cf185ab118cc413665e01","modified":1679482514777},{"_id":"public/img/covers/8.jpg","hash":"4c9641258ca757e1a9bc5a3ebbbd89ecff87bed8","modified":1679482514777},{"_id":"public/img/covers/1.jpg","hash":"8aa6de21cf0bc5783466c0eb1a0f7d8d0cd75c90","modified":1679482514777},{"_id":"public/img/index_img1.jpg","hash":"8ce7c8f4199f30766adf5e5223fd5191fa6401bb","modified":1679482514777},{"_id":"public/img/index_img.jpg","hash":"483ac5cab7508d91d30e0e56423a083ae1bfec0f","modified":1679482514777},{"_id":"public/img/covers/4.jpg","hash":"d9b8b71810a38dc9936bc715419387c9c2bbe7d3","modified":1679482514777},{"_id":"public/img/covers/3.jpg","hash":"7e77782c501a6377f88b7baa7c1b44f03815ac88","modified":1679482514777},{"_id":"public/img/covers/6.jpg","hash":"dd0900ed6ab7f3ff57644a095baba6151dd30618","modified":1679482514777},{"_id":"public/img/covers/7.jpg","hash":"d34bbcf57a03f65c9f98dfbb070f4fd7ba5a743f","modified":1679482514777},{"_id":"public/img/covers/5.jpg","hash":"2e25deda50c281e5d171db3b201aad87b5cedc25","modified":1679482514777},{"_id":"public/img/covers/9.jpg","hash":"0c6b0a5da9d637c4a8114ab655685a7128a892d9","modified":1679482514777},{"_id":"public/2023/03/03/学习记录/考研/计算机网络/image-20230205135027022.png","hash":"8f095e7de7516fdfd929a860dcd0e0f97d88bbb2","modified":1679482514777},{"_id":"public/img/covers/2.jpg","hash":"c6c50a2c3d655dc191f0a44dcfc42d5a3964fd59","modified":1679482514777},{"_id":"public/img/covers/10.jpg","hash":"48071940c9079cd8d1c2e544ab482753c09828b7","modified":1679482514777},{"_id":"public/2023/03/18/学习记录/DataWhale/DataWhale学习记录/image-20230318220521147.png","hash":"b62ae680b576f3375f36ef4eb66adc22af3180b8","modified":1679482514777},{"_id":"public/img/covers/11.jpg","hash":"82282ec71e0f5221b84a530667d6e0ef4563ded5","modified":1679482514777},{"_id":"source/_drafts/WSL位置迁移.md","hash":"7fcfaa99271357149c890ff7eb1196f842f3b5eb","modified":1680247237115},{"_id":"source/_drafts/WSL更换清华源.md","hash":"895844b3c5161fd0573b2eac6b8376db477ed170","modified":1680247237155},{"_id":"source/_drafts/WSL配置zsh.md","hash":"4a6a70d0dd7f8c8f921d1470c21237ad3d3ad14c","modified":1680247237155},{"_id":"source/_drafts/Pandas数据处理.md","hash":"b720fb595658a7d28663c5c3137a63ed0fb87b07","modified":1680245253018}],"Category":[{"name":"学习记录","_id":"clfjkj0bj00068gsz2a84angh"},{"name":"DataWhale","parent":"clfjkj0bj00068gsz2a84angh","_id":"clfjkj0bu000f8gszdmx0a8g6"},{"name":"考研","parent":"clfjkj0bj00068gsz2a84angh","_id":"clfjkj0by000l8gsz2tw8678c"},{"name":"踩坑日记","_id":"clfjkj0ca001b8gsz52d6ginw"},{"name":"Tips","parent":"clfjkj0ca001b8gsz52d6ginw","_id":"clfjkj0ce001v8gsz6tug7uv0"},{"name":"环境配置","parent":"clfjkj0ca001b8gsz52d6ginw","_id":"clfjkj0cf001z8gsze5gi7v3u"},{"name":"Python","parent":"clfjkj0bj00068gsz2a84angh","_id":"clfw9fnf10003v8szcs4q21qt"}],"Data":[],"Page":[{"title":"Music","type":"music","date":"2023-03-21T16:28:24.000Z","_content":"\n{% meting \"8841213199\" \"tencent\" \"playlist\" \"autoplay\" \"mutex:false\" \"listmaxheight:400px\" \"preload:none\" \"theme:#ad7a86\"%}\n\n","source":"Music/index.md","raw":"---\ntitle: Music\ntype: \"music\"\ndate: 2023-03-22 00:28:24\n---\n\n{% meting \"8841213199\" \"tencent\" \"playlist\" \"autoplay\" \"mutex:false\" \"listmaxheight:400px\" \"preload:none\" \"theme:#ad7a86\"%}\n\n","updated":"2023-03-21T16:29:27.504Z","path":"Music/index.html","comments":1,"layout":"page","_id":"clfjkj0b400008gszdk144o8c","content":"\n    <div id=\"aplayer-dnSsRjSc\" class=\"aplayer aplayer-tag-marker meting-tag-marker\" data-id=\"8841213199\" data-server=\"tencent\" data-type=\"playlist\" data-mode=\"circulation\" data-autoplay=\"true\" data-mutex=\"false\" data-listmaxheight=\"400px\" data-preload=\"none\" data-theme=\"#ad7a86\"></div>\n","site":{"data":{}},"cover":"/img/covers/9.jpg","cover_type":"img","excerpt":"","more":"\n    <div id=\"aplayer-dnSsRjSc\" class=\"aplayer aplayer-tag-marker meting-tag-marker\" data-id=\"8841213199\" data-server=\"tencent\" data-type=\"playlist\" data-mode=\"circulation\" data-autoplay=\"true\" data-mutex=\"false\" data-listmaxheight=\"400px\" data-preload=\"none\" data-theme=\"#ad7a86\"></div>\n"},{"title":"Tags","date":"2023-03-21T05:15:39.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: Tags\ndate: 2023-03-21 13:15:39\ntype: \"tags\"\ncomments: false\n---\n","updated":"2023-03-21T09:52:49.760Z","path":"tags/index.html","layout":"page","_id":"clfjkj0be00028gsz215gc5k4","content":"","site":{"data":{}},"cover":"/img/covers/11.jpg","cover_type":"img","excerpt":"","more":""},{"title":"Categories","date":"2023-03-21T05:00:21.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: Categories\ndate: 2023-03-21 13:00:21\ntype: \"categories\"\ncomments: false\n---\n","updated":"2023-03-21T05:09:24.137Z","path":"categories/index.html","layout":"page","_id":"clfjkj0bh00048gsz1pfp02g8","content":"","site":{"data":{}},"cover":"/img/covers/10.jpg","cover_type":"img","excerpt":"","more":""}],"Post":[{"title":"计算机网络","abbrlink":"e255a10a","date":"2023-03-03T12:49:55.000Z","_content":"\n# 计算机网络\n\n## 概述\n\n### 基本概念\n\n* 网络（广义）\n\n    > 广义上的网络，是类似的事物连接在一起，以提供某些功能。\n\n* 计算机网络\n\n    > 使用单一技术相互连接的自主计算机的互联集合；单台计算机具有独立自主性；\n    >\n    > 连接介质可以是光纤、铜线，也可以是微博红外、卫星。\n\n* 拓扑\n\n    > 信道的分布方式\n    >\n    > 常见的拓扑有：总线型、星型、环型、树型和网状\n    >\n    > 最常见的是：总线拓扑和星型拓扑\n    >\n    > * 总部拓扑：\n    >\n    >     > 主机挂接在总线上，相互直接通达；从主机上发出信号在总线上双向同时传输；\n    >     >\n    >     > 所有的主机都可以接收到这个信号。\n    >\n    > * 星型拓扑：\n    >\n    >     > 主机都挂接在一个中心节点上；早期的中心节点有集线器充当，现在的中心节点主要由交换机充当。\n    >     >\n    >     > 缺点：单点故障\n\n* 协议：\n\n    > 一系列规则和约定的规范性描述，它控制网络中的设备之间如何进行信息交换\n\n* 数字带宽：\n\n    > 指在单位时间内流经的信息总量，基本单位为：比特每秒，即bps；常用Kbps、Mbps、Gbps，更大的还有T、P、E、Z、Y。（相邻等级间都是10^3倍）\n\n* 吞吐量：\n\n    > 指实际、可测到的带宽。是用户真真切切感受到的带宽，是能让用户感受网络好坏的**重要指标**\n    >\n    > 会受到多种因素的影响：\n    >\n    > * 网络设备和性能\n    > * 用网的时间\n    > * 网络拓扑\n    > * 用户数量\n    > * 用户计算机性能\n    > * 服务器\n\n* 信息量（S）、带宽（BW）和传输时间（T）之间的关系：\n\n    > * 理想情况：$T = S / BW$\n    >\n    > * 实际情况：$ T = S / P $(吞吐量)\n\n* 点到点和端到端：\n\n    > * 点到点（P2P）：\n    >\n    >     > 信源机和信宿机之间的通行是由一段一段的直接相连的机器间的通信组成，机器间的直接连接叫做点对点连接。如：PC1 —— PC2\n    >\n    > * 端到端：\n    >\n    >     > 信源机和信宿机之间的之间通信，好象拥有一条直接的线路。\n\n* 计算机网络的分类：\n\n    * 按传输介质分类：\n\n        * 有线网络\n        * 无线网络\n\n    * 按网络大小规模来划分：\n\n        * PAN(Personal Area Networks) \n\n            > 最小的计算机网络，覆盖范围在1M左右\n\n        * LAN(Local Area Networks)\n\n            > 比PAN大，覆盖范围约1KM左右，通常覆盖一个校园、一个单位或者一个建筑。\n\n        * MAN(Metropolitan Area Networks)\n\n            > 比LAN大，覆盖约10KM范围，通常覆盖一个城市\n\n        * WAN(Wide Area Networks)\n\n            > 比MAN大，覆盖范围约100KM/1000KM，通常覆盖一个国家、一个州。\n\n        * Internet\n\n            > 最大的网络，覆盖范围约10000KM，覆盖全球。\n\n###  参考模型\n\n* 网络分层的优点：\n\n    * 各层工作独立，层之间同过接口联系，降低协议工作的复杂程度\n    * 灵活性好，任何一层的改变不影响其它层\n    * 每层的实现技术可以不同，减少实现的复杂度\n    * 易于维护，每层可以单独进行调试\n    * 便于标准化\n\n* 分层交流模型：\n\n    > ![image-20230205135027022](计算机网络/image-20230205135027022.png)\n\n* 分层原则：\n\n    > 信宿机第n层收到 的对象和应与信源机第n层发出的对象完全一致\n\n* 典型的分层模型：\n\n    * **OSI 七层模型**:\n\n        > OSI (Open System Interconnection 开放系统互连) 是 ISO (International Standards Organization)在1983年提出的。\n        >\n        > - **7 Application（应用层）：**\n        >\n        >     > 主要为各种各样的网络应用（如 Email、FTP、微信等）提供网络服务。\n        >\n        > - **6 Presentation（表示层）：**\n        >\n        >     > 将信息表示为一定形式和格式的数据流、压缩解压缩、加密解密等都是这层的任务。\n        >\n        > - **5 Session（会话层）：**\n        >\n        >     > 负责通信主机间的会话的建立，管理和拆除；协调双方的会话。\n        >\n        > - **4 Transport（传输层）：**\n        >\n        >     > 是参考模型上的核心层之一，它负责通信主机间的端到端连接；\n        >     >\n        >     > 对于TCP来说，还负责提供可靠传输、差错恢复、拥塞控制等额外的功能。\n        >\n        > - **3 Network（网络层）：**\n        >\n        >     > 是另外一个核心层，它的功能可用**地址**（为通行主机提供标识）和**最优路径**（最优路径是说路由，寻径，每一个中间设备都为到达的分组找到一根最优的路径，并送出）来描述；它负责将每一个分组从源机一路送达目的机。\n        >\n        > - **2 Data Link（数据链路层）：**\n        >\n        >     > 主要提供介质访问服务，通过物理地址识别通信主机，提供可靠的帧传输并做差错控制，流控等。\n        >\n        > - **1 Physical（物理层）：**\n        >\n        >     > 提供透明的比特流传输，可以是光信号、电信号、无线信号，物理层只关心比特流的传递，而忽略比特流里面的具体内容。\n        >\n        > <!--每一层都完成特定的功能，都利用下层的服务为上层提供服务，除了第1层和第7层-->\n\n    * TCP/IP(DoD)四层模型\n\n        * 4 Application（应用层）：\n        * 3 Transport（传输层）：\n        * 2 Internet（网络层）：\n        * Network Access（网络接入层）：\n\n    * OSI模型和DoD模型比较：\n\n        * 相同点：\n            * 都分层\n            * 都有应用层，但服务有所不同\n            * 都有可以比较的传输层和网络层\n            * 使用的分组交换而不是电路交换技术\n        * 不同点：\n            * TCP/IP将表示层和会话层包含到了应用层\n            * TCP/IP将OSI的数据链路层和物理层合为一层中\n            * TCP/IP更简洁，但OSI更易开发和排除故障\n\n###  数据传输\n\n* **协议数据单元**（PDU: Protocol Data Unit）:\n\n    > 数据在各层的形式（或者说各层处理的数据对象），每一层的名称有所不同\n    >\n    > * 信息（Information，应用层）\n    > * 数据流（Data stream，上三层）\n    > * 数据段（Segment，传输层）\n    > * 分组（Packet，网络层）\n    > * 帧（Frame，数据链路层）\n    > * 比特流（Bits，物理层）\n\n    1. **发放方** 的 **封装/打包**：\n\n        >  将信息打包，从最高层——应用层开始逐渐下行到最底层——物理层。每一层上，数据都被加上头部信息，用于传递信息。\n        >\n        >  具体来说，在OSI的上三层，信息被表示为一定格式和形式的数据流（Datastream），数据流被传到传输层，将其切割为适合传输的数据段（Segment）并加上段头，段头中包含定位应用进程的端口号等信息\n\n    2. **收方** 的 **解封装/解包**：\n\n        > 将收到的比特流解包，从最底层——物理层开始，逐渐上行到最高层——应用层，提取出信息。解封装的过程是封装的逆向过程，在每层去掉头部信息，最终还原出应用层的输出：信息\n\n        <!--任何一次通行总是以发方的封装开始，以收方的解封装结束-->\n\n* 收发双发的数据流：\n\n    > ![image-20230205152246881](计算机网络/image-20230205152246881.png)\n\n## 物理层\n\n## 数据链路层\n\n## 介质访问控制子层\n\n## 网络层\n\n## 传输层\n\n## 应用层\n\n","source":"_posts/学习记录/考研/计算机网络.md","raw":"---\ntitle: 计算机网络\ncategories:\n  - 学习记录\n  - 考研\ntags:\n  - 计算机网络\nabbrlink: e255a10a\ndate: 2023-03-03 20:49:55\n---\n\n# 计算机网络\n\n## 概述\n\n### 基本概念\n\n* 网络（广义）\n\n    > 广义上的网络，是类似的事物连接在一起，以提供某些功能。\n\n* 计算机网络\n\n    > 使用单一技术相互连接的自主计算机的互联集合；单台计算机具有独立自主性；\n    >\n    > 连接介质可以是光纤、铜线，也可以是微博红外、卫星。\n\n* 拓扑\n\n    > 信道的分布方式\n    >\n    > 常见的拓扑有：总线型、星型、环型、树型和网状\n    >\n    > 最常见的是：总线拓扑和星型拓扑\n    >\n    > * 总部拓扑：\n    >\n    >     > 主机挂接在总线上，相互直接通达；从主机上发出信号在总线上双向同时传输；\n    >     >\n    >     > 所有的主机都可以接收到这个信号。\n    >\n    > * 星型拓扑：\n    >\n    >     > 主机都挂接在一个中心节点上；早期的中心节点有集线器充当，现在的中心节点主要由交换机充当。\n    >     >\n    >     > 缺点：单点故障\n\n* 协议：\n\n    > 一系列规则和约定的规范性描述，它控制网络中的设备之间如何进行信息交换\n\n* 数字带宽：\n\n    > 指在单位时间内流经的信息总量，基本单位为：比特每秒，即bps；常用Kbps、Mbps、Gbps，更大的还有T、P、E、Z、Y。（相邻等级间都是10^3倍）\n\n* 吞吐量：\n\n    > 指实际、可测到的带宽。是用户真真切切感受到的带宽，是能让用户感受网络好坏的**重要指标**\n    >\n    > 会受到多种因素的影响：\n    >\n    > * 网络设备和性能\n    > * 用网的时间\n    > * 网络拓扑\n    > * 用户数量\n    > * 用户计算机性能\n    > * 服务器\n\n* 信息量（S）、带宽（BW）和传输时间（T）之间的关系：\n\n    > * 理想情况：$T = S / BW$\n    >\n    > * 实际情况：$ T = S / P $(吞吐量)\n\n* 点到点和端到端：\n\n    > * 点到点（P2P）：\n    >\n    >     > 信源机和信宿机之间的通行是由一段一段的直接相连的机器间的通信组成，机器间的直接连接叫做点对点连接。如：PC1 —— PC2\n    >\n    > * 端到端：\n    >\n    >     > 信源机和信宿机之间的之间通信，好象拥有一条直接的线路。\n\n* 计算机网络的分类：\n\n    * 按传输介质分类：\n\n        * 有线网络\n        * 无线网络\n\n    * 按网络大小规模来划分：\n\n        * PAN(Personal Area Networks) \n\n            > 最小的计算机网络，覆盖范围在1M左右\n\n        * LAN(Local Area Networks)\n\n            > 比PAN大，覆盖范围约1KM左右，通常覆盖一个校园、一个单位或者一个建筑。\n\n        * MAN(Metropolitan Area Networks)\n\n            > 比LAN大，覆盖约10KM范围，通常覆盖一个城市\n\n        * WAN(Wide Area Networks)\n\n            > 比MAN大，覆盖范围约100KM/1000KM，通常覆盖一个国家、一个州。\n\n        * Internet\n\n            > 最大的网络，覆盖范围约10000KM，覆盖全球。\n\n###  参考模型\n\n* 网络分层的优点：\n\n    * 各层工作独立，层之间同过接口联系，降低协议工作的复杂程度\n    * 灵活性好，任何一层的改变不影响其它层\n    * 每层的实现技术可以不同，减少实现的复杂度\n    * 易于维护，每层可以单独进行调试\n    * 便于标准化\n\n* 分层交流模型：\n\n    > ![image-20230205135027022](计算机网络/image-20230205135027022.png)\n\n* 分层原则：\n\n    > 信宿机第n层收到 的对象和应与信源机第n层发出的对象完全一致\n\n* 典型的分层模型：\n\n    * **OSI 七层模型**:\n\n        > OSI (Open System Interconnection 开放系统互连) 是 ISO (International Standards Organization)在1983年提出的。\n        >\n        > - **7 Application（应用层）：**\n        >\n        >     > 主要为各种各样的网络应用（如 Email、FTP、微信等）提供网络服务。\n        >\n        > - **6 Presentation（表示层）：**\n        >\n        >     > 将信息表示为一定形式和格式的数据流、压缩解压缩、加密解密等都是这层的任务。\n        >\n        > - **5 Session（会话层）：**\n        >\n        >     > 负责通信主机间的会话的建立，管理和拆除；协调双方的会话。\n        >\n        > - **4 Transport（传输层）：**\n        >\n        >     > 是参考模型上的核心层之一，它负责通信主机间的端到端连接；\n        >     >\n        >     > 对于TCP来说，还负责提供可靠传输、差错恢复、拥塞控制等额外的功能。\n        >\n        > - **3 Network（网络层）：**\n        >\n        >     > 是另外一个核心层，它的功能可用**地址**（为通行主机提供标识）和**最优路径**（最优路径是说路由，寻径，每一个中间设备都为到达的分组找到一根最优的路径，并送出）来描述；它负责将每一个分组从源机一路送达目的机。\n        >\n        > - **2 Data Link（数据链路层）：**\n        >\n        >     > 主要提供介质访问服务，通过物理地址识别通信主机，提供可靠的帧传输并做差错控制，流控等。\n        >\n        > - **1 Physical（物理层）：**\n        >\n        >     > 提供透明的比特流传输，可以是光信号、电信号、无线信号，物理层只关心比特流的传递，而忽略比特流里面的具体内容。\n        >\n        > <!--每一层都完成特定的功能，都利用下层的服务为上层提供服务，除了第1层和第7层-->\n\n    * TCP/IP(DoD)四层模型\n\n        * 4 Application（应用层）：\n        * 3 Transport（传输层）：\n        * 2 Internet（网络层）：\n        * Network Access（网络接入层）：\n\n    * OSI模型和DoD模型比较：\n\n        * 相同点：\n            * 都分层\n            * 都有应用层，但服务有所不同\n            * 都有可以比较的传输层和网络层\n            * 使用的分组交换而不是电路交换技术\n        * 不同点：\n            * TCP/IP将表示层和会话层包含到了应用层\n            * TCP/IP将OSI的数据链路层和物理层合为一层中\n            * TCP/IP更简洁，但OSI更易开发和排除故障\n\n###  数据传输\n\n* **协议数据单元**（PDU: Protocol Data Unit）:\n\n    > 数据在各层的形式（或者说各层处理的数据对象），每一层的名称有所不同\n    >\n    > * 信息（Information，应用层）\n    > * 数据流（Data stream，上三层）\n    > * 数据段（Segment，传输层）\n    > * 分组（Packet，网络层）\n    > * 帧（Frame，数据链路层）\n    > * 比特流（Bits，物理层）\n\n    1. **发放方** 的 **封装/打包**：\n\n        >  将信息打包，从最高层——应用层开始逐渐下行到最底层——物理层。每一层上，数据都被加上头部信息，用于传递信息。\n        >\n        >  具体来说，在OSI的上三层，信息被表示为一定格式和形式的数据流（Datastream），数据流被传到传输层，将其切割为适合传输的数据段（Segment）并加上段头，段头中包含定位应用进程的端口号等信息\n\n    2. **收方** 的 **解封装/解包**：\n\n        > 将收到的比特流解包，从最底层——物理层开始，逐渐上行到最高层——应用层，提取出信息。解封装的过程是封装的逆向过程，在每层去掉头部信息，最终还原出应用层的输出：信息\n\n        <!--任何一次通行总是以发方的封装开始，以收方的解封装结束-->\n\n* 收发双发的数据流：\n\n    > ![image-20230205152246881](计算机网络/image-20230205152246881.png)\n\n## 物理层\n\n## 数据链路层\n\n## 介质访问控制子层\n\n## 网络层\n\n## 传输层\n\n## 应用层\n\n","slug":"学习记录/考研/计算机网络","published":1,"updated":"2023-03-21T11:13:47.614Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clfjkj0bm000a8gsz8cfgf6f9","content":"<h1 id=\"计算机网络\"><a href=\"#计算机网络\" class=\"headerlink\" title=\"计算机网络\"></a>计算机网络</h1><h2 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h2><h3 id=\"基本概念\"><a href=\"#基本概念\" class=\"headerlink\" title=\"基本概念\"></a>基本概念</h3><ul>\n<li><p>网络（广义）</p>\n<blockquote>\n<p>广义上的网络，是类似的事物连接在一起，以提供某些功能。</p>\n</blockquote>\n</li>\n<li><p>计算机网络</p>\n<blockquote>\n<p>使用单一技术相互连接的自主计算机的互联集合；单台计算机具有独立自主性；</p>\n<p>连接介质可以是光纤、铜线，也可以是微博红外、卫星。</p>\n</blockquote>\n</li>\n<li><p>拓扑</p>\n<blockquote>\n<p>信道的分布方式</p>\n<p>常见的拓扑有：总线型、星型、环型、树型和网状</p>\n<p>最常见的是：总线拓扑和星型拓扑</p>\n<ul>\n<li><p>总部拓扑：</p>\n<blockquote>\n<p>主机挂接在总线上，相互直接通达；从主机上发出信号在总线上双向同时传输；</p>\n<p>所有的主机都可以接收到这个信号。</p>\n</blockquote>\n</li>\n<li><p>星型拓扑：</p>\n<blockquote>\n<p>主机都挂接在一个中心节点上；早期的中心节点有集线器充当，现在的中心节点主要由交换机充当。</p>\n<p>缺点：单点故障</p>\n</blockquote>\n</li>\n</ul>\n</blockquote>\n</li>\n<li><p>协议：</p>\n<blockquote>\n<p>一系列规则和约定的规范性描述，它控制网络中的设备之间如何进行信息交换</p>\n</blockquote>\n</li>\n<li><p>数字带宽：</p>\n<blockquote>\n<p>指在单位时间内流经的信息总量，基本单位为：比特每秒，即bps；常用Kbps、Mbps、Gbps，更大的还有T、P、E、Z、Y。（相邻等级间都是10^3倍）</p>\n</blockquote>\n</li>\n<li><p>吞吐量：</p>\n<blockquote>\n<p>指实际、可测到的带宽。是用户真真切切感受到的带宽，是能让用户感受网络好坏的<strong>重要指标</strong></p>\n<p>会受到多种因素的影响：</p>\n<ul>\n<li>网络设备和性能</li>\n<li>用网的时间</li>\n<li>网络拓扑</li>\n<li>用户数量</li>\n<li>用户计算机性能</li>\n<li>服务器</li>\n</ul>\n</blockquote>\n</li>\n<li><p>信息量（S）、带宽（BW）和传输时间（T）之间的关系：</p>\n<blockquote>\n<ul>\n<li><p>理想情况：$T = S / BW$</p>\n</li>\n<li><p>实际情况：$ T = S / P $(吞吐量)</p>\n</li>\n</ul>\n</blockquote>\n</li>\n<li><p>点到点和端到端：</p>\n<blockquote>\n<ul>\n<li><p>点到点（P2P）：</p>\n<blockquote>\n<p>信源机和信宿机之间的通行是由一段一段的直接相连的机器间的通信组成，机器间的直接连接叫做点对点连接。如：PC1 —— PC2</p>\n</blockquote>\n</li>\n<li><p>端到端：</p>\n<blockquote>\n<p>信源机和信宿机之间的之间通信，好象拥有一条直接的线路。</p>\n</blockquote>\n</li>\n</ul>\n</blockquote>\n</li>\n<li><p>计算机网络的分类：</p>\n<ul>\n<li><p>按传输介质分类：</p>\n<ul>\n<li>有线网络</li>\n<li>无线网络</li>\n</ul>\n</li>\n<li><p>按网络大小规模来划分：</p>\n<ul>\n<li><p>PAN(Personal Area Networks) </p>\n<blockquote>\n<p>最小的计算机网络，覆盖范围在1M左右</p>\n</blockquote>\n</li>\n<li><p>LAN(Local Area Networks)</p>\n<blockquote>\n<p>比PAN大，覆盖范围约1KM左右，通常覆盖一个校园、一个单位或者一个建筑。</p>\n</blockquote>\n</li>\n<li><p>MAN(Metropolitan Area Networks)</p>\n<blockquote>\n<p>比LAN大，覆盖约10KM范围，通常覆盖一个城市</p>\n</blockquote>\n</li>\n<li><p>WAN(Wide Area Networks)</p>\n<blockquote>\n<p>比MAN大，覆盖范围约100KM/1000KM，通常覆盖一个国家、一个州。</p>\n</blockquote>\n</li>\n<li><p>Internet</p>\n<blockquote>\n<p>最大的网络，覆盖范围约10000KM，覆盖全球。</p>\n</blockquote>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"参考模型\"><a href=\"#参考模型\" class=\"headerlink\" title=\"参考模型\"></a>参考模型</h3><ul>\n<li><p>网络分层的优点：</p>\n<ul>\n<li>各层工作独立，层之间同过接口联系，降低协议工作的复杂程度</li>\n<li>灵活性好，任何一层的改变不影响其它层</li>\n<li>每层的实现技术可以不同，减少实现的复杂度</li>\n<li>易于维护，每层可以单独进行调试</li>\n<li>便于标准化</li>\n</ul>\n</li>\n<li><p>分层交流模型：</p>\n<blockquote>\n<p><img src=\"/2023/03/03/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/%E8%80%83%E7%A0%94/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/image-20230205135027022.png\" alt=\"image-20230205135027022\"></p>\n</blockquote>\n</li>\n<li><p>分层原则：</p>\n<blockquote>\n<p>信宿机第n层收到 的对象和应与信源机第n层发出的对象完全一致</p>\n</blockquote>\n</li>\n<li><p>典型的分层模型：</p>\n<ul>\n<li><p><strong>OSI 七层模型</strong>:</p>\n<blockquote>\n<p>OSI (Open System Interconnection 开放系统互连) 是 ISO (International Standards Organization)在1983年提出的。</p>\n<ul>\n<li><p><strong>7 Application（应用层）：</strong></p>\n<blockquote>\n<p>主要为各种各样的网络应用（如 Email、FTP、微信等）提供网络服务。</p>\n</blockquote>\n</li>\n<li><p><strong>6 Presentation（表示层）：</strong></p>\n<blockquote>\n<p>将信息表示为一定形式和格式的数据流、压缩解压缩、加密解密等都是这层的任务。</p>\n</blockquote>\n</li>\n<li><p><strong>5 Session（会话层）：</strong></p>\n<blockquote>\n<p>负责通信主机间的会话的建立，管理和拆除；协调双方的会话。</p>\n</blockquote>\n</li>\n<li><p><strong>4 Transport（传输层）：</strong></p>\n<blockquote>\n<p>是参考模型上的核心层之一，它负责通信主机间的端到端连接；</p>\n<p>对于TCP来说，还负责提供可靠传输、差错恢复、拥塞控制等额外的功能。</p>\n</blockquote>\n</li>\n<li><p><strong>3 Network（网络层）：</strong></p>\n<blockquote>\n<p>是另外一个核心层，它的功能可用<strong>地址</strong>（为通行主机提供标识）和<strong>最优路径</strong>（最优路径是说路由，寻径，每一个中间设备都为到达的分组找到一根最优的路径，并送出）来描述；它负责将每一个分组从源机一路送达目的机。</p>\n</blockquote>\n</li>\n<li><p><strong>2 Data Link（数据链路层）：</strong></p>\n<blockquote>\n<p>主要提供介质访问服务，通过物理地址识别通信主机，提供可靠的帧传输并做差错控制，流控等。</p>\n</blockquote>\n</li>\n<li><p><strong>1 Physical（物理层）：</strong></p>\n<blockquote>\n<p>提供透明的比特流传输，可以是光信号、电信号、无线信号，物理层只关心比特流的传递，而忽略比特流里面的具体内容。</p>\n</blockquote>\n</li>\n</ul>\n<!--每一层都完成特定的功能，都利用下层的服务为上层提供服务，除了第1层和第7层-->\n</blockquote>\n</li>\n<li><p>TCP/IP(DoD)四层模型</p>\n<ul>\n<li>4 Application（应用层）：</li>\n<li>3 Transport（传输层）：</li>\n<li>2 Internet（网络层）：</li>\n<li>Network Access（网络接入层）：</li>\n</ul>\n</li>\n<li><p>OSI模型和DoD模型比较：</p>\n<ul>\n<li>相同点：<ul>\n<li>都分层</li>\n<li>都有应用层，但服务有所不同</li>\n<li>都有可以比较的传输层和网络层</li>\n<li>使用的分组交换而不是电路交换技术</li>\n</ul>\n</li>\n<li>不同点：<ul>\n<li>TCP/IP将表示层和会话层包含到了应用层</li>\n<li>TCP/IP将OSI的数据链路层和物理层合为一层中</li>\n<li>TCP/IP更简洁，但OSI更易开发和排除故障</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"数据传输\"><a href=\"#数据传输\" class=\"headerlink\" title=\"数据传输\"></a>数据传输</h3><ul>\n<li><p><strong>协议数据单元</strong>（PDU: Protocol Data Unit）:</p>\n<blockquote>\n<p>数据在各层的形式（或者说各层处理的数据对象），每一层的名称有所不同</p>\n<ul>\n<li>信息（Information，应用层）</li>\n<li>数据流（Data stream，上三层）</li>\n<li>数据段（Segment，传输层）</li>\n<li>分组（Packet，网络层）</li>\n<li>帧（Frame，数据链路层）</li>\n<li>比特流（Bits，物理层）</li>\n</ul>\n</blockquote>\n<ol>\n<li><p><strong>发放方</strong> 的 <strong>封装/打包</strong>：</p>\n<blockquote>\n<p> 将信息打包，从最高层——应用层开始逐渐下行到最底层——物理层。每一层上，数据都被加上头部信息，用于传递信息。</p>\n<p> 具体来说，在OSI的上三层，信息被表示为一定格式和形式的数据流（Datastream），数据流被传到传输层，将其切割为适合传输的数据段（Segment）并加上段头，段头中包含定位应用进程的端口号等信息</p>\n</blockquote>\n</li>\n<li><p><strong>收方</strong> 的 <strong>解封装/解包</strong>：</p>\n<blockquote>\n<p>将收到的比特流解包，从最底层——物理层开始，逐渐上行到最高层——应用层，提取出信息。解封装的过程是封装的逆向过程，在每层去掉头部信息，最终还原出应用层的输出：信息</p>\n</blockquote>\n <!--任何一次通行总是以发方的封装开始，以收方的解封装结束-->\n</li>\n</ol>\n</li>\n<li><p>收发双发的数据流：</p>\n<blockquote>\n<p><img src=\"/2023/03/03/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/%E8%80%83%E7%A0%94/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/image-20230205152246881.png\" alt=\"image-20230205152246881\"></p>\n</blockquote>\n</li>\n</ul>\n<h2 id=\"物理层\"><a href=\"#物理层\" class=\"headerlink\" title=\"物理层\"></a>物理层</h2><h2 id=\"数据链路层\"><a href=\"#数据链路层\" class=\"headerlink\" title=\"数据链路层\"></a>数据链路层</h2><h2 id=\"介质访问控制子层\"><a href=\"#介质访问控制子层\" class=\"headerlink\" title=\"介质访问控制子层\"></a>介质访问控制子层</h2><h2 id=\"网络层\"><a href=\"#网络层\" class=\"headerlink\" title=\"网络层\"></a>网络层</h2><h2 id=\"传输层\"><a href=\"#传输层\" class=\"headerlink\" title=\"传输层\"></a>传输层</h2><h2 id=\"应用层\"><a href=\"#应用层\" class=\"headerlink\" title=\"应用层\"></a>应用层</h2>","site":{"data":{}},"cover":"/img/covers/5.jpg","cover_type":"img","excerpt":"","more":"<h1 id=\"计算机网络\"><a href=\"#计算机网络\" class=\"headerlink\" title=\"计算机网络\"></a>计算机网络</h1><h2 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h2><h3 id=\"基本概念\"><a href=\"#基本概念\" class=\"headerlink\" title=\"基本概念\"></a>基本概念</h3><ul>\n<li><p>网络（广义）</p>\n<blockquote>\n<p>广义上的网络，是类似的事物连接在一起，以提供某些功能。</p>\n</blockquote>\n</li>\n<li><p>计算机网络</p>\n<blockquote>\n<p>使用单一技术相互连接的自主计算机的互联集合；单台计算机具有独立自主性；</p>\n<p>连接介质可以是光纤、铜线，也可以是微博红外、卫星。</p>\n</blockquote>\n</li>\n<li><p>拓扑</p>\n<blockquote>\n<p>信道的分布方式</p>\n<p>常见的拓扑有：总线型、星型、环型、树型和网状</p>\n<p>最常见的是：总线拓扑和星型拓扑</p>\n<ul>\n<li><p>总部拓扑：</p>\n<blockquote>\n<p>主机挂接在总线上，相互直接通达；从主机上发出信号在总线上双向同时传输；</p>\n<p>所有的主机都可以接收到这个信号。</p>\n</blockquote>\n</li>\n<li><p>星型拓扑：</p>\n<blockquote>\n<p>主机都挂接在一个中心节点上；早期的中心节点有集线器充当，现在的中心节点主要由交换机充当。</p>\n<p>缺点：单点故障</p>\n</blockquote>\n</li>\n</ul>\n</blockquote>\n</li>\n<li><p>协议：</p>\n<blockquote>\n<p>一系列规则和约定的规范性描述，它控制网络中的设备之间如何进行信息交换</p>\n</blockquote>\n</li>\n<li><p>数字带宽：</p>\n<blockquote>\n<p>指在单位时间内流经的信息总量，基本单位为：比特每秒，即bps；常用Kbps、Mbps、Gbps，更大的还有T、P、E、Z、Y。（相邻等级间都是10^3倍）</p>\n</blockquote>\n</li>\n<li><p>吞吐量：</p>\n<blockquote>\n<p>指实际、可测到的带宽。是用户真真切切感受到的带宽，是能让用户感受网络好坏的<strong>重要指标</strong></p>\n<p>会受到多种因素的影响：</p>\n<ul>\n<li>网络设备和性能</li>\n<li>用网的时间</li>\n<li>网络拓扑</li>\n<li>用户数量</li>\n<li>用户计算机性能</li>\n<li>服务器</li>\n</ul>\n</blockquote>\n</li>\n<li><p>信息量（S）、带宽（BW）和传输时间（T）之间的关系：</p>\n<blockquote>\n<ul>\n<li><p>理想情况：$T = S / BW$</p>\n</li>\n<li><p>实际情况：$ T = S / P $(吞吐量)</p>\n</li>\n</ul>\n</blockquote>\n</li>\n<li><p>点到点和端到端：</p>\n<blockquote>\n<ul>\n<li><p>点到点（P2P）：</p>\n<blockquote>\n<p>信源机和信宿机之间的通行是由一段一段的直接相连的机器间的通信组成，机器间的直接连接叫做点对点连接。如：PC1 —— PC2</p>\n</blockquote>\n</li>\n<li><p>端到端：</p>\n<blockquote>\n<p>信源机和信宿机之间的之间通信，好象拥有一条直接的线路。</p>\n</blockquote>\n</li>\n</ul>\n</blockquote>\n</li>\n<li><p>计算机网络的分类：</p>\n<ul>\n<li><p>按传输介质分类：</p>\n<ul>\n<li>有线网络</li>\n<li>无线网络</li>\n</ul>\n</li>\n<li><p>按网络大小规模来划分：</p>\n<ul>\n<li><p>PAN(Personal Area Networks) </p>\n<blockquote>\n<p>最小的计算机网络，覆盖范围在1M左右</p>\n</blockquote>\n</li>\n<li><p>LAN(Local Area Networks)</p>\n<blockquote>\n<p>比PAN大，覆盖范围约1KM左右，通常覆盖一个校园、一个单位或者一个建筑。</p>\n</blockquote>\n</li>\n<li><p>MAN(Metropolitan Area Networks)</p>\n<blockquote>\n<p>比LAN大，覆盖约10KM范围，通常覆盖一个城市</p>\n</blockquote>\n</li>\n<li><p>WAN(Wide Area Networks)</p>\n<blockquote>\n<p>比MAN大，覆盖范围约100KM/1000KM，通常覆盖一个国家、一个州。</p>\n</blockquote>\n</li>\n<li><p>Internet</p>\n<blockquote>\n<p>最大的网络，覆盖范围约10000KM，覆盖全球。</p>\n</blockquote>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"参考模型\"><a href=\"#参考模型\" class=\"headerlink\" title=\"参考模型\"></a>参考模型</h3><ul>\n<li><p>网络分层的优点：</p>\n<ul>\n<li>各层工作独立，层之间同过接口联系，降低协议工作的复杂程度</li>\n<li>灵活性好，任何一层的改变不影响其它层</li>\n<li>每层的实现技术可以不同，减少实现的复杂度</li>\n<li>易于维护，每层可以单独进行调试</li>\n<li>便于标准化</li>\n</ul>\n</li>\n<li><p>分层交流模型：</p>\n<blockquote>\n<p><img src=\"/2023/03/03/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/%E8%80%83%E7%A0%94/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/image-20230205135027022.png\" alt=\"image-20230205135027022\"></p>\n</blockquote>\n</li>\n<li><p>分层原则：</p>\n<blockquote>\n<p>信宿机第n层收到 的对象和应与信源机第n层发出的对象完全一致</p>\n</blockquote>\n</li>\n<li><p>典型的分层模型：</p>\n<ul>\n<li><p><strong>OSI 七层模型</strong>:</p>\n<blockquote>\n<p>OSI (Open System Interconnection 开放系统互连) 是 ISO (International Standards Organization)在1983年提出的。</p>\n<ul>\n<li><p><strong>7 Application（应用层）：</strong></p>\n<blockquote>\n<p>主要为各种各样的网络应用（如 Email、FTP、微信等）提供网络服务。</p>\n</blockquote>\n</li>\n<li><p><strong>6 Presentation（表示层）：</strong></p>\n<blockquote>\n<p>将信息表示为一定形式和格式的数据流、压缩解压缩、加密解密等都是这层的任务。</p>\n</blockquote>\n</li>\n<li><p><strong>5 Session（会话层）：</strong></p>\n<blockquote>\n<p>负责通信主机间的会话的建立，管理和拆除；协调双方的会话。</p>\n</blockquote>\n</li>\n<li><p><strong>4 Transport（传输层）：</strong></p>\n<blockquote>\n<p>是参考模型上的核心层之一，它负责通信主机间的端到端连接；</p>\n<p>对于TCP来说，还负责提供可靠传输、差错恢复、拥塞控制等额外的功能。</p>\n</blockquote>\n</li>\n<li><p><strong>3 Network（网络层）：</strong></p>\n<blockquote>\n<p>是另外一个核心层，它的功能可用<strong>地址</strong>（为通行主机提供标识）和<strong>最优路径</strong>（最优路径是说路由，寻径，每一个中间设备都为到达的分组找到一根最优的路径，并送出）来描述；它负责将每一个分组从源机一路送达目的机。</p>\n</blockquote>\n</li>\n<li><p><strong>2 Data Link（数据链路层）：</strong></p>\n<blockquote>\n<p>主要提供介质访问服务，通过物理地址识别通信主机，提供可靠的帧传输并做差错控制，流控等。</p>\n</blockquote>\n</li>\n<li><p><strong>1 Physical（物理层）：</strong></p>\n<blockquote>\n<p>提供透明的比特流传输，可以是光信号、电信号、无线信号，物理层只关心比特流的传递，而忽略比特流里面的具体内容。</p>\n</blockquote>\n</li>\n</ul>\n<!--每一层都完成特定的功能，都利用下层的服务为上层提供服务，除了第1层和第7层-->\n</blockquote>\n</li>\n<li><p>TCP/IP(DoD)四层模型</p>\n<ul>\n<li>4 Application（应用层）：</li>\n<li>3 Transport（传输层）：</li>\n<li>2 Internet（网络层）：</li>\n<li>Network Access（网络接入层）：</li>\n</ul>\n</li>\n<li><p>OSI模型和DoD模型比较：</p>\n<ul>\n<li>相同点：<ul>\n<li>都分层</li>\n<li>都有应用层，但服务有所不同</li>\n<li>都有可以比较的传输层和网络层</li>\n<li>使用的分组交换而不是电路交换技术</li>\n</ul>\n</li>\n<li>不同点：<ul>\n<li>TCP/IP将表示层和会话层包含到了应用层</li>\n<li>TCP/IP将OSI的数据链路层和物理层合为一层中</li>\n<li>TCP/IP更简洁，但OSI更易开发和排除故障</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"数据传输\"><a href=\"#数据传输\" class=\"headerlink\" title=\"数据传输\"></a>数据传输</h3><ul>\n<li><p><strong>协议数据单元</strong>（PDU: Protocol Data Unit）:</p>\n<blockquote>\n<p>数据在各层的形式（或者说各层处理的数据对象），每一层的名称有所不同</p>\n<ul>\n<li>信息（Information，应用层）</li>\n<li>数据流（Data stream，上三层）</li>\n<li>数据段（Segment，传输层）</li>\n<li>分组（Packet，网络层）</li>\n<li>帧（Frame，数据链路层）</li>\n<li>比特流（Bits，物理层）</li>\n</ul>\n</blockquote>\n<ol>\n<li><p><strong>发放方</strong> 的 <strong>封装/打包</strong>：</p>\n<blockquote>\n<p> 将信息打包，从最高层——应用层开始逐渐下行到最底层——物理层。每一层上，数据都被加上头部信息，用于传递信息。</p>\n<p> 具体来说，在OSI的上三层，信息被表示为一定格式和形式的数据流（Datastream），数据流被传到传输层，将其切割为适合传输的数据段（Segment）并加上段头，段头中包含定位应用进程的端口号等信息</p>\n</blockquote>\n</li>\n<li><p><strong>收方</strong> 的 <strong>解封装/解包</strong>：</p>\n<blockquote>\n<p>将收到的比特流解包，从最底层——物理层开始，逐渐上行到最高层——应用层，提取出信息。解封装的过程是封装的逆向过程，在每层去掉头部信息，最终还原出应用层的输出：信息</p>\n</blockquote>\n <!--任何一次通行总是以发方的封装开始，以收方的解封装结束-->\n</li>\n</ol>\n</li>\n<li><p>收发双发的数据流：</p>\n<blockquote>\n<p><img src=\"/2023/03/03/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/%E8%80%83%E7%A0%94/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/image-20230205152246881.png\" alt=\"image-20230205152246881\"></p>\n</blockquote>\n</li>\n</ul>\n<h2 id=\"物理层\"><a href=\"#物理层\" class=\"headerlink\" title=\"物理层\"></a>物理层</h2><h2 id=\"数据链路层\"><a href=\"#数据链路层\" class=\"headerlink\" title=\"数据链路层\"></a>数据链路层</h2><h2 id=\"介质访问控制子层\"><a href=\"#介质访问控制子层\" class=\"headerlink\" title=\"介质访问控制子层\"></a>介质访问控制子层</h2><h2 id=\"网络层\"><a href=\"#网络层\" class=\"headerlink\" title=\"网络层\"></a>网络层</h2><h2 id=\"传输层\"><a href=\"#传输层\" class=\"headerlink\" title=\"传输层\"></a>传输层</h2><h2 id=\"应用层\"><a href=\"#应用层\" class=\"headerlink\" title=\"应用层\"></a>应用层</h2>"},{"title":"微机原理及接口技术","abbrlink":"bb496006","date":"2023-03-06T15:04:30.000Z","_content":"\n# 《微机原理及接口技术 吴宁》 学习记录 \n\n##  微信计算机基础\n\n### 微型计算机概述\n\n计算机系统是一种由硬件系统和软件系统组成的复杂电子装置，它能够存储程序、存储原始数据、中间结果和最终运算结果，并自动完成运算，是一种能对各种数字化处理的 “信息处理机”。微型计算机是一种小型的、可编程的计算机，它可以用于执行复杂的计算任务。它们通常由一个或多个微处理器组成，可以存储和处理大量的数据，并且可以运行复杂的程序。它们可以用于控制机器和设备，以及用于收集、处理和传输数据。\n\n#### 微机的发展\n\n自1946年第一台计算机问世已在，已经历了由电子管计算机、晶体管计算机、集成电路计算机到大规模、超规模集成电路计算机这样五代的更替、并且在不断地向巨型化、微型化、网络化和智能化这4个方向发展。\n\n##### 微机的工作过程\n\n* 冯·诺依曼计算机的主要特点\n\n<img src=\"微机原理及接口技术/微机原理及接口技术.jpg\" alt=\"微机原理及接口技术\" style=\"zoom:80%;\" />\n\n（1）将计算过程描述为由许多条指令按一定顺序组成的程序、并放入存储器保存 \n\n（2）程序中的指令和数据必须采用2进制编码，且能够被执行该程序的计算机所识别 \n\n（3）指令按其在存储器中存放的顺序执行，存储器的字长固定并按顺序线性编制 \n\n（4）由控制器控制整个程序和数据的存取以及程序的执行 \n\n（5）以运算器为核心，所有的执行都经过运算器 \n\n\n\n* 微型计算机的工作过程\n\n   微机的工作过程就是执行程序的过程，也就是逐条执行指令系列的过程。由于每条指令都包括取指令和执行指令两个基本阶段，所以微机的工作过程也是不断地去取指令和执行指令的过程。当计算机进入运行状态时： \n\n   （1）首先将第一条指令由内存取出 \n\n   （2）将取出的指令送给指令译码器译码，以确定要进行的操作 \n\n   （3）读取相应的操作数（即执行的对象）\n\n   （4）执行指令 \n\n   （5）存放执行结果 \n\n   （6）一条指令执行完毕，转到下一条指令的执行阶段\n\n#### 微机的特点\n\n运算速度快、计算精度高、记忆功能和逻辑判断功能强，可自动连续工作等基本特点。\n\n* 功能强、可靠性高 \n\n* 格低廉 \n\n* 系统设计灵活，适应性强 \n\n* 体积小，重量轻，维护方便\n\n### 微型计算机系统的组成\n\n#### 硬件系统\n\n##### CPU（微处理器或中央处理器）\n\n* **运算器**\n\n核心部件是算数逻辑单元（ALU，Arithmetic and Logic Unit）,在控制信号的作用下可完成加、减、乘、除四则运算和各种逻辑运算。新型CPU运算器还可完成浮点运算\n\n* **控制器**\n\n    一般由指令寄存器、指令译码器和 操作控制电路组成。是CPU的控制中心，它从存储器中依次取出程序的各条指令，并根据指令的要求，向微机的各个部件发出相应的控制信号。\n\n* **寄存器组**\n\n    实质上是CPU内部若干个存储单元，可分为专用寄存器和通用寄存器。 专用寄存器：堆栈指针、程序计数器、标志寄存器等 通用寄存器：.......\n\n* **存储器**\n\n    主机系统的存储器又叫做主存或内存，用以存放数据（包括原始数据、中间结果、最终结果）和当前执行的程序\n\n* **输入输出接口和输入输出设备**（I/O接口与I/O设备）\n\n    常用的输入设备有：键盘、鼠标器、扫描仪等 常用的输出设备有：显示器、打印机、绘图仪等 I/O接口：I/O设备之间信息交换的桥梁\n\n* **总线**\n\n    数据总线DB：传输数据信息、双向 地址总线AB：传送CPU发出的地址信息、单向，指明与CPU交换信息的内存单元或者I/O设备 控制总线CB：传送控制信号、时序信号和状态信息等，CB中的每根线是单向的，但CB整体是双向的\n\n#### 软件系统\n\n##### 系统软件\n\n系统软件包括操作系统（OS）和系统实用程序。 操作系统：用于管理计算机的硬件和软件资源、进行任务调度、提供文件管理系统、人机接口等，包含了各种I/O设备的驱动程序 系统实用程序： 包括各种高级语言的翻译/编译程序、汇编程序、数据库系统、文本编辑程序以及诊断和调试程序，此外还包括许多系统工具程序等。\n\n##### 应用软件\n\n应用软件是用户为解决各种实际问题（如数学计算、检测与实时控制、音乐播放等）而编制的程序。\n\n### 数制与编码\n\n#### 进制\n\n* **十进制**：所有数都用0~9这10个符号的组合来表示，用D标识，逢十进一 \n\n* **二进制**：每一位只取0和1两个数字符号，用B标识，逢二进一 \n\n* **十六进制**：每一位数都用0 -9和A~F这10个字符来组合，用H标识，逢十六进一\n\n#### 进制转换\n\n* 十进制 ==\\> 二进制：\n\n    > 整数部分：除“2”取余，直到商为0；\n    >\n    > 小数部分：乘“2”取余，直到满足精度要求 \n\n​\t\t\n\n* 十进制 ==\\> 十六进制：** \n\n    > 整数部分：除“16”取余，直到商为0；\n    >\n    > 小数部分：乘“16”取余，直到满足进度要求 \n    >\n\n* 二进制 ==> 十进制 && 二进制 ==\\> 十六进制：**\n\n    > 按权展开\n\n#### BCD码\n\n计算机用二进制数编码来表示十进制数，常见的有用四位二进制来表示一位十进制 \n\n* **非压缩BCD码：**\n\n    > 用一个字节（8位）来表示一位十进制，高四位清零\n\n* **压缩BCD码：**\n\n    > 用一个字节（8位）来表示两位十进制\n\n#### 二进制数的运算\n##### **算术运算：**\n\n> **加：**逢二进一\n>\n> **减：**借一当二 \n>\n> **乘除：**\n\n##### **逻辑运算：**\n\n> **与：**有一个0即为0\n>\n> **或：**有一个1即为1 \n>\n> **非：**取反\n>\n> **异或：**相同为0，不同为1\n\n#### 带符号在计算机中的表示\n\n* **机器数：**连同书的符号一起数字化了的数据称为机器数。例如：X = +91 = 01011011B\n\n* **真值：**与机器数相应的用正、负符号加绝对值来表示的世界数值。例如：X = +91 = +1011011B \n\n* **原码：**是一种简单直观的机器数表示方法 反码：正数不变，负数符号位不变，取反加1 \n\n* **补码：**正数不变，负数符号位不变，取反加1\n\n## 8086微处理器\n\n### 微处理器概述\n\n#### 运算器：\n\n* **单总线结构运算器**：通过一条内部总线传递信息\n* **双总线结构运算器**：通过两条内部总线传递信息\n* **三总线结构运算器**：通过三条内部总线传递信息\n\n#### 控制器：\n\n##### 基本功能：\n\n> 指令控制、时序控制、操作控制功能\n>\n> 对异常情况及某些外部请求的处理能力，如运算溢出、中断请求等\n\n##### 组成部分：\n\n* **程序计数器**：存放下一条要执行的指令在存储器中的地址，程序执行前应将程序的首地址置于程序计数器\n\n* **指令寄存器（Instruction Register, IR）**：存放从存储器中取出的待执行的指令\n\n* **指令译码器（Instruction Decoder, ID）**：“ 翻译 ”指令寄存器中的指令，即指令译码\n\n* **时序控制部件**：产生计算机工作中所需的各种时序信号\n\n* **微操作控制部件**：产生与各条指令对应的微操作\n\n\n\n### 8088CPU的外部引脚及其功能\n\n#### 最小模式和最大模式\n\n* 最小模式：\n\n    > 是8088微处理器的一种操作模式，它只使用一个段寄存器来指定内存地址，这种模式下，8088只能使用1KB的内存空间。\n\n* 最大模式：\n\n    > 是8088微处理器的另一种操作模式，它使用两个段寄存器来指定内存地址，这种模式下，8088可以使用1MB的内存空间。\n\n### 8088/8086CPU的功能结构\n\n#### 内部结构\n\n* `EU`（执行单元）：\n\n    > 执行命令、分析命令、暂存中间运算结果并保留结果的特征。由ALU、通用寄存器、标志寄存器和EU控制电路组成。\n\n* `BIU`（总线接口单元）：\n\n    > BIU负责CPU与存储器、I/O接口之间的信息传递。由段寄存器、指令寄存器、指令队列、地址加法器以及总线控制逻辑组成\n\n#### 内部寄存器\n\n##### 通用寄存器（8个）\n\n* **数据寄存器**：AX、BX、CX、DX\n\n    > **`AX`：**累加器，常用于存放算术逻辑运算中的操作数，所有的I/O指令都使用累加器与外设接口传送信息\n    >\n    > **`BX`：**基址寄存器，常用来存放访问内存时的基地址\n    >\n    > **`CX`：**计数寄存器，在循环和串操作指令中用作计数器\n    >\n    > **`DX`：**数据寄存器，在寄存器间接寻址的I/O指令中存放I/O端口的地址。\n    > 注：在做双字长乘除运算时，DX与AX合起来存放一个双字长数（32位），其中**DX存放高16位，AX存放低16位。**\n\n* **地址指针寄存器**：SP、BP\n\n    > **`SP` (Stack  Pointer)：** 堆栈指针寄存器，堆栈操作中用来存放栈顶偏移地址，永远指向堆栈的栈顶\n    > **`BP` (Base Pointer)：** 基址指针寄存器，常用来存放访问内存时的基地址，通常与SS搭配使用\n\n* **变址寄存器：SI、DI**\n\n    > `SI` (Source Index)：源变址寄存器\n    >\n    > `DI`： 目的变址寄存器\n\n* **段寄存器**（4个）\n\n    > **段寄存器用于存放短地址，段起始地址的高16位**\n    >\n    > * `CS`： 代码段寄存器\n    > * **`SS`：**堆栈段寄存器\n    > * **`DS`：**数据段寄存器\n    > * **`ES`：** 附加段寄存器\n\n* **控制寄存器**（2个）\n\n    > `IP`（指令指针寄存器）\n    >\n    > 用于存放预取指令的偏移地址。CPU取指令时总是以CS为段基址，以IP为段内偏移地址。\n    > CPU从CS段中偏移地址为IP的内存单元中取出指令代码的一个字节后，IP自动加1，只想指令代码的下一个字节。\n    > 用户程序不能直接访问IP\n\n * `FLAGS`（标志位寄存器）\n\n    * 状态标志位\n\n        > **主要作用：** 记录算术和逻辑运算结果的一些特征\n        > CF：进位标志位，加（减）运算时，最高位向更高位有进（借）位时，`CF = 1`，否则，`CF =0`\n        > PF：奇偶标志位，运算结果低8位中1的个数为偶数时`PF = 1`，为奇数时，`PF =0`\n        > AF：辅助进位标志位，$D_3D3D_3D3$向$D_4D4D_4D4$ 有进（借）位时`AF=1`，否则`AF=0`\n        > ZF：零标志位，运算结果为0时`ZF = 1` ,否则`ZF = 0`\n        > SF：符号标志位，运算结果的最高位为1时 `SF = 1`，否则 `SF = 0`\n        > OF：溢出标志位，有溢出`OF = 1`,无溢出`OF = 0`\n\n    * 控制标志位TF\n\n        > TF：陷阱标志位，`TF = 1`时激活处理器的调试特性，使CPU处于单步执行指令的工作方式。每执行一条指令自动产生一次单步中断，从而达到检查程序的目的。\n        > IF：中断允许标志位，`IF = 1`，CPU可响应可屏蔽中断请求；`IF = 0`，CPU禁止响应可屏蔽中断请求。对不可屏蔽中断及内部中断没有影响。\n        > DF：方向标志位，执行串操作指令时控制操作的方向。`DF = 1`，向减地址方向进行，即从高地址开始，每进行一次操作，地址指针自动减1（或减2）；`DF = 0`，则按增地址方式进行。\n\n## 寻址方式\n\n### 立即寻址\n\n> 只针对源操作数，此时源操作数是一个立即数（8位或16位），存放于内存的代码段中。\n> 当立即数为16位时高地址存放高8位，低地址存放低8位。\n> 例如：`MOV AX, 3120H` （20H——> AL; 31H ——> AH）\n\n### 直接寻址\n\n> 表示参数运算的数据存放在内存中，存放的地址由指令直接给出，即指令中的操作数时存储器操作数（带有'[ ]'）；\n>\n> `\"[ ]\"`内用16位常数表示存放数据的偏移地址，数据的段基地址默认位数据段，可以允许重设。\n\n###  存储器寻址\n\n> 指令的操作数为CPU的内部寄存器，可以是数据寄存器（8位或16位），也可以是地址指针、变址寄存器或段寄存器。 指令操作码存放在代码段，操作数在内部寄存器中，指令执行时不必通过访问内存就能取得操作数，执行速度较快 例如：`MOV SI, AX`   （将AX中的内容送到寄存器SI中）\n\n###  寄存器间接寻址\n\n> 用寄存器的内容表示操作数的偏移地址。 存放操作数偏移地址的寄存器只允许是SI、DI、BX和BP，他们可简称位间接寄存器或地址指针。 默认情况下，选择SI、DI、BX，操作数在数据段，段地址由DS决定；选择BP作间址寄存器，则操作数在堆栈段，段地址由SS决定。 无论哪个简介寄存器都允许重设 指令中 间接寄存器要加 [ ] 例如：`MOV AX, [SI]`（将数据段（DS）中以SI为偏移地址的单元中的内容送到AX中） 若操作数存放在附加段，则指令应为：`MOV AX, ES:[SI]`\n\n###  寄存器先对寻址\n\n> 操作数在内存中存放地址（偏移地址）由间址寄存器的内容加上指令中的一个8位或者16位的位移量组成。 操作数所在段由所使用的间址寄存器决定（规则与寄存器间接寻址方式相同） 例如：`MOV AX, DATA[BX]`  （执行完后AX的物理地址为 DS*16 + BX+DATA）\n\n###  基址、变址寻址\n\n> 基址-变址寻址方式由一个基址寄存器（BX或BP）的内容和一个变址寄存器（SI或DI）的内容相加而形成操作数的偏移地址。 默认情况下，若用BX作为基址寄存器，则段地址在DS中；如果用BP作为基址寄存器，则段地址在SS中。 允许段重设 使用基址-变址寻址方式，不允许同时出现两个基址寄存器或两个变址寄存器。 例如：`MOV AX, [BX][SI]`  （执行后AL的物理地址 = DSx16 + BX+SI， AH的物理地址 = DS x16 + BX+SI +1 ）\n\n###  基址、变址、相对寻址\n\n> 基址-变址-相对寻址方式是基址-变址寻址方式的扩充，指令中指定一个基址寄存器和一个变址寄存器，同时还各处一个8为或者16位的位移量。 操作数的偏移地址 等于 三者之和 默认情况下，段寄存器由基址寄存器决定 允许段重设 同样，不允许同时出现两个基址寄存器或两个变址寄存器。 例如：`MOV AX, 5[DI][BX]`  （段寄存器为DS，偏移地址为BX + DI + 5 的连续两个单元的内容送到AX中）\n\n###  隐含寻址\n\n> 操作数隐含在指令码中 例如：`MUL` （AL x BL ——>AX）\n\n## **指令系统**\n\n### **指令的概述**\n\n#### CISC指令系统\n\n> CISC(Complex Instruction Set Computer)，复杂指令系统计算机。\n> CISC指令的设计目标是增强指令的功能，将一些原来用软件是西安的、常用的功能变成用硬件的指令系统来实现。\n\n* **缺点**\n\n    > 难以使用\n\n* **优点**：\n\n    > 指令经编译后生成的指令程序较小、执行起来较快、节省硬件资源、存取指令的次数少、占用较少的存储器。\n\n* **存在的三个方面的问题**：\n\n    > 1. “8020规律”：20%的指令在各种应用程序中出现频率占整个指令系统的80%。\n    > 2. CISC指令系统中有大量的复杂指令，控制逻辑极不规整，给VLSI（超大规模集成电路）工艺造成了极大的困难\n    > 3. CISC增加了许多复杂指令，这些指令虽然简化了目标程序、缩小了高级语言与机器语言之间的差距，但使程序总的执行时间变长、硬件的复杂度增加。\n\n#### RISC指令系统\n\n> RISC，精简指令系统计算机。\n> 一种计算机体系结构的设计思想，不是产品\n> 核心思想是通过简化指令来使计算机的结构更加简单、合理，从而提高CPU的运算速度。\n\n##### RISC的特点：\n\n> * 大多数指令在一个计算机周期内完成\n> * 指令系统中应尽量减少访问存储器的指令，而采用寄存器与寄存器之间的操作\n> * 减少寻址方式的种类。复杂的寻址方式用简单的寻址方式合成\n> * 减少指令的种类。复杂的指令用软件实现\n> * 指令格式简单\n>\n\n### **8086指令系统**\n\n#### 指令概述\n\n##### 指令\n\n指令的一般格式为  操作码 目标操作数 源操作数\n\n* **零操作数指令**：\n\n    > 形式上只有操作码，操作数是隐含存在的，操作对象通常为处理器本身\n\n* **单操作数指令**：\n\n    > 指令中仅给出一个操作数，另一个操作数隐含存在\n\n* **双操作数指令**：\n\n    > 一般格式\n\n##### 操作数\n\n* **立即操作数*：\n\n    > 立即数是指具有固定数值的操作数，即常数，不因指令的执行而发生变化。8086系统中字长可以是1字节或者2字节；可以是无符号数或有符号数\n\n* **寄存器操作数**：\n\n    > 8086CPU的8个通用寄存器和4个段寄存器可以作为指令中的寄存器操作数，源操作数和目标操作数均可\n\n* 存储器操作数：\n\n    > 其含义是参加运算的数据是存放在内存中的。通常为8位或16位字长\n\n#### 数据传送指令\n\n##### 通用数据传送指令\n\n- **一般数据传送指令**\n\n    > 原则：\n    > （1）两操作数字长必须相同\n    > ​（2）两操作数不允许同时为存储器操作数\n    > ​（3）两操作数不允许同时为段寄存器\n    > ​（4）在源操作数是立即数时，目标操作数不能是段寄存器\n    > ​（5）IP和CS不作为目标操作数，FLAGS一般也不作为操作数在指令中出现\n    >\n    > * `MOV`\n    >\n    >     > * 格式：`MOV dest, src`\n    >     > * 执行过程：`dest ——> src`\n    >     > * 举例：：`MOV AL, BL`\n\n- **堆栈操作指令**\n\n    > 原则：\n    > （1）先进后出\n    > （2）以字为单位\n    > （3）指令操作数必须是16位\n    > （4）操作数可以是寄存器或存储器两单元，但不能是立即数\n    > （5）不能从栈顶弹出一个字给CS\n    > （6）PUSH和POP指令在程序中一般成对出现\n    > （7）PUSH指令的操作方向是从高地址向低地址，而POP指令的操作正好相反\n    >\n    > * `PUSH`（压栈指令）：\n    >\n    >     > `OPRD`：16存储器或存储器两单元\n    >     >\n    >     > * 格式：`PUSH OPRD`\n    >     >\n    >     > * 执行过程\n    >     >\n    >     >     > `SP -2 ——>SP`\n    >     >     > 操作数高字节 ——> `SP+1`\n    >     >     > 操作数低字节 ——>`SP`\n\n- **`POP`（出栈指令）：**\n\n    > - 格式：`POP OPRD`\n    >\n    > - 执行过程：\n    >     \n    >     > `SP` ——> 操作数低字节\n    >     > `SP+ 1` ——> 操作数高字节\n    >     > `SP` <—— `SP +2`\n\n- **交换指令**\n\n    > 原则：\n    > （1）两操作数必须有一个是寄存器操作数\n    > （2）不允许使用段寄存器\n    >\n    > * `XCHG`\n    >\n    >     > * 格式：`XCHG REG, MEM/REG`\n    >     >\n    >     > * 操作：\n    >     >     `REG <——> MEM/ REG`\n    >     >\n    >     > * 例子：\n    >     >     `XCHG AX, BX`\n    >     >     `XCHG [2000],CL`\n- **查表转换指令**\n\n    > 原则：\n    > （1）零操作数指令\n    > （2）用BX的内容代表表格首地址，AL内容为表内偏移量，BX+AL得到要查找元素的偏移地址\n    >\n    > * XLAT\n    >\n    >     > * 格式：XLAT\n    >     >\n    >     > * 操作：\n    >     >\n    >     >     > 将BX+AL所指单元的内容送给AL\n- **字位扩展指令**\n\n    > 原则：\n    > （1）将符号数的符号位扩展到高位\n    > （2）指令为零操作数，采用隐含寻址，隐含的操作数为AX及AX，DX\n    > （3）无符号数的扩展规则为在高位补0\n    >\n    > - `CBW`（字节到字的扩展指令）\n    >\n    >     > - 格式：`CBW`\n    >     >\n    >     > - 操作：\n    >     >     \n    >     >     > 将AL内容扩展到AX\n    >     >     > 若最高位 = 1，则执行后 AH =FFH\n    >     >     > 若最高位 = 0，则执行后AH = 00H\n    >\n    > - `CWD`（字到双子的扩展指令）\n    >\n    >     > * 格式：`CWD`\n    >     >\n    >     > * 操作：\n    >     >\n    >     >     > 将 `AX` 内容扩展到 `DX` `AX`\n    >     >     >\n    >     >     > 若最高位 = 1，则执行后DX = FFFFH\n    >     >     >\n    >     >     > 若最高位 = 0，则执行后DX = 0000H\n\n##### **输入输出指令**\n\n* 输入指令\n\n    * IN\n\n        * 指令：`IN acc, PORT`\n\n        * 操作：\n\n            > 从端口地址读入数据到累加器中\n\n* 输出指令\n\n    * OUT\n\n        * 指令：`OUT PORT, acc`\n\n        * 操作：\n\n            > 将累加器的值输出到端口中\n\n##### **地址传送指令**\n\n* **取近地址指令**\n\n    * `LEA`：\n\n        * 格式：`LEA REG, MEM`（必须是存储器操作数）\n\n        * 操作：\n\n            > 将变量得16位偏移地址写入到目标寄存器\n\n        * 要求：\n\n            > 源操作数必须是一个存储器操作数，目标操作数通常是间址\n\n* **取远地址指令**\n\n    * `LDS`：\n\n        * 格式：`LDS`  通用寄存器，存储器操作数\n\n        * 操作：\n\n            > 将源操作数得偏移地址送目标寄存器，将源操作数得地址送DS\n\n    * `LES`：\n\n        * 格式：`LES` 通用寄存器，存储器操作数\n\n        * 操作：\n\n            > 将源操作数得偏移地址送目标寄存器，将源操作数得地址送ES\n\n##### **标志传送指令**\n\n* 隐含操作数AH\n\n    * `LAHF`(Load AH from Flags)\n\n        * 格式：`LAHF`\n\n        * 操作：\n\n            > 将`FLAGS`的低8位装入`AH`\n\n    * `SAHF`(Store AH into Flags)\n\n        * 格式：`SHAF`\n\n        * 操作：\n\n            > 与`LAHF`相反\n\n* 隐含操作数FLAGS\n\n    * PUSHF(Push flags onto stack)\n    * POPF(Pop flags off stack)\n\n#### **算术运算指令**\n\n##### 加法运算指令\n\n* `ADD`（普通加法指令）\n\n    **ADD指令的执行对全部6个状态标志位都产生影响**\n\n    * 格式：`ADD OPRD1, OPRD2`\n\n    * 操作:\n\n        > `OPRD1 + OPRD2 ——> OPRD1`\n\n    * 例子：\n\n        > `ADD AL, 99H`（）\n\n* `ADC`（带进位的加法指令）\n\n    **ADC指令都用于多字节数相加，使用前要先将CF清零**\n\n    * 格式：`ADC OPRD1, OPRD2`\n\n    * 操作：\n\n        > `OPRD1 + OPRD2 + CF ——> OPRD1`\n\n* `INC`（加1指令）\n\n    **常用于程序中修改地址指针**\n\n    * 格式：`INC OPRD`\n\n    * 操作：\n\n        > `OPRD + 1 ——> OPRD`\n\n##### 减法运算指令\n\n* `SUB` （普通减法指令）\n\n    **对标志位的影响与ADD指令相同**\n\n    * 格式：`SUB OPRD1, OPRD2`\n\n    * 操作：\n\n        > `OPRD1 - OPRD2 ——> OPRD1`\n\n* `SBB`（考虑借位的减法指令）\n\n    * 格式：`SBB OPRD1, OPRD2`\n\n    * 操作：\n\n        > `OPRD1 - OPRD2 - CF——> OPRD1`\n\n* `DEC`（减1指令DEC）\n\n    **指令对操作数的要求与INC相同，指令常用于在程序中修改计数值**\n\n    * 格式：`DEC OPRD`\n\n    * 操作：\n\n        > `OPRD -1 ——> OPRD`\n\n    * 应用程序例子：\n\n        ![image-20230308223135359](微机原理及接口技术/image-20230308223135359.png)\n\n* `CMP`（比较指令）\n\n    用于比较两个数的大小，可作为条件转移指令转移的条件\n\n    指令执行的结果不影响目标操作数，仅影响标志位！\n\n    对操作数的要求及对标志位的影响与SUB指令相同\n\n    * 格式：`CMP OPRD1, OPRD2`\n\n    * 操作：\n\n        > `OPRD1 - OPRD2`\n\n    * 例子：\n\n        * 两个无符号数的比较\n\n            > `CMP AX, BX`\n            >\n            > 若 AX >= BX ——> CF =0\n            >\n            > 若AX < BX ——> CF = 1\n            >\n            > 若 AX = BX ——> CF =1,ZF =1\n\n        * 两个带符号数的比较\n\n            > CMP AX, BX\n            >\n            > 两个数的大小由OF和SF共同决定\n            >\n            > OF和SF状态相同 AX >= BX\n            >\n            > OF和SF状态不同 AX < BX\n\n* `NEG`（求补指令）\n\n    **对一个负数取补码就相当于用零减去此数**\n\n    * 格式：`NEG OPRD`\n\n    * 操作：\n\n        > `0 - OPRD ——> OPRD`\n\n    * 说明：\n\n        * 执行NEG指令后，一般情况下都会使CF为1，除非给定的操作数为零才会使CF为0；\n        * 当指定的操作数的值为80H(-128)或为8000H(-32768)，则执行NEG指令后，结果不变，但OF置1，其它情况下OF置0。\n\n##### 乘法指令\n\n乘法指令采用隐含寻址，隐含的是存放被乘数的累加器AL或AX及存放结果的AX，DX\n\n* **`MUL`无符号的乘法指令**\n\n    * 格式：`MUL OPRD`（OPRD不能是立即数）\n\n    * 操作：\n\n        > OPRD为8字节数 ===> AL x OPRD ——> AX\n        >\n        > OPRD为16位数 ====> AX x OPRD ——> DXAX \n\n    * 例子：\n\n        > ![image-20230308232215011](微机原理及接口技术/image-20230308232215011-16782889368582.png)\n\n* **`IMUL`带符号的乘法指令**\n\n    * 格式：`IMUL OPRD`（OPRD不能是立即数，隐含操作数为AL，存放在AX中）\n\n    * 操作：\n\n        > 1. 将两个操作数取补码（对负数按位取反加1，正数不变）\n        > 2. 做乘法运算\n        > 3. 将乘积按位取反加 1\n\n##### 除法指令\n\n指令要求被除数是除数的双倍字长\n\n* **无符号除法**\n\n    * 格式：`DIV OPRD`\n\n    * 操作：\n\n        > 若`OPRD`是字节数：\n        >\n        > * 执行：`AX/OPRD`\n        > * 结果：AL = 商   AH = 余数\n        >\n        > 若`OPRD`是双字节数：\n        >\n        > * 执行：`DXAX/OPRD`\n        > * 结果：AX = 商 DX = 余数\n\n* **有符号除法**\n\n    * 格式：`IDIV OPRD`\n\n    * 操作：\n\n        > 若OPRD是字节数：\n        >\n        > * 执行：`AX/OPRD`\n        > * 结果：AL = 商   AH = 余数\n        >\n        > 若OPRD是双字节数：\n        >\n        > * 执行：`DXAX/OPRD`\n        > * 结果：AX = 商 DX = 余数\n\n#### **逻辑运算和移位指令**\n\n##### 逻辑运算指令\n\n> * **对操作数的要求**：\n>     * 大多数与`MOV`指令相同\n>     * “非” 运算指令要求操作数不能是立即数\n> * **对标志位的影响**\n>     * 除“非”运算指令，其余指令的执行都会影响除AF外的五个状态标志\n>     * 无论执行结果如何，都会使标志位`OF=CF=0`\n>     * “非”运算指令的执行不影响标志位\n\n* **\"与\"指令**\n\n    * 格式：`AND OPRD1，OPRD2`\n\n    * 操作：\n\n        > 两操作数相“与”，结果送目标地址。\n\n    * 应用：\n\n        * 实现两操作数按位相与的运算：`AND BL, [SI]`\n        * 使目标操作数的某些位不变，某些位清零：`AND AL, 0FH`\n        * 在操作数不变的情况向使CF和OF清零：`AND AX, AX`\n\n* “或”运算指令\n\n    * 格式\n    * 操作\n    * 应用\n\n* “非”运算指令\n\n    * 格式：NOT OPRD\n\n    * 操作：\n\n        > 操作数按位取反在送回原地址，指令的执行对标志位无影响\n\n    * 例子：\n\n        > \n\n* “异或”运算指令\n\n    * 格式：`XOR OPRD1, OPRD2`\n\n    * 操作：\n\n        > 两操作数“异或”，结果送目标地址\n\n    * 例子：\n\n* “测试”指令\n\n    * 格式：`TEST OPRD1, OPRD2`\n\n    * 操作：\n\n        > 执行“与”运算，但运算的结果不送回目标地址\n\n    * 应用：\n\n        > 常用于测试某些位的状态\n\n#### 串操作指令\n\n#### 程序控制指令\n\n#### 处理器控制指令\n\n## **汇编语言程序设计**\n\n### **伪指令**\n\n### **BIOS和DOS功能调用**\n\n### **汇编语言程序设计基础**\n\n> #### **程序设计概述**\n>\n> #### **顺序结构**\n>\n> #### **分支结构**\n>\n> #### **循环结构**\n>\n> #### **子程序**\n>\n> #### **常用程序设计举例**\n>\n\n## **存储器系统**\n\n### **随机存取存储器**\n\n### **只读存储器**\n\n### **高速缓冲存储器**\n\n### **存储器扩展技术**\n\n","source":"_posts/学习记录/考研/微机原理及接口技术.md","raw":"---\ntitle: 微机原理及接口技术\ncategories:\n  - 学习记录\n  - 考研\ntags:\n  - 微机原理及接口技术\n  - 自动化\n  - 控制科学与工程\nabbrlink: bb496006\ndate: 2023-03-06 23:04:30\n---\n\n# 《微机原理及接口技术 吴宁》 学习记录 \n\n##  微信计算机基础\n\n### 微型计算机概述\n\n计算机系统是一种由硬件系统和软件系统组成的复杂电子装置，它能够存储程序、存储原始数据、中间结果和最终运算结果，并自动完成运算，是一种能对各种数字化处理的 “信息处理机”。微型计算机是一种小型的、可编程的计算机，它可以用于执行复杂的计算任务。它们通常由一个或多个微处理器组成，可以存储和处理大量的数据，并且可以运行复杂的程序。它们可以用于控制机器和设备，以及用于收集、处理和传输数据。\n\n#### 微机的发展\n\n自1946年第一台计算机问世已在，已经历了由电子管计算机、晶体管计算机、集成电路计算机到大规模、超规模集成电路计算机这样五代的更替、并且在不断地向巨型化、微型化、网络化和智能化这4个方向发展。\n\n##### 微机的工作过程\n\n* 冯·诺依曼计算机的主要特点\n\n<img src=\"微机原理及接口技术/微机原理及接口技术.jpg\" alt=\"微机原理及接口技术\" style=\"zoom:80%;\" />\n\n（1）将计算过程描述为由许多条指令按一定顺序组成的程序、并放入存储器保存 \n\n（2）程序中的指令和数据必须采用2进制编码，且能够被执行该程序的计算机所识别 \n\n（3）指令按其在存储器中存放的顺序执行，存储器的字长固定并按顺序线性编制 \n\n（4）由控制器控制整个程序和数据的存取以及程序的执行 \n\n（5）以运算器为核心，所有的执行都经过运算器 \n\n\n\n* 微型计算机的工作过程\n\n   微机的工作过程就是执行程序的过程，也就是逐条执行指令系列的过程。由于每条指令都包括取指令和执行指令两个基本阶段，所以微机的工作过程也是不断地去取指令和执行指令的过程。当计算机进入运行状态时： \n\n   （1）首先将第一条指令由内存取出 \n\n   （2）将取出的指令送给指令译码器译码，以确定要进行的操作 \n\n   （3）读取相应的操作数（即执行的对象）\n\n   （4）执行指令 \n\n   （5）存放执行结果 \n\n   （6）一条指令执行完毕，转到下一条指令的执行阶段\n\n#### 微机的特点\n\n运算速度快、计算精度高、记忆功能和逻辑判断功能强，可自动连续工作等基本特点。\n\n* 功能强、可靠性高 \n\n* 格低廉 \n\n* 系统设计灵活，适应性强 \n\n* 体积小，重量轻，维护方便\n\n### 微型计算机系统的组成\n\n#### 硬件系统\n\n##### CPU（微处理器或中央处理器）\n\n* **运算器**\n\n核心部件是算数逻辑单元（ALU，Arithmetic and Logic Unit）,在控制信号的作用下可完成加、减、乘、除四则运算和各种逻辑运算。新型CPU运算器还可完成浮点运算\n\n* **控制器**\n\n    一般由指令寄存器、指令译码器和 操作控制电路组成。是CPU的控制中心，它从存储器中依次取出程序的各条指令，并根据指令的要求，向微机的各个部件发出相应的控制信号。\n\n* **寄存器组**\n\n    实质上是CPU内部若干个存储单元，可分为专用寄存器和通用寄存器。 专用寄存器：堆栈指针、程序计数器、标志寄存器等 通用寄存器：.......\n\n* **存储器**\n\n    主机系统的存储器又叫做主存或内存，用以存放数据（包括原始数据、中间结果、最终结果）和当前执行的程序\n\n* **输入输出接口和输入输出设备**（I/O接口与I/O设备）\n\n    常用的输入设备有：键盘、鼠标器、扫描仪等 常用的输出设备有：显示器、打印机、绘图仪等 I/O接口：I/O设备之间信息交换的桥梁\n\n* **总线**\n\n    数据总线DB：传输数据信息、双向 地址总线AB：传送CPU发出的地址信息、单向，指明与CPU交换信息的内存单元或者I/O设备 控制总线CB：传送控制信号、时序信号和状态信息等，CB中的每根线是单向的，但CB整体是双向的\n\n#### 软件系统\n\n##### 系统软件\n\n系统软件包括操作系统（OS）和系统实用程序。 操作系统：用于管理计算机的硬件和软件资源、进行任务调度、提供文件管理系统、人机接口等，包含了各种I/O设备的驱动程序 系统实用程序： 包括各种高级语言的翻译/编译程序、汇编程序、数据库系统、文本编辑程序以及诊断和调试程序，此外还包括许多系统工具程序等。\n\n##### 应用软件\n\n应用软件是用户为解决各种实际问题（如数学计算、检测与实时控制、音乐播放等）而编制的程序。\n\n### 数制与编码\n\n#### 进制\n\n* **十进制**：所有数都用0~9这10个符号的组合来表示，用D标识，逢十进一 \n\n* **二进制**：每一位只取0和1两个数字符号，用B标识，逢二进一 \n\n* **十六进制**：每一位数都用0 -9和A~F这10个字符来组合，用H标识，逢十六进一\n\n#### 进制转换\n\n* 十进制 ==\\> 二进制：\n\n    > 整数部分：除“2”取余，直到商为0；\n    >\n    > 小数部分：乘“2”取余，直到满足精度要求 \n\n​\t\t\n\n* 十进制 ==\\> 十六进制：** \n\n    > 整数部分：除“16”取余，直到商为0；\n    >\n    > 小数部分：乘“16”取余，直到满足进度要求 \n    >\n\n* 二进制 ==> 十进制 && 二进制 ==\\> 十六进制：**\n\n    > 按权展开\n\n#### BCD码\n\n计算机用二进制数编码来表示十进制数，常见的有用四位二进制来表示一位十进制 \n\n* **非压缩BCD码：**\n\n    > 用一个字节（8位）来表示一位十进制，高四位清零\n\n* **压缩BCD码：**\n\n    > 用一个字节（8位）来表示两位十进制\n\n#### 二进制数的运算\n##### **算术运算：**\n\n> **加：**逢二进一\n>\n> **减：**借一当二 \n>\n> **乘除：**\n\n##### **逻辑运算：**\n\n> **与：**有一个0即为0\n>\n> **或：**有一个1即为1 \n>\n> **非：**取反\n>\n> **异或：**相同为0，不同为1\n\n#### 带符号在计算机中的表示\n\n* **机器数：**连同书的符号一起数字化了的数据称为机器数。例如：X = +91 = 01011011B\n\n* **真值：**与机器数相应的用正、负符号加绝对值来表示的世界数值。例如：X = +91 = +1011011B \n\n* **原码：**是一种简单直观的机器数表示方法 反码：正数不变，负数符号位不变，取反加1 \n\n* **补码：**正数不变，负数符号位不变，取反加1\n\n## 8086微处理器\n\n### 微处理器概述\n\n#### 运算器：\n\n* **单总线结构运算器**：通过一条内部总线传递信息\n* **双总线结构运算器**：通过两条内部总线传递信息\n* **三总线结构运算器**：通过三条内部总线传递信息\n\n#### 控制器：\n\n##### 基本功能：\n\n> 指令控制、时序控制、操作控制功能\n>\n> 对异常情况及某些外部请求的处理能力，如运算溢出、中断请求等\n\n##### 组成部分：\n\n* **程序计数器**：存放下一条要执行的指令在存储器中的地址，程序执行前应将程序的首地址置于程序计数器\n\n* **指令寄存器（Instruction Register, IR）**：存放从存储器中取出的待执行的指令\n\n* **指令译码器（Instruction Decoder, ID）**：“ 翻译 ”指令寄存器中的指令，即指令译码\n\n* **时序控制部件**：产生计算机工作中所需的各种时序信号\n\n* **微操作控制部件**：产生与各条指令对应的微操作\n\n\n\n### 8088CPU的外部引脚及其功能\n\n#### 最小模式和最大模式\n\n* 最小模式：\n\n    > 是8088微处理器的一种操作模式，它只使用一个段寄存器来指定内存地址，这种模式下，8088只能使用1KB的内存空间。\n\n* 最大模式：\n\n    > 是8088微处理器的另一种操作模式，它使用两个段寄存器来指定内存地址，这种模式下，8088可以使用1MB的内存空间。\n\n### 8088/8086CPU的功能结构\n\n#### 内部结构\n\n* `EU`（执行单元）：\n\n    > 执行命令、分析命令、暂存中间运算结果并保留结果的特征。由ALU、通用寄存器、标志寄存器和EU控制电路组成。\n\n* `BIU`（总线接口单元）：\n\n    > BIU负责CPU与存储器、I/O接口之间的信息传递。由段寄存器、指令寄存器、指令队列、地址加法器以及总线控制逻辑组成\n\n#### 内部寄存器\n\n##### 通用寄存器（8个）\n\n* **数据寄存器**：AX、BX、CX、DX\n\n    > **`AX`：**累加器，常用于存放算术逻辑运算中的操作数，所有的I/O指令都使用累加器与外设接口传送信息\n    >\n    > **`BX`：**基址寄存器，常用来存放访问内存时的基地址\n    >\n    > **`CX`：**计数寄存器，在循环和串操作指令中用作计数器\n    >\n    > **`DX`：**数据寄存器，在寄存器间接寻址的I/O指令中存放I/O端口的地址。\n    > 注：在做双字长乘除运算时，DX与AX合起来存放一个双字长数（32位），其中**DX存放高16位，AX存放低16位。**\n\n* **地址指针寄存器**：SP、BP\n\n    > **`SP` (Stack  Pointer)：** 堆栈指针寄存器，堆栈操作中用来存放栈顶偏移地址，永远指向堆栈的栈顶\n    > **`BP` (Base Pointer)：** 基址指针寄存器，常用来存放访问内存时的基地址，通常与SS搭配使用\n\n* **变址寄存器：SI、DI**\n\n    > `SI` (Source Index)：源变址寄存器\n    >\n    > `DI`： 目的变址寄存器\n\n* **段寄存器**（4个）\n\n    > **段寄存器用于存放短地址，段起始地址的高16位**\n    >\n    > * `CS`： 代码段寄存器\n    > * **`SS`：**堆栈段寄存器\n    > * **`DS`：**数据段寄存器\n    > * **`ES`：** 附加段寄存器\n\n* **控制寄存器**（2个）\n\n    > `IP`（指令指针寄存器）\n    >\n    > 用于存放预取指令的偏移地址。CPU取指令时总是以CS为段基址，以IP为段内偏移地址。\n    > CPU从CS段中偏移地址为IP的内存单元中取出指令代码的一个字节后，IP自动加1，只想指令代码的下一个字节。\n    > 用户程序不能直接访问IP\n\n * `FLAGS`（标志位寄存器）\n\n    * 状态标志位\n\n        > **主要作用：** 记录算术和逻辑运算结果的一些特征\n        > CF：进位标志位，加（减）运算时，最高位向更高位有进（借）位时，`CF = 1`，否则，`CF =0`\n        > PF：奇偶标志位，运算结果低8位中1的个数为偶数时`PF = 1`，为奇数时，`PF =0`\n        > AF：辅助进位标志位，$D_3D3D_3D3$向$D_4D4D_4D4$ 有进（借）位时`AF=1`，否则`AF=0`\n        > ZF：零标志位，运算结果为0时`ZF = 1` ,否则`ZF = 0`\n        > SF：符号标志位，运算结果的最高位为1时 `SF = 1`，否则 `SF = 0`\n        > OF：溢出标志位，有溢出`OF = 1`,无溢出`OF = 0`\n\n    * 控制标志位TF\n\n        > TF：陷阱标志位，`TF = 1`时激活处理器的调试特性，使CPU处于单步执行指令的工作方式。每执行一条指令自动产生一次单步中断，从而达到检查程序的目的。\n        > IF：中断允许标志位，`IF = 1`，CPU可响应可屏蔽中断请求；`IF = 0`，CPU禁止响应可屏蔽中断请求。对不可屏蔽中断及内部中断没有影响。\n        > DF：方向标志位，执行串操作指令时控制操作的方向。`DF = 1`，向减地址方向进行，即从高地址开始，每进行一次操作，地址指针自动减1（或减2）；`DF = 0`，则按增地址方式进行。\n\n## 寻址方式\n\n### 立即寻址\n\n> 只针对源操作数，此时源操作数是一个立即数（8位或16位），存放于内存的代码段中。\n> 当立即数为16位时高地址存放高8位，低地址存放低8位。\n> 例如：`MOV AX, 3120H` （20H——> AL; 31H ——> AH）\n\n### 直接寻址\n\n> 表示参数运算的数据存放在内存中，存放的地址由指令直接给出，即指令中的操作数时存储器操作数（带有'[ ]'）；\n>\n> `\"[ ]\"`内用16位常数表示存放数据的偏移地址，数据的段基地址默认位数据段，可以允许重设。\n\n###  存储器寻址\n\n> 指令的操作数为CPU的内部寄存器，可以是数据寄存器（8位或16位），也可以是地址指针、变址寄存器或段寄存器。 指令操作码存放在代码段，操作数在内部寄存器中，指令执行时不必通过访问内存就能取得操作数，执行速度较快 例如：`MOV SI, AX`   （将AX中的内容送到寄存器SI中）\n\n###  寄存器间接寻址\n\n> 用寄存器的内容表示操作数的偏移地址。 存放操作数偏移地址的寄存器只允许是SI、DI、BX和BP，他们可简称位间接寄存器或地址指针。 默认情况下，选择SI、DI、BX，操作数在数据段，段地址由DS决定；选择BP作间址寄存器，则操作数在堆栈段，段地址由SS决定。 无论哪个简介寄存器都允许重设 指令中 间接寄存器要加 [ ] 例如：`MOV AX, [SI]`（将数据段（DS）中以SI为偏移地址的单元中的内容送到AX中） 若操作数存放在附加段，则指令应为：`MOV AX, ES:[SI]`\n\n###  寄存器先对寻址\n\n> 操作数在内存中存放地址（偏移地址）由间址寄存器的内容加上指令中的一个8位或者16位的位移量组成。 操作数所在段由所使用的间址寄存器决定（规则与寄存器间接寻址方式相同） 例如：`MOV AX, DATA[BX]`  （执行完后AX的物理地址为 DS*16 + BX+DATA）\n\n###  基址、变址寻址\n\n> 基址-变址寻址方式由一个基址寄存器（BX或BP）的内容和一个变址寄存器（SI或DI）的内容相加而形成操作数的偏移地址。 默认情况下，若用BX作为基址寄存器，则段地址在DS中；如果用BP作为基址寄存器，则段地址在SS中。 允许段重设 使用基址-变址寻址方式，不允许同时出现两个基址寄存器或两个变址寄存器。 例如：`MOV AX, [BX][SI]`  （执行后AL的物理地址 = DSx16 + BX+SI， AH的物理地址 = DS x16 + BX+SI +1 ）\n\n###  基址、变址、相对寻址\n\n> 基址-变址-相对寻址方式是基址-变址寻址方式的扩充，指令中指定一个基址寄存器和一个变址寄存器，同时还各处一个8为或者16位的位移量。 操作数的偏移地址 等于 三者之和 默认情况下，段寄存器由基址寄存器决定 允许段重设 同样，不允许同时出现两个基址寄存器或两个变址寄存器。 例如：`MOV AX, 5[DI][BX]`  （段寄存器为DS，偏移地址为BX + DI + 5 的连续两个单元的内容送到AX中）\n\n###  隐含寻址\n\n> 操作数隐含在指令码中 例如：`MUL` （AL x BL ——>AX）\n\n## **指令系统**\n\n### **指令的概述**\n\n#### CISC指令系统\n\n> CISC(Complex Instruction Set Computer)，复杂指令系统计算机。\n> CISC指令的设计目标是增强指令的功能，将一些原来用软件是西安的、常用的功能变成用硬件的指令系统来实现。\n\n* **缺点**\n\n    > 难以使用\n\n* **优点**：\n\n    > 指令经编译后生成的指令程序较小、执行起来较快、节省硬件资源、存取指令的次数少、占用较少的存储器。\n\n* **存在的三个方面的问题**：\n\n    > 1. “8020规律”：20%的指令在各种应用程序中出现频率占整个指令系统的80%。\n    > 2. CISC指令系统中有大量的复杂指令，控制逻辑极不规整，给VLSI（超大规模集成电路）工艺造成了极大的困难\n    > 3. CISC增加了许多复杂指令，这些指令虽然简化了目标程序、缩小了高级语言与机器语言之间的差距，但使程序总的执行时间变长、硬件的复杂度增加。\n\n#### RISC指令系统\n\n> RISC，精简指令系统计算机。\n> 一种计算机体系结构的设计思想，不是产品\n> 核心思想是通过简化指令来使计算机的结构更加简单、合理，从而提高CPU的运算速度。\n\n##### RISC的特点：\n\n> * 大多数指令在一个计算机周期内完成\n> * 指令系统中应尽量减少访问存储器的指令，而采用寄存器与寄存器之间的操作\n> * 减少寻址方式的种类。复杂的寻址方式用简单的寻址方式合成\n> * 减少指令的种类。复杂的指令用软件实现\n> * 指令格式简单\n>\n\n### **8086指令系统**\n\n#### 指令概述\n\n##### 指令\n\n指令的一般格式为  操作码 目标操作数 源操作数\n\n* **零操作数指令**：\n\n    > 形式上只有操作码，操作数是隐含存在的，操作对象通常为处理器本身\n\n* **单操作数指令**：\n\n    > 指令中仅给出一个操作数，另一个操作数隐含存在\n\n* **双操作数指令**：\n\n    > 一般格式\n\n##### 操作数\n\n* **立即操作数*：\n\n    > 立即数是指具有固定数值的操作数，即常数，不因指令的执行而发生变化。8086系统中字长可以是1字节或者2字节；可以是无符号数或有符号数\n\n* **寄存器操作数**：\n\n    > 8086CPU的8个通用寄存器和4个段寄存器可以作为指令中的寄存器操作数，源操作数和目标操作数均可\n\n* 存储器操作数：\n\n    > 其含义是参加运算的数据是存放在内存中的。通常为8位或16位字长\n\n#### 数据传送指令\n\n##### 通用数据传送指令\n\n- **一般数据传送指令**\n\n    > 原则：\n    > （1）两操作数字长必须相同\n    > ​（2）两操作数不允许同时为存储器操作数\n    > ​（3）两操作数不允许同时为段寄存器\n    > ​（4）在源操作数是立即数时，目标操作数不能是段寄存器\n    > ​（5）IP和CS不作为目标操作数，FLAGS一般也不作为操作数在指令中出现\n    >\n    > * `MOV`\n    >\n    >     > * 格式：`MOV dest, src`\n    >     > * 执行过程：`dest ——> src`\n    >     > * 举例：：`MOV AL, BL`\n\n- **堆栈操作指令**\n\n    > 原则：\n    > （1）先进后出\n    > （2）以字为单位\n    > （3）指令操作数必须是16位\n    > （4）操作数可以是寄存器或存储器两单元，但不能是立即数\n    > （5）不能从栈顶弹出一个字给CS\n    > （6）PUSH和POP指令在程序中一般成对出现\n    > （7）PUSH指令的操作方向是从高地址向低地址，而POP指令的操作正好相反\n    >\n    > * `PUSH`（压栈指令）：\n    >\n    >     > `OPRD`：16存储器或存储器两单元\n    >     >\n    >     > * 格式：`PUSH OPRD`\n    >     >\n    >     > * 执行过程\n    >     >\n    >     >     > `SP -2 ——>SP`\n    >     >     > 操作数高字节 ——> `SP+1`\n    >     >     > 操作数低字节 ——>`SP`\n\n- **`POP`（出栈指令）：**\n\n    > - 格式：`POP OPRD`\n    >\n    > - 执行过程：\n    >     \n    >     > `SP` ——> 操作数低字节\n    >     > `SP+ 1` ——> 操作数高字节\n    >     > `SP` <—— `SP +2`\n\n- **交换指令**\n\n    > 原则：\n    > （1）两操作数必须有一个是寄存器操作数\n    > （2）不允许使用段寄存器\n    >\n    > * `XCHG`\n    >\n    >     > * 格式：`XCHG REG, MEM/REG`\n    >     >\n    >     > * 操作：\n    >     >     `REG <——> MEM/ REG`\n    >     >\n    >     > * 例子：\n    >     >     `XCHG AX, BX`\n    >     >     `XCHG [2000],CL`\n- **查表转换指令**\n\n    > 原则：\n    > （1）零操作数指令\n    > （2）用BX的内容代表表格首地址，AL内容为表内偏移量，BX+AL得到要查找元素的偏移地址\n    >\n    > * XLAT\n    >\n    >     > * 格式：XLAT\n    >     >\n    >     > * 操作：\n    >     >\n    >     >     > 将BX+AL所指单元的内容送给AL\n- **字位扩展指令**\n\n    > 原则：\n    > （1）将符号数的符号位扩展到高位\n    > （2）指令为零操作数，采用隐含寻址，隐含的操作数为AX及AX，DX\n    > （3）无符号数的扩展规则为在高位补0\n    >\n    > - `CBW`（字节到字的扩展指令）\n    >\n    >     > - 格式：`CBW`\n    >     >\n    >     > - 操作：\n    >     >     \n    >     >     > 将AL内容扩展到AX\n    >     >     > 若最高位 = 1，则执行后 AH =FFH\n    >     >     > 若最高位 = 0，则执行后AH = 00H\n    >\n    > - `CWD`（字到双子的扩展指令）\n    >\n    >     > * 格式：`CWD`\n    >     >\n    >     > * 操作：\n    >     >\n    >     >     > 将 `AX` 内容扩展到 `DX` `AX`\n    >     >     >\n    >     >     > 若最高位 = 1，则执行后DX = FFFFH\n    >     >     >\n    >     >     > 若最高位 = 0，则执行后DX = 0000H\n\n##### **输入输出指令**\n\n* 输入指令\n\n    * IN\n\n        * 指令：`IN acc, PORT`\n\n        * 操作：\n\n            > 从端口地址读入数据到累加器中\n\n* 输出指令\n\n    * OUT\n\n        * 指令：`OUT PORT, acc`\n\n        * 操作：\n\n            > 将累加器的值输出到端口中\n\n##### **地址传送指令**\n\n* **取近地址指令**\n\n    * `LEA`：\n\n        * 格式：`LEA REG, MEM`（必须是存储器操作数）\n\n        * 操作：\n\n            > 将变量得16位偏移地址写入到目标寄存器\n\n        * 要求：\n\n            > 源操作数必须是一个存储器操作数，目标操作数通常是间址\n\n* **取远地址指令**\n\n    * `LDS`：\n\n        * 格式：`LDS`  通用寄存器，存储器操作数\n\n        * 操作：\n\n            > 将源操作数得偏移地址送目标寄存器，将源操作数得地址送DS\n\n    * `LES`：\n\n        * 格式：`LES` 通用寄存器，存储器操作数\n\n        * 操作：\n\n            > 将源操作数得偏移地址送目标寄存器，将源操作数得地址送ES\n\n##### **标志传送指令**\n\n* 隐含操作数AH\n\n    * `LAHF`(Load AH from Flags)\n\n        * 格式：`LAHF`\n\n        * 操作：\n\n            > 将`FLAGS`的低8位装入`AH`\n\n    * `SAHF`(Store AH into Flags)\n\n        * 格式：`SHAF`\n\n        * 操作：\n\n            > 与`LAHF`相反\n\n* 隐含操作数FLAGS\n\n    * PUSHF(Push flags onto stack)\n    * POPF(Pop flags off stack)\n\n#### **算术运算指令**\n\n##### 加法运算指令\n\n* `ADD`（普通加法指令）\n\n    **ADD指令的执行对全部6个状态标志位都产生影响**\n\n    * 格式：`ADD OPRD1, OPRD2`\n\n    * 操作:\n\n        > `OPRD1 + OPRD2 ——> OPRD1`\n\n    * 例子：\n\n        > `ADD AL, 99H`（）\n\n* `ADC`（带进位的加法指令）\n\n    **ADC指令都用于多字节数相加，使用前要先将CF清零**\n\n    * 格式：`ADC OPRD1, OPRD2`\n\n    * 操作：\n\n        > `OPRD1 + OPRD2 + CF ——> OPRD1`\n\n* `INC`（加1指令）\n\n    **常用于程序中修改地址指针**\n\n    * 格式：`INC OPRD`\n\n    * 操作：\n\n        > `OPRD + 1 ——> OPRD`\n\n##### 减法运算指令\n\n* `SUB` （普通减法指令）\n\n    **对标志位的影响与ADD指令相同**\n\n    * 格式：`SUB OPRD1, OPRD2`\n\n    * 操作：\n\n        > `OPRD1 - OPRD2 ——> OPRD1`\n\n* `SBB`（考虑借位的减法指令）\n\n    * 格式：`SBB OPRD1, OPRD2`\n\n    * 操作：\n\n        > `OPRD1 - OPRD2 - CF——> OPRD1`\n\n* `DEC`（减1指令DEC）\n\n    **指令对操作数的要求与INC相同，指令常用于在程序中修改计数值**\n\n    * 格式：`DEC OPRD`\n\n    * 操作：\n\n        > `OPRD -1 ——> OPRD`\n\n    * 应用程序例子：\n\n        ![image-20230308223135359](微机原理及接口技术/image-20230308223135359.png)\n\n* `CMP`（比较指令）\n\n    用于比较两个数的大小，可作为条件转移指令转移的条件\n\n    指令执行的结果不影响目标操作数，仅影响标志位！\n\n    对操作数的要求及对标志位的影响与SUB指令相同\n\n    * 格式：`CMP OPRD1, OPRD2`\n\n    * 操作：\n\n        > `OPRD1 - OPRD2`\n\n    * 例子：\n\n        * 两个无符号数的比较\n\n            > `CMP AX, BX`\n            >\n            > 若 AX >= BX ——> CF =0\n            >\n            > 若AX < BX ——> CF = 1\n            >\n            > 若 AX = BX ——> CF =1,ZF =1\n\n        * 两个带符号数的比较\n\n            > CMP AX, BX\n            >\n            > 两个数的大小由OF和SF共同决定\n            >\n            > OF和SF状态相同 AX >= BX\n            >\n            > OF和SF状态不同 AX < BX\n\n* `NEG`（求补指令）\n\n    **对一个负数取补码就相当于用零减去此数**\n\n    * 格式：`NEG OPRD`\n\n    * 操作：\n\n        > `0 - OPRD ——> OPRD`\n\n    * 说明：\n\n        * 执行NEG指令后，一般情况下都会使CF为1，除非给定的操作数为零才会使CF为0；\n        * 当指定的操作数的值为80H(-128)或为8000H(-32768)，则执行NEG指令后，结果不变，但OF置1，其它情况下OF置0。\n\n##### 乘法指令\n\n乘法指令采用隐含寻址，隐含的是存放被乘数的累加器AL或AX及存放结果的AX，DX\n\n* **`MUL`无符号的乘法指令**\n\n    * 格式：`MUL OPRD`（OPRD不能是立即数）\n\n    * 操作：\n\n        > OPRD为8字节数 ===> AL x OPRD ——> AX\n        >\n        > OPRD为16位数 ====> AX x OPRD ——> DXAX \n\n    * 例子：\n\n        > ![image-20230308232215011](微机原理及接口技术/image-20230308232215011-16782889368582.png)\n\n* **`IMUL`带符号的乘法指令**\n\n    * 格式：`IMUL OPRD`（OPRD不能是立即数，隐含操作数为AL，存放在AX中）\n\n    * 操作：\n\n        > 1. 将两个操作数取补码（对负数按位取反加1，正数不变）\n        > 2. 做乘法运算\n        > 3. 将乘积按位取反加 1\n\n##### 除法指令\n\n指令要求被除数是除数的双倍字长\n\n* **无符号除法**\n\n    * 格式：`DIV OPRD`\n\n    * 操作：\n\n        > 若`OPRD`是字节数：\n        >\n        > * 执行：`AX/OPRD`\n        > * 结果：AL = 商   AH = 余数\n        >\n        > 若`OPRD`是双字节数：\n        >\n        > * 执行：`DXAX/OPRD`\n        > * 结果：AX = 商 DX = 余数\n\n* **有符号除法**\n\n    * 格式：`IDIV OPRD`\n\n    * 操作：\n\n        > 若OPRD是字节数：\n        >\n        > * 执行：`AX/OPRD`\n        > * 结果：AL = 商   AH = 余数\n        >\n        > 若OPRD是双字节数：\n        >\n        > * 执行：`DXAX/OPRD`\n        > * 结果：AX = 商 DX = 余数\n\n#### **逻辑运算和移位指令**\n\n##### 逻辑运算指令\n\n> * **对操作数的要求**：\n>     * 大多数与`MOV`指令相同\n>     * “非” 运算指令要求操作数不能是立即数\n> * **对标志位的影响**\n>     * 除“非”运算指令，其余指令的执行都会影响除AF外的五个状态标志\n>     * 无论执行结果如何，都会使标志位`OF=CF=0`\n>     * “非”运算指令的执行不影响标志位\n\n* **\"与\"指令**\n\n    * 格式：`AND OPRD1，OPRD2`\n\n    * 操作：\n\n        > 两操作数相“与”，结果送目标地址。\n\n    * 应用：\n\n        * 实现两操作数按位相与的运算：`AND BL, [SI]`\n        * 使目标操作数的某些位不变，某些位清零：`AND AL, 0FH`\n        * 在操作数不变的情况向使CF和OF清零：`AND AX, AX`\n\n* “或”运算指令\n\n    * 格式\n    * 操作\n    * 应用\n\n* “非”运算指令\n\n    * 格式：NOT OPRD\n\n    * 操作：\n\n        > 操作数按位取反在送回原地址，指令的执行对标志位无影响\n\n    * 例子：\n\n        > \n\n* “异或”运算指令\n\n    * 格式：`XOR OPRD1, OPRD2`\n\n    * 操作：\n\n        > 两操作数“异或”，结果送目标地址\n\n    * 例子：\n\n* “测试”指令\n\n    * 格式：`TEST OPRD1, OPRD2`\n\n    * 操作：\n\n        > 执行“与”运算，但运算的结果不送回目标地址\n\n    * 应用：\n\n        > 常用于测试某些位的状态\n\n#### 串操作指令\n\n#### 程序控制指令\n\n#### 处理器控制指令\n\n## **汇编语言程序设计**\n\n### **伪指令**\n\n### **BIOS和DOS功能调用**\n\n### **汇编语言程序设计基础**\n\n> #### **程序设计概述**\n>\n> #### **顺序结构**\n>\n> #### **分支结构**\n>\n> #### **循环结构**\n>\n> #### **子程序**\n>\n> #### **常用程序设计举例**\n>\n\n## **存储器系统**\n\n### **随机存取存储器**\n\n### **只读存储器**\n\n### **高速缓冲存储器**\n\n### **存储器扩展技术**\n\n","slug":"学习记录/考研/微机原理及接口技术","published":1,"updated":"2023-03-21T11:13:47.610Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clfjkj0br000d8gsz1xpr35nt","content":"<h1 id=\"《微机原理及接口技术-吴宁》-学习记录\"><a href=\"#《微机原理及接口技术-吴宁》-学习记录\" class=\"headerlink\" title=\"《微机原理及接口技术 吴宁》 学习记录\"></a>《微机原理及接口技术 吴宁》 学习记录</h1><h2 id=\"微信计算机基础\"><a href=\"#微信计算机基础\" class=\"headerlink\" title=\"微信计算机基础\"></a>微信计算机基础</h2><h3 id=\"微型计算机概述\"><a href=\"#微型计算机概述\" class=\"headerlink\" title=\"微型计算机概述\"></a>微型计算机概述</h3><p>计算机系统是一种由硬件系统和软件系统组成的复杂电子装置，它能够存储程序、存储原始数据、中间结果和最终运算结果，并自动完成运算，是一种能对各种数字化处理的 “信息处理机”。微型计算机是一种小型的、可编程的计算机，它可以用于执行复杂的计算任务。它们通常由一个或多个微处理器组成，可以存储和处理大量的数据，并且可以运行复杂的程序。它们可以用于控制机器和设备，以及用于收集、处理和传输数据。</p>\n<h4 id=\"微机的发展\"><a href=\"#微机的发展\" class=\"headerlink\" title=\"微机的发展\"></a>微机的发展</h4><p>自1946年第一台计算机问世已在，已经历了由电子管计算机、晶体管计算机、集成电路计算机到大规模、超规模集成电路计算机这样五代的更替、并且在不断地向巨型化、微型化、网络化和智能化这4个方向发展。</p>\n<h5 id=\"微机的工作过程\"><a href=\"#微机的工作过程\" class=\"headerlink\" title=\"微机的工作过程\"></a>微机的工作过程</h5><ul>\n<li>冯·诺依曼计算机的主要特点</li>\n</ul>\n<p><img src=\"/2023/03/06/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/%E8%80%83%E7%A0%94/%E5%BE%AE%E6%9C%BA%E5%8E%9F%E7%90%86%E5%8F%8A%E6%8E%A5%E5%8F%A3%E6%8A%80%E6%9C%AF/微机原理及接口技术.jpg\" alt=\"微机原理及接口技术\" style=\"zoom:80%;\"></p>\n<p>（1）将计算过程描述为由许多条指令按一定顺序组成的程序、并放入存储器保存 </p>\n<p>（2）程序中的指令和数据必须采用2进制编码，且能够被执行该程序的计算机所识别 </p>\n<p>（3）指令按其在存储器中存放的顺序执行，存储器的字长固定并按顺序线性编制 </p>\n<p>（4）由控制器控制整个程序和数据的存取以及程序的执行 </p>\n<p>（5）以运算器为核心，所有的执行都经过运算器 </p>\n<ul>\n<li><p>微型计算机的工作过程</p>\n<p> 微机的工作过程就是执行程序的过程，也就是逐条执行指令系列的过程。由于每条指令都包括取指令和执行指令两个基本阶段，所以微机的工作过程也是不断地去取指令和执行指令的过程。当计算机进入运行状态时： </p>\n<p> （1）首先将第一条指令由内存取出 </p>\n<p> （2）将取出的指令送给指令译码器译码，以确定要进行的操作 </p>\n<p> （3）读取相应的操作数（即执行的对象）</p>\n<p> （4）执行指令 </p>\n<p> （5）存放执行结果 </p>\n<p> （6）一条指令执行完毕，转到下一条指令的执行阶段</p>\n</li>\n</ul>\n<h4 id=\"微机的特点\"><a href=\"#微机的特点\" class=\"headerlink\" title=\"微机的特点\"></a>微机的特点</h4><p>运算速度快、计算精度高、记忆功能和逻辑判断功能强，可自动连续工作等基本特点。</p>\n<ul>\n<li><p>功能强、可靠性高 </p>\n</li>\n<li><p>格低廉 </p>\n</li>\n<li><p>系统设计灵活，适应性强 </p>\n</li>\n<li><p>体积小，重量轻，维护方便</p>\n</li>\n</ul>\n<h3 id=\"微型计算机系统的组成\"><a href=\"#微型计算机系统的组成\" class=\"headerlink\" title=\"微型计算机系统的组成\"></a>微型计算机系统的组成</h3><h4 id=\"硬件系统\"><a href=\"#硬件系统\" class=\"headerlink\" title=\"硬件系统\"></a>硬件系统</h4><h5 id=\"CPU（微处理器或中央处理器）\"><a href=\"#CPU（微处理器或中央处理器）\" class=\"headerlink\" title=\"CPU（微处理器或中央处理器）\"></a>CPU（微处理器或中央处理器）</h5><ul>\n<li><strong>运算器</strong></li>\n</ul>\n<p>核心部件是算数逻辑单元（ALU，Arithmetic and Logic Unit）,在控制信号的作用下可完成加、减、乘、除四则运算和各种逻辑运算。新型CPU运算器还可完成浮点运算</p>\n<ul>\n<li><p><strong>控制器</strong></p>\n<p>  一般由指令寄存器、指令译码器和 操作控制电路组成。是CPU的控制中心，它从存储器中依次取出程序的各条指令，并根据指令的要求，向微机的各个部件发出相应的控制信号。</p>\n</li>\n<li><p><strong>寄存器组</strong></p>\n<p>  实质上是CPU内部若干个存储单元，可分为专用寄存器和通用寄存器。 专用寄存器：堆栈指针、程序计数器、标志寄存器等 通用寄存器：…….</p>\n</li>\n<li><p><strong>存储器</strong></p>\n<p>  主机系统的存储器又叫做主存或内存，用以存放数据（包括原始数据、中间结果、最终结果）和当前执行的程序</p>\n</li>\n<li><p><strong>输入输出接口和输入输出设备</strong>（I/O接口与I/O设备）</p>\n<p>  常用的输入设备有：键盘、鼠标器、扫描仪等 常用的输出设备有：显示器、打印机、绘图仪等 I/O接口：I/O设备之间信息交换的桥梁</p>\n</li>\n<li><p><strong>总线</strong></p>\n<p>  数据总线DB：传输数据信息、双向 地址总线AB：传送CPU发出的地址信息、单向，指明与CPU交换信息的内存单元或者I/O设备 控制总线CB：传送控制信号、时序信号和状态信息等，CB中的每根线是单向的，但CB整体是双向的</p>\n</li>\n</ul>\n<h4 id=\"软件系统\"><a href=\"#软件系统\" class=\"headerlink\" title=\"软件系统\"></a>软件系统</h4><h5 id=\"系统软件\"><a href=\"#系统软件\" class=\"headerlink\" title=\"系统软件\"></a>系统软件</h5><p>系统软件包括操作系统（OS）和系统实用程序。 操作系统：用于管理计算机的硬件和软件资源、进行任务调度、提供文件管理系统、人机接口等，包含了各种I/O设备的驱动程序 系统实用程序： 包括各种高级语言的翻译/编译程序、汇编程序、数据库系统、文本编辑程序以及诊断和调试程序，此外还包括许多系统工具程序等。</p>\n<h5 id=\"应用软件\"><a href=\"#应用软件\" class=\"headerlink\" title=\"应用软件\"></a>应用软件</h5><p>应用软件是用户为解决各种实际问题（如数学计算、检测与实时控制、音乐播放等）而编制的程序。</p>\n<h3 id=\"数制与编码\"><a href=\"#数制与编码\" class=\"headerlink\" title=\"数制与编码\"></a>数制与编码</h3><h4 id=\"进制\"><a href=\"#进制\" class=\"headerlink\" title=\"进制\"></a>进制</h4><ul>\n<li><p><strong>十进制</strong>：所有数都用0~9这10个符号的组合来表示，用D标识，逢十进一 </p>\n</li>\n<li><p><strong>二进制</strong>：每一位只取0和1两个数字符号，用B标识，逢二进一 </p>\n</li>\n<li><p><strong>十六进制</strong>：每一位数都用0 -9和A~F这10个字符来组合，用H标识，逢十六进一</p>\n</li>\n</ul>\n<h4 id=\"进制转换\"><a href=\"#进制转换\" class=\"headerlink\" title=\"进制转换\"></a>进制转换</h4><ul>\n<li><p>十进制 ==> 二进制：</p>\n<blockquote>\n<p>整数部分：除“2”取余，直到商为0；</p>\n<p>小数部分：乘“2”取余，直到满足精度要求 </p>\n</blockquote>\n</li>\n</ul>\n<p>​        </p>\n<ul>\n<li><p>十进制 ==> 十六进制：** </p>\n<blockquote>\n<p>整数部分：除“16”取余，直到商为0；</p>\n<p>小数部分：乘“16”取余，直到满足进度要求 </p>\n</blockquote>\n</li>\n<li><p>二进制 ==&gt; 十进制 &amp;&amp; 二进制 ==> 十六进制：**</p>\n<blockquote>\n<p>按权展开</p>\n</blockquote>\n</li>\n</ul>\n<h4 id=\"BCD码\"><a href=\"#BCD码\" class=\"headerlink\" title=\"BCD码\"></a>BCD码</h4><p>计算机用二进制数编码来表示十进制数，常见的有用四位二进制来表示一位十进制 </p>\n<ul>\n<li><p><strong>非压缩BCD码：</strong></p>\n<blockquote>\n<p>用一个字节（8位）来表示一位十进制，高四位清零</p>\n</blockquote>\n</li>\n<li><p><strong>压缩BCD码：</strong></p>\n<blockquote>\n<p>用一个字节（8位）来表示两位十进制</p>\n</blockquote>\n</li>\n</ul>\n<h4 id=\"二进制数的运算\"><a href=\"#二进制数的运算\" class=\"headerlink\" title=\"二进制数的运算\"></a>二进制数的运算</h4><h5 id=\"算术运算：\"><a href=\"#算术运算：\" class=\"headerlink\" title=\"算术运算：\"></a><strong>算术运算：</strong></h5><blockquote>\n<p><strong>加：</strong>逢二进一</p>\n<p><strong>减：</strong>借一当二 </p>\n<p><strong>乘除：</strong></p>\n</blockquote>\n<h5 id=\"逻辑运算：\"><a href=\"#逻辑运算：\" class=\"headerlink\" title=\"逻辑运算：\"></a><strong>逻辑运算：</strong></h5><blockquote>\n<p><strong>与：</strong>有一个0即为0</p>\n<p><strong>或：</strong>有一个1即为1 </p>\n<p><strong>非：</strong>取反</p>\n<p><strong>异或：</strong>相同为0，不同为1</p>\n</blockquote>\n<h4 id=\"带符号在计算机中的表示\"><a href=\"#带符号在计算机中的表示\" class=\"headerlink\" title=\"带符号在计算机中的表示\"></a>带符号在计算机中的表示</h4><ul>\n<li><p><strong>机器数：</strong>连同书的符号一起数字化了的数据称为机器数。例如：X = +91 = 01011011B</p>\n</li>\n<li><p><strong>真值：</strong>与机器数相应的用正、负符号加绝对值来表示的世界数值。例如：X = +91 = +1011011B </p>\n</li>\n<li><p><strong>原码：</strong>是一种简单直观的机器数表示方法 反码：正数不变，负数符号位不变，取反加1 </p>\n</li>\n<li><p><strong>补码：</strong>正数不变，负数符号位不变，取反加1</p>\n</li>\n</ul>\n<h2 id=\"8086微处理器\"><a href=\"#8086微处理器\" class=\"headerlink\" title=\"8086微处理器\"></a>8086微处理器</h2><h3 id=\"微处理器概述\"><a href=\"#微处理器概述\" class=\"headerlink\" title=\"微处理器概述\"></a>微处理器概述</h3><h4 id=\"运算器：\"><a href=\"#运算器：\" class=\"headerlink\" title=\"运算器：\"></a>运算器：</h4><ul>\n<li><strong>单总线结构运算器</strong>：通过一条内部总线传递信息</li>\n<li><strong>双总线结构运算器</strong>：通过两条内部总线传递信息</li>\n<li><strong>三总线结构运算器</strong>：通过三条内部总线传递信息</li>\n</ul>\n<h4 id=\"控制器：\"><a href=\"#控制器：\" class=\"headerlink\" title=\"控制器：\"></a>控制器：</h4><h5 id=\"基本功能：\"><a href=\"#基本功能：\" class=\"headerlink\" title=\"基本功能：\"></a>基本功能：</h5><blockquote>\n<p>指令控制、时序控制、操作控制功能</p>\n<p>对异常情况及某些外部请求的处理能力，如运算溢出、中断请求等</p>\n</blockquote>\n<h5 id=\"组成部分：\"><a href=\"#组成部分：\" class=\"headerlink\" title=\"组成部分：\"></a>组成部分：</h5><ul>\n<li><p><strong>程序计数器</strong>：存放下一条要执行的指令在存储器中的地址，程序执行前应将程序的首地址置于程序计数器</p>\n</li>\n<li><p><strong>指令寄存器（Instruction Register, IR）</strong>：存放从存储器中取出的待执行的指令</p>\n</li>\n<li><p><strong>指令译码器（Instruction Decoder, ID）</strong>：“ 翻译 ”指令寄存器中的指令，即指令译码</p>\n</li>\n<li><p><strong>时序控制部件</strong>：产生计算机工作中所需的各种时序信号</p>\n</li>\n<li><p><strong>微操作控制部件</strong>：产生与各条指令对应的微操作</p>\n</li>\n</ul>\n<h3 id=\"8088CPU的外部引脚及其功能\"><a href=\"#8088CPU的外部引脚及其功能\" class=\"headerlink\" title=\"8088CPU的外部引脚及其功能\"></a>8088CPU的外部引脚及其功能</h3><h4 id=\"最小模式和最大模式\"><a href=\"#最小模式和最大模式\" class=\"headerlink\" title=\"最小模式和最大模式\"></a>最小模式和最大模式</h4><ul>\n<li><p>最小模式：</p>\n<blockquote>\n<p>是8088微处理器的一种操作模式，它只使用一个段寄存器来指定内存地址，这种模式下，8088只能使用1KB的内存空间。</p>\n</blockquote>\n</li>\n<li><p>最大模式：</p>\n<blockquote>\n<p>是8088微处理器的另一种操作模式，它使用两个段寄存器来指定内存地址，这种模式下，8088可以使用1MB的内存空间。</p>\n</blockquote>\n</li>\n</ul>\n<h3 id=\"8088-8086CPU的功能结构\"><a href=\"#8088-8086CPU的功能结构\" class=\"headerlink\" title=\"8088/8086CPU的功能结构\"></a>8088/8086CPU的功能结构</h3><h4 id=\"内部结构\"><a href=\"#内部结构\" class=\"headerlink\" title=\"内部结构\"></a>内部结构</h4><ul>\n<li><p><code>EU</code>（执行单元）：</p>\n<blockquote>\n<p>执行命令、分析命令、暂存中间运算结果并保留结果的特征。由ALU、通用寄存器、标志寄存器和EU控制电路组成。</p>\n</blockquote>\n</li>\n<li><p><code>BIU</code>（总线接口单元）：</p>\n<blockquote>\n<p>BIU负责CPU与存储器、I/O接口之间的信息传递。由段寄存器、指令寄存器、指令队列、地址加法器以及总线控制逻辑组成</p>\n</blockquote>\n</li>\n</ul>\n<h4 id=\"内部寄存器\"><a href=\"#内部寄存器\" class=\"headerlink\" title=\"内部寄存器\"></a>内部寄存器</h4><h5 id=\"通用寄存器（8个）\"><a href=\"#通用寄存器（8个）\" class=\"headerlink\" title=\"通用寄存器（8个）\"></a>通用寄存器（8个）</h5><ul>\n<li><p><strong>数据寄存器</strong>：AX、BX、CX、DX</p>\n<blockquote>\n<p><strong><code>AX</code>：</strong>累加器，常用于存放算术逻辑运算中的操作数，所有的I/O指令都使用累加器与外设接口传送信息</p>\n<p><strong><code>BX</code>：</strong>基址寄存器，常用来存放访问内存时的基地址</p>\n<p><strong><code>CX</code>：</strong>计数寄存器，在循环和串操作指令中用作计数器</p>\n<p><strong><code>DX</code>：</strong>数据寄存器，在寄存器间接寻址的I/O指令中存放I/O端口的地址。<br>注：在做双字长乘除运算时，DX与AX合起来存放一个双字长数（32位），其中<strong>DX存放高16位，AX存放低16位。</strong></p>\n</blockquote>\n</li>\n<li><p><strong>地址指针寄存器</strong>：SP、BP</p>\n<blockquote>\n<p><strong><code>SP</code> (Stack  Pointer)：</strong> 堆栈指针寄存器，堆栈操作中用来存放栈顶偏移地址，永远指向堆栈的栈顶<br><strong><code>BP</code> (Base Pointer)：</strong> 基址指针寄存器，常用来存放访问内存时的基地址，通常与SS搭配使用</p>\n</blockquote>\n</li>\n<li><p><strong>变址寄存器：SI、DI</strong></p>\n<blockquote>\n<p><code>SI</code> (Source Index)：源变址寄存器</p>\n<p><code>DI</code>： 目的变址寄存器</p>\n</blockquote>\n</li>\n<li><p><strong>段寄存器</strong>（4个）</p>\n<blockquote>\n<p><strong>段寄存器用于存放短地址，段起始地址的高16位</strong></p>\n<ul>\n<li><code>CS</code>： 代码段寄存器</li>\n<li><strong><code>SS</code>：</strong>堆栈段寄存器</li>\n<li><strong><code>DS</code>：</strong>数据段寄存器</li>\n<li><strong><code>ES</code>：</strong> 附加段寄存器</li>\n</ul>\n</blockquote>\n</li>\n<li><p><strong>控制寄存器</strong>（2个）</p>\n<blockquote>\n<p><code>IP</code>（指令指针寄存器）</p>\n<p>用于存放预取指令的偏移地址。CPU取指令时总是以CS为段基址，以IP为段内偏移地址。<br>CPU从CS段中偏移地址为IP的内存单元中取出指令代码的一个字节后，IP自动加1，只想指令代码的下一个字节。<br>用户程序不能直接访问IP</p>\n</blockquote>\n<ul>\n<li><p><code>FLAGS</code>（标志位寄存器）</p>\n<ul>\n<li><p>状态标志位</p>\n<blockquote>\n<p><strong>主要作用：</strong> 记录算术和逻辑运算结果的一些特征<br>CF：进位标志位，加（减）运算时，最高位向更高位有进（借）位时，<code>CF = 1</code>，否则，<code>CF =0</code><br>PF：奇偶标志位，运算结果低8位中1的个数为偶数时<code>PF = 1</code>，为奇数时，<code>PF =0</code><br>AF：辅助进位标志位，$D_3D3D_3D3$向$D_4D4D_4D4$ 有进（借）位时<code>AF=1</code>，否则<code>AF=0</code><br>ZF：零标志位，运算结果为0时<code>ZF = 1</code> ,否则<code>ZF = 0</code><br>SF：符号标志位，运算结果的最高位为1时 <code>SF = 1</code>，否则 <code>SF = 0</code><br>OF：溢出标志位，有溢出<code>OF = 1</code>,无溢出<code>OF = 0</code></p>\n</blockquote>\n</li>\n<li><p>控制标志位TF</p>\n<blockquote>\n<p>TF：陷阱标志位，<code>TF = 1</code>时激活处理器的调试特性，使CPU处于单步执行指令的工作方式。每执行一条指令自动产生一次单步中断，从而达到检查程序的目的。<br>IF：中断允许标志位，<code>IF = 1</code>，CPU可响应可屏蔽中断请求；<code>IF = 0</code>，CPU禁止响应可屏蔽中断请求。对不可屏蔽中断及内部中断没有影响。<br>DF：方向标志位，执行串操作指令时控制操作的方向。<code>DF = 1</code>，向减地址方向进行，即从高地址开始，每进行一次操作，地址指针自动减1（或减2）；<code>DF = 0</code>，则按增地址方式进行。</p>\n</blockquote>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"寻址方式\"><a href=\"#寻址方式\" class=\"headerlink\" title=\"寻址方式\"></a>寻址方式</h2><h3 id=\"立即寻址\"><a href=\"#立即寻址\" class=\"headerlink\" title=\"立即寻址\"></a>立即寻址</h3><blockquote>\n<p>只针对源操作数，此时源操作数是一个立即数（8位或16位），存放于内存的代码段中。<br>当立即数为16位时高地址存放高8位，低地址存放低8位。<br>例如：<code>MOV AX, 3120H</code> （20H——&gt; AL; 31H ——&gt; AH）</p>\n</blockquote>\n<h3 id=\"直接寻址\"><a href=\"#直接寻址\" class=\"headerlink\" title=\"直接寻址\"></a>直接寻址</h3><blockquote>\n<p>表示参数运算的数据存放在内存中，存放的地址由指令直接给出，即指令中的操作数时存储器操作数（带有’[ ]’）；</p>\n<p><code>&quot;[ ]&quot;</code>内用16位常数表示存放数据的偏移地址，数据的段基地址默认位数据段，可以允许重设。</p>\n</blockquote>\n<h3 id=\"存储器寻址\"><a href=\"#存储器寻址\" class=\"headerlink\" title=\"存储器寻址\"></a>存储器寻址</h3><blockquote>\n<p>指令的操作数为CPU的内部寄存器，可以是数据寄存器（8位或16位），也可以是地址指针、变址寄存器或段寄存器。 指令操作码存放在代码段，操作数在内部寄存器中，指令执行时不必通过访问内存就能取得操作数，执行速度较快 例如：<code>MOV SI, AX</code>   （将AX中的内容送到寄存器SI中）</p>\n</blockquote>\n<h3 id=\"寄存器间接寻址\"><a href=\"#寄存器间接寻址\" class=\"headerlink\" title=\"寄存器间接寻址\"></a>寄存器间接寻址</h3><blockquote>\n<p>用寄存器的内容表示操作数的偏移地址。 存放操作数偏移地址的寄存器只允许是SI、DI、BX和BP，他们可简称位间接寄存器或地址指针。 默认情况下，选择SI、DI、BX，操作数在数据段，段地址由DS决定；选择BP作间址寄存器，则操作数在堆栈段，段地址由SS决定。 无论哪个简介寄存器都允许重设 指令中 间接寄存器要加 [ ] 例如：<code>MOV AX, [SI]</code>（将数据段（DS）中以SI为偏移地址的单元中的内容送到AX中） 若操作数存放在附加段，则指令应为：<code>MOV AX, ES:[SI]</code></p>\n</blockquote>\n<h3 id=\"寄存器先对寻址\"><a href=\"#寄存器先对寻址\" class=\"headerlink\" title=\"寄存器先对寻址\"></a>寄存器先对寻址</h3><blockquote>\n<p>操作数在内存中存放地址（偏移地址）由间址寄存器的内容加上指令中的一个8位或者16位的位移量组成。 操作数所在段由所使用的间址寄存器决定（规则与寄存器间接寻址方式相同） 例如：<code>MOV AX, DATA[BX]</code>  （执行完后AX的物理地址为 DS*16 + BX+DATA）</p>\n</blockquote>\n<h3 id=\"基址、变址寻址\"><a href=\"#基址、变址寻址\" class=\"headerlink\" title=\"基址、变址寻址\"></a>基址、变址寻址</h3><blockquote>\n<p>基址-变址寻址方式由一个基址寄存器（BX或BP）的内容和一个变址寄存器（SI或DI）的内容相加而形成操作数的偏移地址。 默认情况下，若用BX作为基址寄存器，则段地址在DS中；如果用BP作为基址寄存器，则段地址在SS中。 允许段重设 使用基址-变址寻址方式，不允许同时出现两个基址寄存器或两个变址寄存器。 例如：<code>MOV AX, [BX][SI]</code>  （执行后AL的物理地址 = DSx16 + BX+SI， AH的物理地址 = DS x16 + BX+SI +1 ）</p>\n</blockquote>\n<h3 id=\"基址、变址、相对寻址\"><a href=\"#基址、变址、相对寻址\" class=\"headerlink\" title=\"基址、变址、相对寻址\"></a>基址、变址、相对寻址</h3><blockquote>\n<p>基址-变址-相对寻址方式是基址-变址寻址方式的扩充，指令中指定一个基址寄存器和一个变址寄存器，同时还各处一个8为或者16位的位移量。 操作数的偏移地址 等于 三者之和 默认情况下，段寄存器由基址寄存器决定 允许段重设 同样，不允许同时出现两个基址寄存器或两个变址寄存器。 例如：<code>MOV AX, 5[DI][BX]</code>  （段寄存器为DS，偏移地址为BX + DI + 5 的连续两个单元的内容送到AX中）</p>\n</blockquote>\n<h3 id=\"隐含寻址\"><a href=\"#隐含寻址\" class=\"headerlink\" title=\"隐含寻址\"></a>隐含寻址</h3><blockquote>\n<p>操作数隐含在指令码中 例如：<code>MUL</code> （AL x BL ——&gt;AX）</p>\n</blockquote>\n<h2 id=\"指令系统\"><a href=\"#指令系统\" class=\"headerlink\" title=\"指令系统\"></a><strong>指令系统</strong></h2><h3 id=\"指令的概述\"><a href=\"#指令的概述\" class=\"headerlink\" title=\"指令的概述\"></a><strong>指令的概述</strong></h3><h4 id=\"CISC指令系统\"><a href=\"#CISC指令系统\" class=\"headerlink\" title=\"CISC指令系统\"></a>CISC指令系统</h4><blockquote>\n<p>CISC(Complex Instruction Set Computer)，复杂指令系统计算机。<br>CISC指令的设计目标是增强指令的功能，将一些原来用软件是西安的、常用的功能变成用硬件的指令系统来实现。</p>\n</blockquote>\n<ul>\n<li><p><strong>缺点</strong></p>\n<blockquote>\n<p>难以使用</p>\n</blockquote>\n</li>\n<li><p><strong>优点</strong>：</p>\n<blockquote>\n<p>指令经编译后生成的指令程序较小、执行起来较快、节省硬件资源、存取指令的次数少、占用较少的存储器。</p>\n</blockquote>\n</li>\n<li><p><strong>存在的三个方面的问题</strong>：</p>\n<blockquote>\n<ol>\n<li>“8020规律”：20%的指令在各种应用程序中出现频率占整个指令系统的80%。</li>\n<li>CISC指令系统中有大量的复杂指令，控制逻辑极不规整，给VLSI（超大规模集成电路）工艺造成了极大的困难</li>\n<li>CISC增加了许多复杂指令，这些指令虽然简化了目标程序、缩小了高级语言与机器语言之间的差距，但使程序总的执行时间变长、硬件的复杂度增加。</li>\n</ol>\n</blockquote>\n</li>\n</ul>\n<h4 id=\"RISC指令系统\"><a href=\"#RISC指令系统\" class=\"headerlink\" title=\"RISC指令系统\"></a>RISC指令系统</h4><blockquote>\n<p>RISC，精简指令系统计算机。<br>一种计算机体系结构的设计思想，不是产品<br>核心思想是通过简化指令来使计算机的结构更加简单、合理，从而提高CPU的运算速度。</p>\n</blockquote>\n<h5 id=\"RISC的特点：\"><a href=\"#RISC的特点：\" class=\"headerlink\" title=\"RISC的特点：\"></a>RISC的特点：</h5><blockquote>\n<ul>\n<li>大多数指令在一个计算机周期内完成</li>\n<li>指令系统中应尽量减少访问存储器的指令，而采用寄存器与寄存器之间的操作</li>\n<li>减少寻址方式的种类。复杂的寻址方式用简单的寻址方式合成</li>\n<li>减少指令的种类。复杂的指令用软件实现</li>\n<li>指令格式简单</li>\n</ul>\n</blockquote>\n<h3 id=\"8086指令系统\"><a href=\"#8086指令系统\" class=\"headerlink\" title=\"8086指令系统\"></a><strong>8086指令系统</strong></h3><h4 id=\"指令概述\"><a href=\"#指令概述\" class=\"headerlink\" title=\"指令概述\"></a>指令概述</h4><h5 id=\"指令\"><a href=\"#指令\" class=\"headerlink\" title=\"指令\"></a>指令</h5><p>指令的一般格式为  操作码 目标操作数 源操作数</p>\n<ul>\n<li><p><strong>零操作数指令</strong>：</p>\n<blockquote>\n<p>形式上只有操作码，操作数是隐含存在的，操作对象通常为处理器本身</p>\n</blockquote>\n</li>\n<li><p><strong>单操作数指令</strong>：</p>\n<blockquote>\n<p>指令中仅给出一个操作数，另一个操作数隐含存在</p>\n</blockquote>\n</li>\n<li><p><strong>双操作数指令</strong>：</p>\n<blockquote>\n<p>一般格式</p>\n</blockquote>\n</li>\n</ul>\n<h5 id=\"操作数\"><a href=\"#操作数\" class=\"headerlink\" title=\"操作数\"></a>操作数</h5><ul>\n<li><p><em>*立即操作数</em>：</p>\n<blockquote>\n<p>立即数是指具有固定数值的操作数，即常数，不因指令的执行而发生变化。8086系统中字长可以是1字节或者2字节；可以是无符号数或有符号数</p>\n</blockquote>\n</li>\n<li><p><strong>寄存器操作数</strong>：</p>\n<blockquote>\n<p>8086CPU的8个通用寄存器和4个段寄存器可以作为指令中的寄存器操作数，源操作数和目标操作数均可</p>\n</blockquote>\n</li>\n<li><p>存储器操作数：</p>\n<blockquote>\n<p>其含义是参加运算的数据是存放在内存中的。通常为8位或16位字长</p>\n</blockquote>\n</li>\n</ul>\n<h4 id=\"数据传送指令\"><a href=\"#数据传送指令\" class=\"headerlink\" title=\"数据传送指令\"></a>数据传送指令</h4><h5 id=\"通用数据传送指令\"><a href=\"#通用数据传送指令\" class=\"headerlink\" title=\"通用数据传送指令\"></a>通用数据传送指令</h5><ul>\n<li><p><strong>一般数据传送指令</strong></p>\n<blockquote>\n<p>原则：<br>（1）两操作数字长必须相同<br>​（2）两操作数不允许同时为存储器操作数<br>​（3）两操作数不允许同时为段寄存器<br>​（4）在源操作数是立即数时，目标操作数不能是段寄存器<br>​（5）IP和CS不作为目标操作数，FLAGS一般也不作为操作数在指令中出现</p>\n<ul>\n<li><p><code>MOV</code></p>\n<blockquote>\n<ul>\n<li>格式：<code>MOV dest, src</code></li>\n<li>执行过程：<code>dest ——&gt; src</code></li>\n<li>举例：：<code>MOV AL, BL</code></li>\n</ul>\n</blockquote>\n</li>\n</ul>\n</blockquote>\n</li>\n<li><p><strong>堆栈操作指令</strong></p>\n<blockquote>\n<p>原则：<br>（1）先进后出<br>（2）以字为单位<br>（3）指令操作数必须是16位<br>（4）操作数可以是寄存器或存储器两单元，但不能是立即数<br>（5）不能从栈顶弹出一个字给CS<br>（6）PUSH和POP指令在程序中一般成对出现<br>（7）PUSH指令的操作方向是从高地址向低地址，而POP指令的操作正好相反</p>\n<ul>\n<li><p><code>PUSH</code>（压栈指令）：</p>\n<blockquote>\n<p><code>OPRD</code>：16存储器或存储器两单元</p>\n<ul>\n<li><p>格式：<code>PUSH OPRD</code></p>\n</li>\n<li><p>执行过程</p>\n<blockquote>\n<p><code>SP -2 ——&gt;SP</code><br>操作数高字节 ——&gt; <code>SP+1</code><br>操作数低字节 ——&gt;<code>SP</code></p>\n</blockquote>\n</li>\n</ul>\n</blockquote>\n</li>\n</ul>\n</blockquote>\n</li>\n<li><p><strong><code>POP</code>（出栈指令）：</strong></p>\n<blockquote>\n<ul>\n<li><p>格式：<code>POP OPRD</code></p>\n</li>\n<li><p>执行过程：</p>\n<blockquote>\n<p><code>SP</code> ——&gt; 操作数低字节<br><code>SP+ 1</code> ——&gt; 操作数高字节<br><code>SP</code> &lt;—— <code>SP +2</code></p>\n</blockquote>\n</li>\n</ul>\n</blockquote>\n</li>\n<li><p><strong>交换指令</strong></p>\n<blockquote>\n<p>原则：<br>（1）两操作数必须有一个是寄存器操作数<br>（2）不允许使用段寄存器</p>\n<ul>\n<li><p><code>XCHG</code></p>\n<blockquote>\n<ul>\n<li><p>格式：<code>XCHG REG, MEM/REG</code></p>\n</li>\n<li><p>操作：<br>  <code>REG &lt;——&gt; MEM/ REG</code></p>\n</li>\n<li><p>例子：<br>  <code>XCHG AX, BX</code><br>  <code>XCHG [2000],CL</code></p>\n</li>\n</ul>\n</blockquote>\n</li>\n</ul>\n</blockquote>\n</li>\n<li><p><strong>查表转换指令</strong></p>\n<blockquote>\n<p>原则：<br>（1）零操作数指令<br>（2）用BX的内容代表表格首地址，AL内容为表内偏移量，BX+AL得到要查找元素的偏移地址</p>\n<ul>\n<li><p>XLAT</p>\n<blockquote>\n<ul>\n<li><p>格式：XLAT</p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p>将BX+AL所指单元的内容送给AL</p>\n</blockquote>\n</li>\n</ul>\n</blockquote>\n</li>\n</ul>\n</blockquote>\n</li>\n<li><p><strong>字位扩展指令</strong></p>\n<blockquote>\n<p>原则：<br>（1）将符号数的符号位扩展到高位<br>（2）指令为零操作数，采用隐含寻址，隐含的操作数为AX及AX，DX<br>（3）无符号数的扩展规则为在高位补0</p>\n<ul>\n<li><p><code>CBW</code>（字节到字的扩展指令）</p>\n<blockquote>\n<ul>\n<li><p>格式：<code>CBW</code></p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p>将AL内容扩展到AX<br>若最高位 = 1，则执行后 AH =FFH<br>若最高位 = 0，则执行后AH = 00H</p>\n</blockquote>\n</li>\n</ul>\n</blockquote>\n</li>\n<li><p><code>CWD</code>（字到双子的扩展指令）</p>\n<blockquote>\n<ul>\n<li><p>格式：<code>CWD</code></p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p>将 <code>AX</code> 内容扩展到 <code>DX</code> <code>AX</code></p>\n<p>若最高位 = 1，则执行后DX = FFFFH</p>\n<p>若最高位 = 0，则执行后DX = 0000H</p>\n</blockquote>\n</li>\n</ul>\n</blockquote>\n</li>\n</ul>\n</blockquote>\n</li>\n</ul>\n<h5 id=\"输入输出指令\"><a href=\"#输入输出指令\" class=\"headerlink\" title=\"输入输出指令\"></a><strong>输入输出指令</strong></h5><ul>\n<li><p>输入指令</p>\n<ul>\n<li><p>IN</p>\n<ul>\n<li><p>指令：<code>IN acc, PORT</code></p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p>从端口地址读入数据到累加器中</p>\n</blockquote>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>输出指令</p>\n<ul>\n<li><p>OUT</p>\n<ul>\n<li><p>指令：<code>OUT PORT, acc</code></p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p>将累加器的值输出到端口中</p>\n</blockquote>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h5 id=\"地址传送指令\"><a href=\"#地址传送指令\" class=\"headerlink\" title=\"地址传送指令\"></a><strong>地址传送指令</strong></h5><ul>\n<li><p><strong>取近地址指令</strong></p>\n<ul>\n<li><p><code>LEA</code>：</p>\n<ul>\n<li><p>格式：<code>LEA REG, MEM</code>（必须是存储器操作数）</p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p>将变量得16位偏移地址写入到目标寄存器</p>\n</blockquote>\n</li>\n<li><p>要求：</p>\n<blockquote>\n<p>源操作数必须是一个存储器操作数，目标操作数通常是间址</p>\n</blockquote>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong>取远地址指令</strong></p>\n<ul>\n<li><p><code>LDS</code>：</p>\n<ul>\n<li><p>格式：<code>LDS</code>  通用寄存器，存储器操作数</p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p>将源操作数得偏移地址送目标寄存器，将源操作数得地址送DS</p>\n</blockquote>\n</li>\n</ul>\n</li>\n<li><p><code>LES</code>：</p>\n<ul>\n<li><p>格式：<code>LES</code> 通用寄存器，存储器操作数</p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p>将源操作数得偏移地址送目标寄存器，将源操作数得地址送ES</p>\n</blockquote>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h5 id=\"标志传送指令\"><a href=\"#标志传送指令\" class=\"headerlink\" title=\"标志传送指令\"></a><strong>标志传送指令</strong></h5><ul>\n<li><p>隐含操作数AH</p>\n<ul>\n<li><p><code>LAHF</code>(Load AH from Flags)</p>\n<ul>\n<li><p>格式：<code>LAHF</code></p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p>将<code>FLAGS</code>的低8位装入<code>AH</code></p>\n</blockquote>\n</li>\n</ul>\n</li>\n<li><p><code>SAHF</code>(Store AH into Flags)</p>\n<ul>\n<li><p>格式：<code>SHAF</code></p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p>与<code>LAHF</code>相反</p>\n</blockquote>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>隐含操作数FLAGS</p>\n<ul>\n<li>PUSHF(Push flags onto stack)</li>\n<li>POPF(Pop flags off stack)</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"算术运算指令\"><a href=\"#算术运算指令\" class=\"headerlink\" title=\"算术运算指令\"></a><strong>算术运算指令</strong></h4><h5 id=\"加法运算指令\"><a href=\"#加法运算指令\" class=\"headerlink\" title=\"加法运算指令\"></a>加法运算指令</h5><ul>\n<li><p><code>ADD</code>（普通加法指令）</p>\n<p>  <strong>ADD指令的执行对全部6个状态标志位都产生影响</strong></p>\n<ul>\n<li><p>格式：<code>ADD OPRD1, OPRD2</code></p>\n</li>\n<li><p>操作:</p>\n<blockquote>\n<p><code>OPRD1 + OPRD2 ——&gt; OPRD1</code></p>\n</blockquote>\n</li>\n<li><p>例子：</p>\n<blockquote>\n<p><code>ADD AL, 99H</code>（）</p>\n</blockquote>\n</li>\n</ul>\n</li>\n<li><p><code>ADC</code>（带进位的加法指令）</p>\n<p>  <strong>ADC指令都用于多字节数相加，使用前要先将CF清零</strong></p>\n<ul>\n<li><p>格式：<code>ADC OPRD1, OPRD2</code></p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p><code>OPRD1 + OPRD2 + CF ——&gt; OPRD1</code></p>\n</blockquote>\n</li>\n</ul>\n</li>\n<li><p><code>INC</code>（加1指令）</p>\n<p>  <strong>常用于程序中修改地址指针</strong></p>\n<ul>\n<li><p>格式：<code>INC OPRD</code></p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p><code>OPRD + 1 ——&gt; OPRD</code></p>\n</blockquote>\n</li>\n</ul>\n</li>\n</ul>\n<h5 id=\"减法运算指令\"><a href=\"#减法运算指令\" class=\"headerlink\" title=\"减法运算指令\"></a>减法运算指令</h5><ul>\n<li><p><code>SUB</code> （普通减法指令）</p>\n<p>  <strong>对标志位的影响与ADD指令相同</strong></p>\n<ul>\n<li><p>格式：<code>SUB OPRD1, OPRD2</code></p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p><code>OPRD1 - OPRD2 ——&gt; OPRD1</code></p>\n</blockquote>\n</li>\n</ul>\n</li>\n<li><p><code>SBB</code>（考虑借位的减法指令）</p>\n<ul>\n<li><p>格式：<code>SBB OPRD1, OPRD2</code></p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p><code>OPRD1 - OPRD2 - CF——&gt; OPRD1</code></p>\n</blockquote>\n</li>\n</ul>\n</li>\n<li><p><code>DEC</code>（减1指令DEC）</p>\n<p>  <strong>指令对操作数的要求与INC相同，指令常用于在程序中修改计数值</strong></p>\n<ul>\n<li><p>格式：<code>DEC OPRD</code></p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p><code>OPRD -1 ——&gt; OPRD</code></p>\n</blockquote>\n</li>\n<li><p>应用程序例子：</p>\n<p>  <img src=\"/2023/03/06/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/%E8%80%83%E7%A0%94/%E5%BE%AE%E6%9C%BA%E5%8E%9F%E7%90%86%E5%8F%8A%E6%8E%A5%E5%8F%A3%E6%8A%80%E6%9C%AF/image-20230308223135359.png\" alt=\"image-20230308223135359\"></p>\n</li>\n</ul>\n</li>\n<li><p><code>CMP</code>（比较指令）</p>\n<p>  用于比较两个数的大小，可作为条件转移指令转移的条件</p>\n<p>  指令执行的结果不影响目标操作数，仅影响标志位！</p>\n<p>  对操作数的要求及对标志位的影响与SUB指令相同</p>\n<ul>\n<li><p>格式：<code>CMP OPRD1, OPRD2</code></p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p><code>OPRD1 - OPRD2</code></p>\n</blockquote>\n</li>\n<li><p>例子：</p>\n<ul>\n<li><p>两个无符号数的比较</p>\n<blockquote>\n<p><code>CMP AX, BX</code></p>\n<p>若 AX &gt;= BX ——&gt; CF =0</p>\n<p>若AX &lt; BX ——&gt; CF = 1</p>\n<p>若 AX = BX ——&gt; CF =1,ZF =1</p>\n</blockquote>\n</li>\n<li><p>两个带符号数的比较</p>\n<blockquote>\n<p>CMP AX, BX</p>\n<p>两个数的大小由OF和SF共同决定</p>\n<p>OF和SF状态相同 AX &gt;= BX</p>\n<p>OF和SF状态不同 AX &lt; BX</p>\n</blockquote>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><code>NEG</code>（求补指令）</p>\n<p>  <strong>对一个负数取补码就相当于用零减去此数</strong></p>\n<ul>\n<li><p>格式：<code>NEG OPRD</code></p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p><code>0 - OPRD ——&gt; OPRD</code></p>\n</blockquote>\n</li>\n<li><p>说明：</p>\n<ul>\n<li>执行NEG指令后，一般情况下都会使CF为1，除非给定的操作数为零才会使CF为0；</li>\n<li>当指定的操作数的值为80H(-128)或为8000H(-32768)，则执行NEG指令后，结果不变，但OF置1，其它情况下OF置0。</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h5 id=\"乘法指令\"><a href=\"#乘法指令\" class=\"headerlink\" title=\"乘法指令\"></a>乘法指令</h5><p>乘法指令采用隐含寻址，隐含的是存放被乘数的累加器AL或AX及存放结果的AX，DX</p>\n<ul>\n<li><p><strong><code>MUL</code>无符号的乘法指令</strong></p>\n<ul>\n<li><p>格式：<code>MUL OPRD</code>（OPRD不能是立即数）</p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p>OPRD为8字节数 ===&gt; AL x OPRD ——&gt; AX</p>\n<p>OPRD为16位数 ====&gt; AX x OPRD ——&gt; DXAX </p>\n</blockquote>\n</li>\n<li><p>例子：</p>\n<blockquote>\n<p><img src=\"/2023/03/06/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/%E8%80%83%E7%A0%94/%E5%BE%AE%E6%9C%BA%E5%8E%9F%E7%90%86%E5%8F%8A%E6%8E%A5%E5%8F%A3%E6%8A%80%E6%9C%AF/image-20230308232215011-16782889368582.png\" alt=\"image-20230308232215011\"></p>\n</blockquote>\n</li>\n</ul>\n</li>\n<li><p><strong><code>IMUL</code>带符号的乘法指令</strong></p>\n<ul>\n<li><p>格式：<code>IMUL OPRD</code>（OPRD不能是立即数，隐含操作数为AL，存放在AX中）</p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<ol>\n<li>将两个操作数取补码（对负数按位取反加1，正数不变）</li>\n<li>做乘法运算</li>\n<li>将乘积按位取反加 1</li>\n</ol>\n</blockquote>\n</li>\n</ul>\n</li>\n</ul>\n<h5 id=\"除法指令\"><a href=\"#除法指令\" class=\"headerlink\" title=\"除法指令\"></a>除法指令</h5><p>指令要求被除数是除数的双倍字长</p>\n<ul>\n<li><p><strong>无符号除法</strong></p>\n<ul>\n<li><p>格式：<code>DIV OPRD</code></p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p>若<code>OPRD</code>是字节数：</p>\n<ul>\n<li>执行：<code>AX/OPRD</code></li>\n<li>结果：AL = 商   AH = 余数</li>\n</ul>\n<p>若<code>OPRD</code>是双字节数：</p>\n<ul>\n<li>执行：<code>DXAX/OPRD</code></li>\n<li>结果：AX = 商 DX = 余数</li>\n</ul>\n</blockquote>\n</li>\n</ul>\n</li>\n<li><p><strong>有符号除法</strong></p>\n<ul>\n<li><p>格式：<code>IDIV OPRD</code></p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p>若OPRD是字节数：</p>\n<ul>\n<li>执行：<code>AX/OPRD</code></li>\n<li>结果：AL = 商   AH = 余数</li>\n</ul>\n<p>若OPRD是双字节数：</p>\n<ul>\n<li>执行：<code>DXAX/OPRD</code></li>\n<li>结果：AX = 商 DX = 余数</li>\n</ul>\n</blockquote>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"逻辑运算和移位指令\"><a href=\"#逻辑运算和移位指令\" class=\"headerlink\" title=\"逻辑运算和移位指令\"></a><strong>逻辑运算和移位指令</strong></h4><h5 id=\"逻辑运算指令\"><a href=\"#逻辑运算指令\" class=\"headerlink\" title=\"逻辑运算指令\"></a>逻辑运算指令</h5><blockquote>\n<ul>\n<li><strong>对操作数的要求</strong>：<ul>\n<li>大多数与<code>MOV</code>指令相同</li>\n<li>“非” 运算指令要求操作数不能是立即数</li>\n</ul>\n</li>\n<li><strong>对标志位的影响</strong><ul>\n<li>除“非”运算指令，其余指令的执行都会影响除AF外的五个状态标志</li>\n<li>无论执行结果如何，都会使标志位<code>OF=CF=0</code></li>\n<li>“非”运算指令的执行不影响标志位</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<ul>\n<li><p><strong>“与”指令</strong></p>\n<ul>\n<li><p>格式：<code>AND OPRD1，OPRD2</code></p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p>两操作数相“与”，结果送目标地址。</p>\n</blockquote>\n</li>\n<li><p>应用：</p>\n<ul>\n<li>实现两操作数按位相与的运算：<code>AND BL, [SI]</code></li>\n<li>使目标操作数的某些位不变，某些位清零：<code>AND AL, 0FH</code></li>\n<li>在操作数不变的情况向使CF和OF清零：<code>AND AX, AX</code></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>“或”运算指令</p>\n<ul>\n<li>格式</li>\n<li>操作</li>\n<li>应用</li>\n</ul>\n</li>\n<li><p>“非”运算指令</p>\n<ul>\n<li><p>格式：NOT OPRD</p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p>操作数按位取反在送回原地址，指令的执行对标志位无影响</p>\n</blockquote>\n</li>\n<li><p>例子：</p>\n<blockquote>\n</blockquote>\n</li>\n</ul>\n</li>\n<li><p>“异或”运算指令</p>\n<ul>\n<li><p>格式：<code>XOR OPRD1, OPRD2</code></p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p>两操作数“异或”，结果送目标地址</p>\n</blockquote>\n</li>\n<li><p>例子：</p>\n</li>\n</ul>\n</li>\n<li><p>“测试”指令</p>\n<ul>\n<li><p>格式：<code>TEST OPRD1, OPRD2</code></p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p>执行“与”运算，但运算的结果不送回目标地址</p>\n</blockquote>\n</li>\n<li><p>应用：</p>\n<blockquote>\n<p>常用于测试某些位的状态</p>\n</blockquote>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"串操作指令\"><a href=\"#串操作指令\" class=\"headerlink\" title=\"串操作指令\"></a>串操作指令</h4><h4 id=\"程序控制指令\"><a href=\"#程序控制指令\" class=\"headerlink\" title=\"程序控制指令\"></a>程序控制指令</h4><h4 id=\"处理器控制指令\"><a href=\"#处理器控制指令\" class=\"headerlink\" title=\"处理器控制指令\"></a>处理器控制指令</h4><h2 id=\"汇编语言程序设计\"><a href=\"#汇编语言程序设计\" class=\"headerlink\" title=\"汇编语言程序设计\"></a><strong>汇编语言程序设计</strong></h2><h3 id=\"伪指令\"><a href=\"#伪指令\" class=\"headerlink\" title=\"伪指令\"></a><strong>伪指令</strong></h3><h3 id=\"BIOS和DOS功能调用\"><a href=\"#BIOS和DOS功能调用\" class=\"headerlink\" title=\"BIOS和DOS功能调用\"></a><strong>BIOS和DOS功能调用</strong></h3><h3 id=\"汇编语言程序设计基础\"><a href=\"#汇编语言程序设计基础\" class=\"headerlink\" title=\"汇编语言程序设计基础\"></a><strong>汇编语言程序设计基础</strong></h3><blockquote>\n<h4 id=\"程序设计概述\"><a href=\"#程序设计概述\" class=\"headerlink\" title=\"程序设计概述\"></a><strong>程序设计概述</strong></h4><h4 id=\"顺序结构\"><a href=\"#顺序结构\" class=\"headerlink\" title=\"顺序结构\"></a><strong>顺序结构</strong></h4><h4 id=\"分支结构\"><a href=\"#分支结构\" class=\"headerlink\" title=\"分支结构\"></a><strong>分支结构</strong></h4><h4 id=\"循环结构\"><a href=\"#循环结构\" class=\"headerlink\" title=\"循环结构\"></a><strong>循环结构</strong></h4><h4 id=\"子程序\"><a href=\"#子程序\" class=\"headerlink\" title=\"子程序\"></a><strong>子程序</strong></h4><h4 id=\"常用程序设计举例\"><a href=\"#常用程序设计举例\" class=\"headerlink\" title=\"常用程序设计举例\"></a><strong>常用程序设计举例</strong></h4></blockquote>\n<h2 id=\"存储器系统\"><a href=\"#存储器系统\" class=\"headerlink\" title=\"存储器系统\"></a><strong>存储器系统</strong></h2><h3 id=\"随机存取存储器\"><a href=\"#随机存取存储器\" class=\"headerlink\" title=\"随机存取存储器\"></a><strong>随机存取存储器</strong></h3><h3 id=\"只读存储器\"><a href=\"#只读存储器\" class=\"headerlink\" title=\"只读存储器\"></a><strong>只读存储器</strong></h3><h3 id=\"高速缓冲存储器\"><a href=\"#高速缓冲存储器\" class=\"headerlink\" title=\"高速缓冲存储器\"></a><strong>高速缓冲存储器</strong></h3><h3 id=\"存储器扩展技术\"><a href=\"#存储器扩展技术\" class=\"headerlink\" title=\"存储器扩展技术\"></a><strong>存储器扩展技术</strong></h3>","site":{"data":{}},"cover":"/img/covers/4.jpg","cover_type":"img","excerpt":"","more":"<h1 id=\"《微机原理及接口技术-吴宁》-学习记录\"><a href=\"#《微机原理及接口技术-吴宁》-学习记录\" class=\"headerlink\" title=\"《微机原理及接口技术 吴宁》 学习记录\"></a>《微机原理及接口技术 吴宁》 学习记录</h1><h2 id=\"微信计算机基础\"><a href=\"#微信计算机基础\" class=\"headerlink\" title=\"微信计算机基础\"></a>微信计算机基础</h2><h3 id=\"微型计算机概述\"><a href=\"#微型计算机概述\" class=\"headerlink\" title=\"微型计算机概述\"></a>微型计算机概述</h3><p>计算机系统是一种由硬件系统和软件系统组成的复杂电子装置，它能够存储程序、存储原始数据、中间结果和最终运算结果，并自动完成运算，是一种能对各种数字化处理的 “信息处理机”。微型计算机是一种小型的、可编程的计算机，它可以用于执行复杂的计算任务。它们通常由一个或多个微处理器组成，可以存储和处理大量的数据，并且可以运行复杂的程序。它们可以用于控制机器和设备，以及用于收集、处理和传输数据。</p>\n<h4 id=\"微机的发展\"><a href=\"#微机的发展\" class=\"headerlink\" title=\"微机的发展\"></a>微机的发展</h4><p>自1946年第一台计算机问世已在，已经历了由电子管计算机、晶体管计算机、集成电路计算机到大规模、超规模集成电路计算机这样五代的更替、并且在不断地向巨型化、微型化、网络化和智能化这4个方向发展。</p>\n<h5 id=\"微机的工作过程\"><a href=\"#微机的工作过程\" class=\"headerlink\" title=\"微机的工作过程\"></a>微机的工作过程</h5><ul>\n<li>冯·诺依曼计算机的主要特点</li>\n</ul>\n<p><img src=\"/2023/03/06/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/%E8%80%83%E7%A0%94/%E5%BE%AE%E6%9C%BA%E5%8E%9F%E7%90%86%E5%8F%8A%E6%8E%A5%E5%8F%A3%E6%8A%80%E6%9C%AF/微机原理及接口技术.jpg\" alt=\"微机原理及接口技术\" style=\"zoom:80%;\"></p>\n<p>（1）将计算过程描述为由许多条指令按一定顺序组成的程序、并放入存储器保存 </p>\n<p>（2）程序中的指令和数据必须采用2进制编码，且能够被执行该程序的计算机所识别 </p>\n<p>（3）指令按其在存储器中存放的顺序执行，存储器的字长固定并按顺序线性编制 </p>\n<p>（4）由控制器控制整个程序和数据的存取以及程序的执行 </p>\n<p>（5）以运算器为核心，所有的执行都经过运算器 </p>\n<ul>\n<li><p>微型计算机的工作过程</p>\n<p> 微机的工作过程就是执行程序的过程，也就是逐条执行指令系列的过程。由于每条指令都包括取指令和执行指令两个基本阶段，所以微机的工作过程也是不断地去取指令和执行指令的过程。当计算机进入运行状态时： </p>\n<p> （1）首先将第一条指令由内存取出 </p>\n<p> （2）将取出的指令送给指令译码器译码，以确定要进行的操作 </p>\n<p> （3）读取相应的操作数（即执行的对象）</p>\n<p> （4）执行指令 </p>\n<p> （5）存放执行结果 </p>\n<p> （6）一条指令执行完毕，转到下一条指令的执行阶段</p>\n</li>\n</ul>\n<h4 id=\"微机的特点\"><a href=\"#微机的特点\" class=\"headerlink\" title=\"微机的特点\"></a>微机的特点</h4><p>运算速度快、计算精度高、记忆功能和逻辑判断功能强，可自动连续工作等基本特点。</p>\n<ul>\n<li><p>功能强、可靠性高 </p>\n</li>\n<li><p>格低廉 </p>\n</li>\n<li><p>系统设计灵活，适应性强 </p>\n</li>\n<li><p>体积小，重量轻，维护方便</p>\n</li>\n</ul>\n<h3 id=\"微型计算机系统的组成\"><a href=\"#微型计算机系统的组成\" class=\"headerlink\" title=\"微型计算机系统的组成\"></a>微型计算机系统的组成</h3><h4 id=\"硬件系统\"><a href=\"#硬件系统\" class=\"headerlink\" title=\"硬件系统\"></a>硬件系统</h4><h5 id=\"CPU（微处理器或中央处理器）\"><a href=\"#CPU（微处理器或中央处理器）\" class=\"headerlink\" title=\"CPU（微处理器或中央处理器）\"></a>CPU（微处理器或中央处理器）</h5><ul>\n<li><strong>运算器</strong></li>\n</ul>\n<p>核心部件是算数逻辑单元（ALU，Arithmetic and Logic Unit）,在控制信号的作用下可完成加、减、乘、除四则运算和各种逻辑运算。新型CPU运算器还可完成浮点运算</p>\n<ul>\n<li><p><strong>控制器</strong></p>\n<p>  一般由指令寄存器、指令译码器和 操作控制电路组成。是CPU的控制中心，它从存储器中依次取出程序的各条指令，并根据指令的要求，向微机的各个部件发出相应的控制信号。</p>\n</li>\n<li><p><strong>寄存器组</strong></p>\n<p>  实质上是CPU内部若干个存储单元，可分为专用寄存器和通用寄存器。 专用寄存器：堆栈指针、程序计数器、标志寄存器等 通用寄存器：…….</p>\n</li>\n<li><p><strong>存储器</strong></p>\n<p>  主机系统的存储器又叫做主存或内存，用以存放数据（包括原始数据、中间结果、最终结果）和当前执行的程序</p>\n</li>\n<li><p><strong>输入输出接口和输入输出设备</strong>（I/O接口与I/O设备）</p>\n<p>  常用的输入设备有：键盘、鼠标器、扫描仪等 常用的输出设备有：显示器、打印机、绘图仪等 I/O接口：I/O设备之间信息交换的桥梁</p>\n</li>\n<li><p><strong>总线</strong></p>\n<p>  数据总线DB：传输数据信息、双向 地址总线AB：传送CPU发出的地址信息、单向，指明与CPU交换信息的内存单元或者I/O设备 控制总线CB：传送控制信号、时序信号和状态信息等，CB中的每根线是单向的，但CB整体是双向的</p>\n</li>\n</ul>\n<h4 id=\"软件系统\"><a href=\"#软件系统\" class=\"headerlink\" title=\"软件系统\"></a>软件系统</h4><h5 id=\"系统软件\"><a href=\"#系统软件\" class=\"headerlink\" title=\"系统软件\"></a>系统软件</h5><p>系统软件包括操作系统（OS）和系统实用程序。 操作系统：用于管理计算机的硬件和软件资源、进行任务调度、提供文件管理系统、人机接口等，包含了各种I/O设备的驱动程序 系统实用程序： 包括各种高级语言的翻译/编译程序、汇编程序、数据库系统、文本编辑程序以及诊断和调试程序，此外还包括许多系统工具程序等。</p>\n<h5 id=\"应用软件\"><a href=\"#应用软件\" class=\"headerlink\" title=\"应用软件\"></a>应用软件</h5><p>应用软件是用户为解决各种实际问题（如数学计算、检测与实时控制、音乐播放等）而编制的程序。</p>\n<h3 id=\"数制与编码\"><a href=\"#数制与编码\" class=\"headerlink\" title=\"数制与编码\"></a>数制与编码</h3><h4 id=\"进制\"><a href=\"#进制\" class=\"headerlink\" title=\"进制\"></a>进制</h4><ul>\n<li><p><strong>十进制</strong>：所有数都用0~9这10个符号的组合来表示，用D标识，逢十进一 </p>\n</li>\n<li><p><strong>二进制</strong>：每一位只取0和1两个数字符号，用B标识，逢二进一 </p>\n</li>\n<li><p><strong>十六进制</strong>：每一位数都用0 -9和A~F这10个字符来组合，用H标识，逢十六进一</p>\n</li>\n</ul>\n<h4 id=\"进制转换\"><a href=\"#进制转换\" class=\"headerlink\" title=\"进制转换\"></a>进制转换</h4><ul>\n<li><p>十进制 ==> 二进制：</p>\n<blockquote>\n<p>整数部分：除“2”取余，直到商为0；</p>\n<p>小数部分：乘“2”取余，直到满足精度要求 </p>\n</blockquote>\n</li>\n</ul>\n<p>​        </p>\n<ul>\n<li><p>十进制 ==> 十六进制：** </p>\n<blockquote>\n<p>整数部分：除“16”取余，直到商为0；</p>\n<p>小数部分：乘“16”取余，直到满足进度要求 </p>\n</blockquote>\n</li>\n<li><p>二进制 ==&gt; 十进制 &amp;&amp; 二进制 ==> 十六进制：**</p>\n<blockquote>\n<p>按权展开</p>\n</blockquote>\n</li>\n</ul>\n<h4 id=\"BCD码\"><a href=\"#BCD码\" class=\"headerlink\" title=\"BCD码\"></a>BCD码</h4><p>计算机用二进制数编码来表示十进制数，常见的有用四位二进制来表示一位十进制 </p>\n<ul>\n<li><p><strong>非压缩BCD码：</strong></p>\n<blockquote>\n<p>用一个字节（8位）来表示一位十进制，高四位清零</p>\n</blockquote>\n</li>\n<li><p><strong>压缩BCD码：</strong></p>\n<blockquote>\n<p>用一个字节（8位）来表示两位十进制</p>\n</blockquote>\n</li>\n</ul>\n<h4 id=\"二进制数的运算\"><a href=\"#二进制数的运算\" class=\"headerlink\" title=\"二进制数的运算\"></a>二进制数的运算</h4><h5 id=\"算术运算：\"><a href=\"#算术运算：\" class=\"headerlink\" title=\"算术运算：\"></a><strong>算术运算：</strong></h5><blockquote>\n<p><strong>加：</strong>逢二进一</p>\n<p><strong>减：</strong>借一当二 </p>\n<p><strong>乘除：</strong></p>\n</blockquote>\n<h5 id=\"逻辑运算：\"><a href=\"#逻辑运算：\" class=\"headerlink\" title=\"逻辑运算：\"></a><strong>逻辑运算：</strong></h5><blockquote>\n<p><strong>与：</strong>有一个0即为0</p>\n<p><strong>或：</strong>有一个1即为1 </p>\n<p><strong>非：</strong>取反</p>\n<p><strong>异或：</strong>相同为0，不同为1</p>\n</blockquote>\n<h4 id=\"带符号在计算机中的表示\"><a href=\"#带符号在计算机中的表示\" class=\"headerlink\" title=\"带符号在计算机中的表示\"></a>带符号在计算机中的表示</h4><ul>\n<li><p><strong>机器数：</strong>连同书的符号一起数字化了的数据称为机器数。例如：X = +91 = 01011011B</p>\n</li>\n<li><p><strong>真值：</strong>与机器数相应的用正、负符号加绝对值来表示的世界数值。例如：X = +91 = +1011011B </p>\n</li>\n<li><p><strong>原码：</strong>是一种简单直观的机器数表示方法 反码：正数不变，负数符号位不变，取反加1 </p>\n</li>\n<li><p><strong>补码：</strong>正数不变，负数符号位不变，取反加1</p>\n</li>\n</ul>\n<h2 id=\"8086微处理器\"><a href=\"#8086微处理器\" class=\"headerlink\" title=\"8086微处理器\"></a>8086微处理器</h2><h3 id=\"微处理器概述\"><a href=\"#微处理器概述\" class=\"headerlink\" title=\"微处理器概述\"></a>微处理器概述</h3><h4 id=\"运算器：\"><a href=\"#运算器：\" class=\"headerlink\" title=\"运算器：\"></a>运算器：</h4><ul>\n<li><strong>单总线结构运算器</strong>：通过一条内部总线传递信息</li>\n<li><strong>双总线结构运算器</strong>：通过两条内部总线传递信息</li>\n<li><strong>三总线结构运算器</strong>：通过三条内部总线传递信息</li>\n</ul>\n<h4 id=\"控制器：\"><a href=\"#控制器：\" class=\"headerlink\" title=\"控制器：\"></a>控制器：</h4><h5 id=\"基本功能：\"><a href=\"#基本功能：\" class=\"headerlink\" title=\"基本功能：\"></a>基本功能：</h5><blockquote>\n<p>指令控制、时序控制、操作控制功能</p>\n<p>对异常情况及某些外部请求的处理能力，如运算溢出、中断请求等</p>\n</blockquote>\n<h5 id=\"组成部分：\"><a href=\"#组成部分：\" class=\"headerlink\" title=\"组成部分：\"></a>组成部分：</h5><ul>\n<li><p><strong>程序计数器</strong>：存放下一条要执行的指令在存储器中的地址，程序执行前应将程序的首地址置于程序计数器</p>\n</li>\n<li><p><strong>指令寄存器（Instruction Register, IR）</strong>：存放从存储器中取出的待执行的指令</p>\n</li>\n<li><p><strong>指令译码器（Instruction Decoder, ID）</strong>：“ 翻译 ”指令寄存器中的指令，即指令译码</p>\n</li>\n<li><p><strong>时序控制部件</strong>：产生计算机工作中所需的各种时序信号</p>\n</li>\n<li><p><strong>微操作控制部件</strong>：产生与各条指令对应的微操作</p>\n</li>\n</ul>\n<h3 id=\"8088CPU的外部引脚及其功能\"><a href=\"#8088CPU的外部引脚及其功能\" class=\"headerlink\" title=\"8088CPU的外部引脚及其功能\"></a>8088CPU的外部引脚及其功能</h3><h4 id=\"最小模式和最大模式\"><a href=\"#最小模式和最大模式\" class=\"headerlink\" title=\"最小模式和最大模式\"></a>最小模式和最大模式</h4><ul>\n<li><p>最小模式：</p>\n<blockquote>\n<p>是8088微处理器的一种操作模式，它只使用一个段寄存器来指定内存地址，这种模式下，8088只能使用1KB的内存空间。</p>\n</blockquote>\n</li>\n<li><p>最大模式：</p>\n<blockquote>\n<p>是8088微处理器的另一种操作模式，它使用两个段寄存器来指定内存地址，这种模式下，8088可以使用1MB的内存空间。</p>\n</blockquote>\n</li>\n</ul>\n<h3 id=\"8088-8086CPU的功能结构\"><a href=\"#8088-8086CPU的功能结构\" class=\"headerlink\" title=\"8088/8086CPU的功能结构\"></a>8088/8086CPU的功能结构</h3><h4 id=\"内部结构\"><a href=\"#内部结构\" class=\"headerlink\" title=\"内部结构\"></a>内部结构</h4><ul>\n<li><p><code>EU</code>（执行单元）：</p>\n<blockquote>\n<p>执行命令、分析命令、暂存中间运算结果并保留结果的特征。由ALU、通用寄存器、标志寄存器和EU控制电路组成。</p>\n</blockquote>\n</li>\n<li><p><code>BIU</code>（总线接口单元）：</p>\n<blockquote>\n<p>BIU负责CPU与存储器、I/O接口之间的信息传递。由段寄存器、指令寄存器、指令队列、地址加法器以及总线控制逻辑组成</p>\n</blockquote>\n</li>\n</ul>\n<h4 id=\"内部寄存器\"><a href=\"#内部寄存器\" class=\"headerlink\" title=\"内部寄存器\"></a>内部寄存器</h4><h5 id=\"通用寄存器（8个）\"><a href=\"#通用寄存器（8个）\" class=\"headerlink\" title=\"通用寄存器（8个）\"></a>通用寄存器（8个）</h5><ul>\n<li><p><strong>数据寄存器</strong>：AX、BX、CX、DX</p>\n<blockquote>\n<p><strong><code>AX</code>：</strong>累加器，常用于存放算术逻辑运算中的操作数，所有的I/O指令都使用累加器与外设接口传送信息</p>\n<p><strong><code>BX</code>：</strong>基址寄存器，常用来存放访问内存时的基地址</p>\n<p><strong><code>CX</code>：</strong>计数寄存器，在循环和串操作指令中用作计数器</p>\n<p><strong><code>DX</code>：</strong>数据寄存器，在寄存器间接寻址的I/O指令中存放I/O端口的地址。<br>注：在做双字长乘除运算时，DX与AX合起来存放一个双字长数（32位），其中<strong>DX存放高16位，AX存放低16位。</strong></p>\n</blockquote>\n</li>\n<li><p><strong>地址指针寄存器</strong>：SP、BP</p>\n<blockquote>\n<p><strong><code>SP</code> (Stack  Pointer)：</strong> 堆栈指针寄存器，堆栈操作中用来存放栈顶偏移地址，永远指向堆栈的栈顶<br><strong><code>BP</code> (Base Pointer)：</strong> 基址指针寄存器，常用来存放访问内存时的基地址，通常与SS搭配使用</p>\n</blockquote>\n</li>\n<li><p><strong>变址寄存器：SI、DI</strong></p>\n<blockquote>\n<p><code>SI</code> (Source Index)：源变址寄存器</p>\n<p><code>DI</code>： 目的变址寄存器</p>\n</blockquote>\n</li>\n<li><p><strong>段寄存器</strong>（4个）</p>\n<blockquote>\n<p><strong>段寄存器用于存放短地址，段起始地址的高16位</strong></p>\n<ul>\n<li><code>CS</code>： 代码段寄存器</li>\n<li><strong><code>SS</code>：</strong>堆栈段寄存器</li>\n<li><strong><code>DS</code>：</strong>数据段寄存器</li>\n<li><strong><code>ES</code>：</strong> 附加段寄存器</li>\n</ul>\n</blockquote>\n</li>\n<li><p><strong>控制寄存器</strong>（2个）</p>\n<blockquote>\n<p><code>IP</code>（指令指针寄存器）</p>\n<p>用于存放预取指令的偏移地址。CPU取指令时总是以CS为段基址，以IP为段内偏移地址。<br>CPU从CS段中偏移地址为IP的内存单元中取出指令代码的一个字节后，IP自动加1，只想指令代码的下一个字节。<br>用户程序不能直接访问IP</p>\n</blockquote>\n<ul>\n<li><p><code>FLAGS</code>（标志位寄存器）</p>\n<ul>\n<li><p>状态标志位</p>\n<blockquote>\n<p><strong>主要作用：</strong> 记录算术和逻辑运算结果的一些特征<br>CF：进位标志位，加（减）运算时，最高位向更高位有进（借）位时，<code>CF = 1</code>，否则，<code>CF =0</code><br>PF：奇偶标志位，运算结果低8位中1的个数为偶数时<code>PF = 1</code>，为奇数时，<code>PF =0</code><br>AF：辅助进位标志位，$D_3D3D_3D3$向$D_4D4D_4D4$ 有进（借）位时<code>AF=1</code>，否则<code>AF=0</code><br>ZF：零标志位，运算结果为0时<code>ZF = 1</code> ,否则<code>ZF = 0</code><br>SF：符号标志位，运算结果的最高位为1时 <code>SF = 1</code>，否则 <code>SF = 0</code><br>OF：溢出标志位，有溢出<code>OF = 1</code>,无溢出<code>OF = 0</code></p>\n</blockquote>\n</li>\n<li><p>控制标志位TF</p>\n<blockquote>\n<p>TF：陷阱标志位，<code>TF = 1</code>时激活处理器的调试特性，使CPU处于单步执行指令的工作方式。每执行一条指令自动产生一次单步中断，从而达到检查程序的目的。<br>IF：中断允许标志位，<code>IF = 1</code>，CPU可响应可屏蔽中断请求；<code>IF = 0</code>，CPU禁止响应可屏蔽中断请求。对不可屏蔽中断及内部中断没有影响。<br>DF：方向标志位，执行串操作指令时控制操作的方向。<code>DF = 1</code>，向减地址方向进行，即从高地址开始，每进行一次操作，地址指针自动减1（或减2）；<code>DF = 0</code>，则按增地址方式进行。</p>\n</blockquote>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"寻址方式\"><a href=\"#寻址方式\" class=\"headerlink\" title=\"寻址方式\"></a>寻址方式</h2><h3 id=\"立即寻址\"><a href=\"#立即寻址\" class=\"headerlink\" title=\"立即寻址\"></a>立即寻址</h3><blockquote>\n<p>只针对源操作数，此时源操作数是一个立即数（8位或16位），存放于内存的代码段中。<br>当立即数为16位时高地址存放高8位，低地址存放低8位。<br>例如：<code>MOV AX, 3120H</code> （20H——&gt; AL; 31H ——&gt; AH）</p>\n</blockquote>\n<h3 id=\"直接寻址\"><a href=\"#直接寻址\" class=\"headerlink\" title=\"直接寻址\"></a>直接寻址</h3><blockquote>\n<p>表示参数运算的数据存放在内存中，存放的地址由指令直接给出，即指令中的操作数时存储器操作数（带有’[ ]’）；</p>\n<p><code>&quot;[ ]&quot;</code>内用16位常数表示存放数据的偏移地址，数据的段基地址默认位数据段，可以允许重设。</p>\n</blockquote>\n<h3 id=\"存储器寻址\"><a href=\"#存储器寻址\" class=\"headerlink\" title=\"存储器寻址\"></a>存储器寻址</h3><blockquote>\n<p>指令的操作数为CPU的内部寄存器，可以是数据寄存器（8位或16位），也可以是地址指针、变址寄存器或段寄存器。 指令操作码存放在代码段，操作数在内部寄存器中，指令执行时不必通过访问内存就能取得操作数，执行速度较快 例如：<code>MOV SI, AX</code>   （将AX中的内容送到寄存器SI中）</p>\n</blockquote>\n<h3 id=\"寄存器间接寻址\"><a href=\"#寄存器间接寻址\" class=\"headerlink\" title=\"寄存器间接寻址\"></a>寄存器间接寻址</h3><blockquote>\n<p>用寄存器的内容表示操作数的偏移地址。 存放操作数偏移地址的寄存器只允许是SI、DI、BX和BP，他们可简称位间接寄存器或地址指针。 默认情况下，选择SI、DI、BX，操作数在数据段，段地址由DS决定；选择BP作间址寄存器，则操作数在堆栈段，段地址由SS决定。 无论哪个简介寄存器都允许重设 指令中 间接寄存器要加 [ ] 例如：<code>MOV AX, [SI]</code>（将数据段（DS）中以SI为偏移地址的单元中的内容送到AX中） 若操作数存放在附加段，则指令应为：<code>MOV AX, ES:[SI]</code></p>\n</blockquote>\n<h3 id=\"寄存器先对寻址\"><a href=\"#寄存器先对寻址\" class=\"headerlink\" title=\"寄存器先对寻址\"></a>寄存器先对寻址</h3><blockquote>\n<p>操作数在内存中存放地址（偏移地址）由间址寄存器的内容加上指令中的一个8位或者16位的位移量组成。 操作数所在段由所使用的间址寄存器决定（规则与寄存器间接寻址方式相同） 例如：<code>MOV AX, DATA[BX]</code>  （执行完后AX的物理地址为 DS*16 + BX+DATA）</p>\n</blockquote>\n<h3 id=\"基址、变址寻址\"><a href=\"#基址、变址寻址\" class=\"headerlink\" title=\"基址、变址寻址\"></a>基址、变址寻址</h3><blockquote>\n<p>基址-变址寻址方式由一个基址寄存器（BX或BP）的内容和一个变址寄存器（SI或DI）的内容相加而形成操作数的偏移地址。 默认情况下，若用BX作为基址寄存器，则段地址在DS中；如果用BP作为基址寄存器，则段地址在SS中。 允许段重设 使用基址-变址寻址方式，不允许同时出现两个基址寄存器或两个变址寄存器。 例如：<code>MOV AX, [BX][SI]</code>  （执行后AL的物理地址 = DSx16 + BX+SI， AH的物理地址 = DS x16 + BX+SI +1 ）</p>\n</blockquote>\n<h3 id=\"基址、变址、相对寻址\"><a href=\"#基址、变址、相对寻址\" class=\"headerlink\" title=\"基址、变址、相对寻址\"></a>基址、变址、相对寻址</h3><blockquote>\n<p>基址-变址-相对寻址方式是基址-变址寻址方式的扩充，指令中指定一个基址寄存器和一个变址寄存器，同时还各处一个8为或者16位的位移量。 操作数的偏移地址 等于 三者之和 默认情况下，段寄存器由基址寄存器决定 允许段重设 同样，不允许同时出现两个基址寄存器或两个变址寄存器。 例如：<code>MOV AX, 5[DI][BX]</code>  （段寄存器为DS，偏移地址为BX + DI + 5 的连续两个单元的内容送到AX中）</p>\n</blockquote>\n<h3 id=\"隐含寻址\"><a href=\"#隐含寻址\" class=\"headerlink\" title=\"隐含寻址\"></a>隐含寻址</h3><blockquote>\n<p>操作数隐含在指令码中 例如：<code>MUL</code> （AL x BL ——&gt;AX）</p>\n</blockquote>\n<h2 id=\"指令系统\"><a href=\"#指令系统\" class=\"headerlink\" title=\"指令系统\"></a><strong>指令系统</strong></h2><h3 id=\"指令的概述\"><a href=\"#指令的概述\" class=\"headerlink\" title=\"指令的概述\"></a><strong>指令的概述</strong></h3><h4 id=\"CISC指令系统\"><a href=\"#CISC指令系统\" class=\"headerlink\" title=\"CISC指令系统\"></a>CISC指令系统</h4><blockquote>\n<p>CISC(Complex Instruction Set Computer)，复杂指令系统计算机。<br>CISC指令的设计目标是增强指令的功能，将一些原来用软件是西安的、常用的功能变成用硬件的指令系统来实现。</p>\n</blockquote>\n<ul>\n<li><p><strong>缺点</strong></p>\n<blockquote>\n<p>难以使用</p>\n</blockquote>\n</li>\n<li><p><strong>优点</strong>：</p>\n<blockquote>\n<p>指令经编译后生成的指令程序较小、执行起来较快、节省硬件资源、存取指令的次数少、占用较少的存储器。</p>\n</blockquote>\n</li>\n<li><p><strong>存在的三个方面的问题</strong>：</p>\n<blockquote>\n<ol>\n<li>“8020规律”：20%的指令在各种应用程序中出现频率占整个指令系统的80%。</li>\n<li>CISC指令系统中有大量的复杂指令，控制逻辑极不规整，给VLSI（超大规模集成电路）工艺造成了极大的困难</li>\n<li>CISC增加了许多复杂指令，这些指令虽然简化了目标程序、缩小了高级语言与机器语言之间的差距，但使程序总的执行时间变长、硬件的复杂度增加。</li>\n</ol>\n</blockquote>\n</li>\n</ul>\n<h4 id=\"RISC指令系统\"><a href=\"#RISC指令系统\" class=\"headerlink\" title=\"RISC指令系统\"></a>RISC指令系统</h4><blockquote>\n<p>RISC，精简指令系统计算机。<br>一种计算机体系结构的设计思想，不是产品<br>核心思想是通过简化指令来使计算机的结构更加简单、合理，从而提高CPU的运算速度。</p>\n</blockquote>\n<h5 id=\"RISC的特点：\"><a href=\"#RISC的特点：\" class=\"headerlink\" title=\"RISC的特点：\"></a>RISC的特点：</h5><blockquote>\n<ul>\n<li>大多数指令在一个计算机周期内完成</li>\n<li>指令系统中应尽量减少访问存储器的指令，而采用寄存器与寄存器之间的操作</li>\n<li>减少寻址方式的种类。复杂的寻址方式用简单的寻址方式合成</li>\n<li>减少指令的种类。复杂的指令用软件实现</li>\n<li>指令格式简单</li>\n</ul>\n</blockquote>\n<h3 id=\"8086指令系统\"><a href=\"#8086指令系统\" class=\"headerlink\" title=\"8086指令系统\"></a><strong>8086指令系统</strong></h3><h4 id=\"指令概述\"><a href=\"#指令概述\" class=\"headerlink\" title=\"指令概述\"></a>指令概述</h4><h5 id=\"指令\"><a href=\"#指令\" class=\"headerlink\" title=\"指令\"></a>指令</h5><p>指令的一般格式为  操作码 目标操作数 源操作数</p>\n<ul>\n<li><p><strong>零操作数指令</strong>：</p>\n<blockquote>\n<p>形式上只有操作码，操作数是隐含存在的，操作对象通常为处理器本身</p>\n</blockquote>\n</li>\n<li><p><strong>单操作数指令</strong>：</p>\n<blockquote>\n<p>指令中仅给出一个操作数，另一个操作数隐含存在</p>\n</blockquote>\n</li>\n<li><p><strong>双操作数指令</strong>：</p>\n<blockquote>\n<p>一般格式</p>\n</blockquote>\n</li>\n</ul>\n<h5 id=\"操作数\"><a href=\"#操作数\" class=\"headerlink\" title=\"操作数\"></a>操作数</h5><ul>\n<li><p><em>*立即操作数</em>：</p>\n<blockquote>\n<p>立即数是指具有固定数值的操作数，即常数，不因指令的执行而发生变化。8086系统中字长可以是1字节或者2字节；可以是无符号数或有符号数</p>\n</blockquote>\n</li>\n<li><p><strong>寄存器操作数</strong>：</p>\n<blockquote>\n<p>8086CPU的8个通用寄存器和4个段寄存器可以作为指令中的寄存器操作数，源操作数和目标操作数均可</p>\n</blockquote>\n</li>\n<li><p>存储器操作数：</p>\n<blockquote>\n<p>其含义是参加运算的数据是存放在内存中的。通常为8位或16位字长</p>\n</blockquote>\n</li>\n</ul>\n<h4 id=\"数据传送指令\"><a href=\"#数据传送指令\" class=\"headerlink\" title=\"数据传送指令\"></a>数据传送指令</h4><h5 id=\"通用数据传送指令\"><a href=\"#通用数据传送指令\" class=\"headerlink\" title=\"通用数据传送指令\"></a>通用数据传送指令</h5><ul>\n<li><p><strong>一般数据传送指令</strong></p>\n<blockquote>\n<p>原则：<br>（1）两操作数字长必须相同<br>​（2）两操作数不允许同时为存储器操作数<br>​（3）两操作数不允许同时为段寄存器<br>​（4）在源操作数是立即数时，目标操作数不能是段寄存器<br>​（5）IP和CS不作为目标操作数，FLAGS一般也不作为操作数在指令中出现</p>\n<ul>\n<li><p><code>MOV</code></p>\n<blockquote>\n<ul>\n<li>格式：<code>MOV dest, src</code></li>\n<li>执行过程：<code>dest ——&gt; src</code></li>\n<li>举例：：<code>MOV AL, BL</code></li>\n</ul>\n</blockquote>\n</li>\n</ul>\n</blockquote>\n</li>\n<li><p><strong>堆栈操作指令</strong></p>\n<blockquote>\n<p>原则：<br>（1）先进后出<br>（2）以字为单位<br>（3）指令操作数必须是16位<br>（4）操作数可以是寄存器或存储器两单元，但不能是立即数<br>（5）不能从栈顶弹出一个字给CS<br>（6）PUSH和POP指令在程序中一般成对出现<br>（7）PUSH指令的操作方向是从高地址向低地址，而POP指令的操作正好相反</p>\n<ul>\n<li><p><code>PUSH</code>（压栈指令）：</p>\n<blockquote>\n<p><code>OPRD</code>：16存储器或存储器两单元</p>\n<ul>\n<li><p>格式：<code>PUSH OPRD</code></p>\n</li>\n<li><p>执行过程</p>\n<blockquote>\n<p><code>SP -2 ——&gt;SP</code><br>操作数高字节 ——&gt; <code>SP+1</code><br>操作数低字节 ——&gt;<code>SP</code></p>\n</blockquote>\n</li>\n</ul>\n</blockquote>\n</li>\n</ul>\n</blockquote>\n</li>\n<li><p><strong><code>POP</code>（出栈指令）：</strong></p>\n<blockquote>\n<ul>\n<li><p>格式：<code>POP OPRD</code></p>\n</li>\n<li><p>执行过程：</p>\n<blockquote>\n<p><code>SP</code> ——&gt; 操作数低字节<br><code>SP+ 1</code> ——&gt; 操作数高字节<br><code>SP</code> &lt;—— <code>SP +2</code></p>\n</blockquote>\n</li>\n</ul>\n</blockquote>\n</li>\n<li><p><strong>交换指令</strong></p>\n<blockquote>\n<p>原则：<br>（1）两操作数必须有一个是寄存器操作数<br>（2）不允许使用段寄存器</p>\n<ul>\n<li><p><code>XCHG</code></p>\n<blockquote>\n<ul>\n<li><p>格式：<code>XCHG REG, MEM/REG</code></p>\n</li>\n<li><p>操作：<br>  <code>REG &lt;——&gt; MEM/ REG</code></p>\n</li>\n<li><p>例子：<br>  <code>XCHG AX, BX</code><br>  <code>XCHG [2000],CL</code></p>\n</li>\n</ul>\n</blockquote>\n</li>\n</ul>\n</blockquote>\n</li>\n<li><p><strong>查表转换指令</strong></p>\n<blockquote>\n<p>原则：<br>（1）零操作数指令<br>（2）用BX的内容代表表格首地址，AL内容为表内偏移量，BX+AL得到要查找元素的偏移地址</p>\n<ul>\n<li><p>XLAT</p>\n<blockquote>\n<ul>\n<li><p>格式：XLAT</p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p>将BX+AL所指单元的内容送给AL</p>\n</blockquote>\n</li>\n</ul>\n</blockquote>\n</li>\n</ul>\n</blockquote>\n</li>\n<li><p><strong>字位扩展指令</strong></p>\n<blockquote>\n<p>原则：<br>（1）将符号数的符号位扩展到高位<br>（2）指令为零操作数，采用隐含寻址，隐含的操作数为AX及AX，DX<br>（3）无符号数的扩展规则为在高位补0</p>\n<ul>\n<li><p><code>CBW</code>（字节到字的扩展指令）</p>\n<blockquote>\n<ul>\n<li><p>格式：<code>CBW</code></p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p>将AL内容扩展到AX<br>若最高位 = 1，则执行后 AH =FFH<br>若最高位 = 0，则执行后AH = 00H</p>\n</blockquote>\n</li>\n</ul>\n</blockquote>\n</li>\n<li><p><code>CWD</code>（字到双子的扩展指令）</p>\n<blockquote>\n<ul>\n<li><p>格式：<code>CWD</code></p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p>将 <code>AX</code> 内容扩展到 <code>DX</code> <code>AX</code></p>\n<p>若最高位 = 1，则执行后DX = FFFFH</p>\n<p>若最高位 = 0，则执行后DX = 0000H</p>\n</blockquote>\n</li>\n</ul>\n</blockquote>\n</li>\n</ul>\n</blockquote>\n</li>\n</ul>\n<h5 id=\"输入输出指令\"><a href=\"#输入输出指令\" class=\"headerlink\" title=\"输入输出指令\"></a><strong>输入输出指令</strong></h5><ul>\n<li><p>输入指令</p>\n<ul>\n<li><p>IN</p>\n<ul>\n<li><p>指令：<code>IN acc, PORT</code></p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p>从端口地址读入数据到累加器中</p>\n</blockquote>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>输出指令</p>\n<ul>\n<li><p>OUT</p>\n<ul>\n<li><p>指令：<code>OUT PORT, acc</code></p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p>将累加器的值输出到端口中</p>\n</blockquote>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h5 id=\"地址传送指令\"><a href=\"#地址传送指令\" class=\"headerlink\" title=\"地址传送指令\"></a><strong>地址传送指令</strong></h5><ul>\n<li><p><strong>取近地址指令</strong></p>\n<ul>\n<li><p><code>LEA</code>：</p>\n<ul>\n<li><p>格式：<code>LEA REG, MEM</code>（必须是存储器操作数）</p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p>将变量得16位偏移地址写入到目标寄存器</p>\n</blockquote>\n</li>\n<li><p>要求：</p>\n<blockquote>\n<p>源操作数必须是一个存储器操作数，目标操作数通常是间址</p>\n</blockquote>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong>取远地址指令</strong></p>\n<ul>\n<li><p><code>LDS</code>：</p>\n<ul>\n<li><p>格式：<code>LDS</code>  通用寄存器，存储器操作数</p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p>将源操作数得偏移地址送目标寄存器，将源操作数得地址送DS</p>\n</blockquote>\n</li>\n</ul>\n</li>\n<li><p><code>LES</code>：</p>\n<ul>\n<li><p>格式：<code>LES</code> 通用寄存器，存储器操作数</p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p>将源操作数得偏移地址送目标寄存器，将源操作数得地址送ES</p>\n</blockquote>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h5 id=\"标志传送指令\"><a href=\"#标志传送指令\" class=\"headerlink\" title=\"标志传送指令\"></a><strong>标志传送指令</strong></h5><ul>\n<li><p>隐含操作数AH</p>\n<ul>\n<li><p><code>LAHF</code>(Load AH from Flags)</p>\n<ul>\n<li><p>格式：<code>LAHF</code></p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p>将<code>FLAGS</code>的低8位装入<code>AH</code></p>\n</blockquote>\n</li>\n</ul>\n</li>\n<li><p><code>SAHF</code>(Store AH into Flags)</p>\n<ul>\n<li><p>格式：<code>SHAF</code></p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p>与<code>LAHF</code>相反</p>\n</blockquote>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>隐含操作数FLAGS</p>\n<ul>\n<li>PUSHF(Push flags onto stack)</li>\n<li>POPF(Pop flags off stack)</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"算术运算指令\"><a href=\"#算术运算指令\" class=\"headerlink\" title=\"算术运算指令\"></a><strong>算术运算指令</strong></h4><h5 id=\"加法运算指令\"><a href=\"#加法运算指令\" class=\"headerlink\" title=\"加法运算指令\"></a>加法运算指令</h5><ul>\n<li><p><code>ADD</code>（普通加法指令）</p>\n<p>  <strong>ADD指令的执行对全部6个状态标志位都产生影响</strong></p>\n<ul>\n<li><p>格式：<code>ADD OPRD1, OPRD2</code></p>\n</li>\n<li><p>操作:</p>\n<blockquote>\n<p><code>OPRD1 + OPRD2 ——&gt; OPRD1</code></p>\n</blockquote>\n</li>\n<li><p>例子：</p>\n<blockquote>\n<p><code>ADD AL, 99H</code>（）</p>\n</blockquote>\n</li>\n</ul>\n</li>\n<li><p><code>ADC</code>（带进位的加法指令）</p>\n<p>  <strong>ADC指令都用于多字节数相加，使用前要先将CF清零</strong></p>\n<ul>\n<li><p>格式：<code>ADC OPRD1, OPRD2</code></p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p><code>OPRD1 + OPRD2 + CF ——&gt; OPRD1</code></p>\n</blockquote>\n</li>\n</ul>\n</li>\n<li><p><code>INC</code>（加1指令）</p>\n<p>  <strong>常用于程序中修改地址指针</strong></p>\n<ul>\n<li><p>格式：<code>INC OPRD</code></p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p><code>OPRD + 1 ——&gt; OPRD</code></p>\n</blockquote>\n</li>\n</ul>\n</li>\n</ul>\n<h5 id=\"减法运算指令\"><a href=\"#减法运算指令\" class=\"headerlink\" title=\"减法运算指令\"></a>减法运算指令</h5><ul>\n<li><p><code>SUB</code> （普通减法指令）</p>\n<p>  <strong>对标志位的影响与ADD指令相同</strong></p>\n<ul>\n<li><p>格式：<code>SUB OPRD1, OPRD2</code></p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p><code>OPRD1 - OPRD2 ——&gt; OPRD1</code></p>\n</blockquote>\n</li>\n</ul>\n</li>\n<li><p><code>SBB</code>（考虑借位的减法指令）</p>\n<ul>\n<li><p>格式：<code>SBB OPRD1, OPRD2</code></p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p><code>OPRD1 - OPRD2 - CF——&gt; OPRD1</code></p>\n</blockquote>\n</li>\n</ul>\n</li>\n<li><p><code>DEC</code>（减1指令DEC）</p>\n<p>  <strong>指令对操作数的要求与INC相同，指令常用于在程序中修改计数值</strong></p>\n<ul>\n<li><p>格式：<code>DEC OPRD</code></p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p><code>OPRD -1 ——&gt; OPRD</code></p>\n</blockquote>\n</li>\n<li><p>应用程序例子：</p>\n<p>  <img src=\"/2023/03/06/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/%E8%80%83%E7%A0%94/%E5%BE%AE%E6%9C%BA%E5%8E%9F%E7%90%86%E5%8F%8A%E6%8E%A5%E5%8F%A3%E6%8A%80%E6%9C%AF/image-20230308223135359.png\" alt=\"image-20230308223135359\"></p>\n</li>\n</ul>\n</li>\n<li><p><code>CMP</code>（比较指令）</p>\n<p>  用于比较两个数的大小，可作为条件转移指令转移的条件</p>\n<p>  指令执行的结果不影响目标操作数，仅影响标志位！</p>\n<p>  对操作数的要求及对标志位的影响与SUB指令相同</p>\n<ul>\n<li><p>格式：<code>CMP OPRD1, OPRD2</code></p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p><code>OPRD1 - OPRD2</code></p>\n</blockquote>\n</li>\n<li><p>例子：</p>\n<ul>\n<li><p>两个无符号数的比较</p>\n<blockquote>\n<p><code>CMP AX, BX</code></p>\n<p>若 AX &gt;= BX ——&gt; CF =0</p>\n<p>若AX &lt; BX ——&gt; CF = 1</p>\n<p>若 AX = BX ——&gt; CF =1,ZF =1</p>\n</blockquote>\n</li>\n<li><p>两个带符号数的比较</p>\n<blockquote>\n<p>CMP AX, BX</p>\n<p>两个数的大小由OF和SF共同决定</p>\n<p>OF和SF状态相同 AX &gt;= BX</p>\n<p>OF和SF状态不同 AX &lt; BX</p>\n</blockquote>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><code>NEG</code>（求补指令）</p>\n<p>  <strong>对一个负数取补码就相当于用零减去此数</strong></p>\n<ul>\n<li><p>格式：<code>NEG OPRD</code></p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p><code>0 - OPRD ——&gt; OPRD</code></p>\n</blockquote>\n</li>\n<li><p>说明：</p>\n<ul>\n<li>执行NEG指令后，一般情况下都会使CF为1，除非给定的操作数为零才会使CF为0；</li>\n<li>当指定的操作数的值为80H(-128)或为8000H(-32768)，则执行NEG指令后，结果不变，但OF置1，其它情况下OF置0。</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h5 id=\"乘法指令\"><a href=\"#乘法指令\" class=\"headerlink\" title=\"乘法指令\"></a>乘法指令</h5><p>乘法指令采用隐含寻址，隐含的是存放被乘数的累加器AL或AX及存放结果的AX，DX</p>\n<ul>\n<li><p><strong><code>MUL</code>无符号的乘法指令</strong></p>\n<ul>\n<li><p>格式：<code>MUL OPRD</code>（OPRD不能是立即数）</p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p>OPRD为8字节数 ===&gt; AL x OPRD ——&gt; AX</p>\n<p>OPRD为16位数 ====&gt; AX x OPRD ——&gt; DXAX </p>\n</blockquote>\n</li>\n<li><p>例子：</p>\n<blockquote>\n<p><img src=\"/2023/03/06/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/%E8%80%83%E7%A0%94/%E5%BE%AE%E6%9C%BA%E5%8E%9F%E7%90%86%E5%8F%8A%E6%8E%A5%E5%8F%A3%E6%8A%80%E6%9C%AF/image-20230308232215011-16782889368582.png\" alt=\"image-20230308232215011\"></p>\n</blockquote>\n</li>\n</ul>\n</li>\n<li><p><strong><code>IMUL</code>带符号的乘法指令</strong></p>\n<ul>\n<li><p>格式：<code>IMUL OPRD</code>（OPRD不能是立即数，隐含操作数为AL，存放在AX中）</p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<ol>\n<li>将两个操作数取补码（对负数按位取反加1，正数不变）</li>\n<li>做乘法运算</li>\n<li>将乘积按位取反加 1</li>\n</ol>\n</blockquote>\n</li>\n</ul>\n</li>\n</ul>\n<h5 id=\"除法指令\"><a href=\"#除法指令\" class=\"headerlink\" title=\"除法指令\"></a>除法指令</h5><p>指令要求被除数是除数的双倍字长</p>\n<ul>\n<li><p><strong>无符号除法</strong></p>\n<ul>\n<li><p>格式：<code>DIV OPRD</code></p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p>若<code>OPRD</code>是字节数：</p>\n<ul>\n<li>执行：<code>AX/OPRD</code></li>\n<li>结果：AL = 商   AH = 余数</li>\n</ul>\n<p>若<code>OPRD</code>是双字节数：</p>\n<ul>\n<li>执行：<code>DXAX/OPRD</code></li>\n<li>结果：AX = 商 DX = 余数</li>\n</ul>\n</blockquote>\n</li>\n</ul>\n</li>\n<li><p><strong>有符号除法</strong></p>\n<ul>\n<li><p>格式：<code>IDIV OPRD</code></p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p>若OPRD是字节数：</p>\n<ul>\n<li>执行：<code>AX/OPRD</code></li>\n<li>结果：AL = 商   AH = 余数</li>\n</ul>\n<p>若OPRD是双字节数：</p>\n<ul>\n<li>执行：<code>DXAX/OPRD</code></li>\n<li>结果：AX = 商 DX = 余数</li>\n</ul>\n</blockquote>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"逻辑运算和移位指令\"><a href=\"#逻辑运算和移位指令\" class=\"headerlink\" title=\"逻辑运算和移位指令\"></a><strong>逻辑运算和移位指令</strong></h4><h5 id=\"逻辑运算指令\"><a href=\"#逻辑运算指令\" class=\"headerlink\" title=\"逻辑运算指令\"></a>逻辑运算指令</h5><blockquote>\n<ul>\n<li><strong>对操作数的要求</strong>：<ul>\n<li>大多数与<code>MOV</code>指令相同</li>\n<li>“非” 运算指令要求操作数不能是立即数</li>\n</ul>\n</li>\n<li><strong>对标志位的影响</strong><ul>\n<li>除“非”运算指令，其余指令的执行都会影响除AF外的五个状态标志</li>\n<li>无论执行结果如何，都会使标志位<code>OF=CF=0</code></li>\n<li>“非”运算指令的执行不影响标志位</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<ul>\n<li><p><strong>“与”指令</strong></p>\n<ul>\n<li><p>格式：<code>AND OPRD1，OPRD2</code></p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p>两操作数相“与”，结果送目标地址。</p>\n</blockquote>\n</li>\n<li><p>应用：</p>\n<ul>\n<li>实现两操作数按位相与的运算：<code>AND BL, [SI]</code></li>\n<li>使目标操作数的某些位不变，某些位清零：<code>AND AL, 0FH</code></li>\n<li>在操作数不变的情况向使CF和OF清零：<code>AND AX, AX</code></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>“或”运算指令</p>\n<ul>\n<li>格式</li>\n<li>操作</li>\n<li>应用</li>\n</ul>\n</li>\n<li><p>“非”运算指令</p>\n<ul>\n<li><p>格式：NOT OPRD</p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p>操作数按位取反在送回原地址，指令的执行对标志位无影响</p>\n</blockquote>\n</li>\n<li><p>例子：</p>\n<blockquote>\n</blockquote>\n</li>\n</ul>\n</li>\n<li><p>“异或”运算指令</p>\n<ul>\n<li><p>格式：<code>XOR OPRD1, OPRD2</code></p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p>两操作数“异或”，结果送目标地址</p>\n</blockquote>\n</li>\n<li><p>例子：</p>\n</li>\n</ul>\n</li>\n<li><p>“测试”指令</p>\n<ul>\n<li><p>格式：<code>TEST OPRD1, OPRD2</code></p>\n</li>\n<li><p>操作：</p>\n<blockquote>\n<p>执行“与”运算，但运算的结果不送回目标地址</p>\n</blockquote>\n</li>\n<li><p>应用：</p>\n<blockquote>\n<p>常用于测试某些位的状态</p>\n</blockquote>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"串操作指令\"><a href=\"#串操作指令\" class=\"headerlink\" title=\"串操作指令\"></a>串操作指令</h4><h4 id=\"程序控制指令\"><a href=\"#程序控制指令\" class=\"headerlink\" title=\"程序控制指令\"></a>程序控制指令</h4><h4 id=\"处理器控制指令\"><a href=\"#处理器控制指令\" class=\"headerlink\" title=\"处理器控制指令\"></a>处理器控制指令</h4><h2 id=\"汇编语言程序设计\"><a href=\"#汇编语言程序设计\" class=\"headerlink\" title=\"汇编语言程序设计\"></a><strong>汇编语言程序设计</strong></h2><h3 id=\"伪指令\"><a href=\"#伪指令\" class=\"headerlink\" title=\"伪指令\"></a><strong>伪指令</strong></h3><h3 id=\"BIOS和DOS功能调用\"><a href=\"#BIOS和DOS功能调用\" class=\"headerlink\" title=\"BIOS和DOS功能调用\"></a><strong>BIOS和DOS功能调用</strong></h3><h3 id=\"汇编语言程序设计基础\"><a href=\"#汇编语言程序设计基础\" class=\"headerlink\" title=\"汇编语言程序设计基础\"></a><strong>汇编语言程序设计基础</strong></h3><blockquote>\n<h4 id=\"程序设计概述\"><a href=\"#程序设计概述\" class=\"headerlink\" title=\"程序设计概述\"></a><strong>程序设计概述</strong></h4><h4 id=\"顺序结构\"><a href=\"#顺序结构\" class=\"headerlink\" title=\"顺序结构\"></a><strong>顺序结构</strong></h4><h4 id=\"分支结构\"><a href=\"#分支结构\" class=\"headerlink\" title=\"分支结构\"></a><strong>分支结构</strong></h4><h4 id=\"循环结构\"><a href=\"#循环结构\" class=\"headerlink\" title=\"循环结构\"></a><strong>循环结构</strong></h4><h4 id=\"子程序\"><a href=\"#子程序\" class=\"headerlink\" title=\"子程序\"></a><strong>子程序</strong></h4><h4 id=\"常用程序设计举例\"><a href=\"#常用程序设计举例\" class=\"headerlink\" title=\"常用程序设计举例\"></a><strong>常用程序设计举例</strong></h4></blockquote>\n<h2 id=\"存储器系统\"><a href=\"#存储器系统\" class=\"headerlink\" title=\"存储器系统\"></a><strong>存储器系统</strong></h2><h3 id=\"随机存取存储器\"><a href=\"#随机存取存储器\" class=\"headerlink\" title=\"随机存取存储器\"></a><strong>随机存取存储器</strong></h3><h3 id=\"只读存储器\"><a href=\"#只读存储器\" class=\"headerlink\" title=\"只读存储器\"></a><strong>只读存储器</strong></h3><h3 id=\"高速缓冲存储器\"><a href=\"#高速缓冲存储器\" class=\"headerlink\" title=\"高速缓冲存储器\"></a><strong>高速缓冲存储器</strong></h3><h3 id=\"存储器扩展技术\"><a href=\"#存储器扩展技术\" class=\"headerlink\" title=\"存储器扩展技术\"></a><strong>存储器扩展技术</strong></h3>"},{"title":"D2L_ch02_预备知识","date":"2023-03-22T08:17:58.000Z","mathjax":true,"_content":"\n\n## 数据操作\n\n\n\n首先，我们导入`torch`。请注意，虽然它被称为`PyTorch`，但是代码中使用`torch`而不是`pytorch`\n\n\n```python\nimport torch \n```\n\n### **张量**：\n\n> 表示一个由数值组成的数组，这个数组可能有多个维度\n\n\n```python\nx = torch.arange(12)\nx\n\n# 结果为：tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n```\n\n### **`shape`属性：**\n\n> 访问张量（沿每个轴的长度）的*形状*\n\n\n```python\nx.shape  # 获取张量的形状\n\n# 结果为： torch.Size([12])\n```\n\n### **`numel`函数：**\n\n> 获取张量中元素的总数\n\n\n```python\nx.numel() # 获取张量中元素的总数\n\n# 结果为： 12\n```\n\n### **`reshape`函数：**\n\n> 改变一个张量的形状而不改变元素数量和元素值\n\n\n````python\nX = x.reshape(3, 4) # 不改变元素数量和元素值的前提下，改变张量的形状。\nX\n\n\"\"\"\n结果为：\ntensor([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]])\n\"\"\"\n````\n\n### **`zeros((a,b,c...))`函数：**\n\n> 构造全0张量\n\n\n````python\ntorch.zeros((2, 3, 4))\n\n'''\n结果为：\ntensor([[[0., 0., 0., 0.],\n         [0., 0., 0., 0.],\n         [0., 0., 0., 0.]],\n\n        [[0., 0., 0., 0.],\n         [0., 0., 0., 0.],\n         [0., 0., 0., 0.]]])\n'''\n````\n\n### `ones((a,b,c...))`函数：\n\n> 构造全1张量\n\n\n````python\ntorch.ones((2, 3, 4)) \n\n'''\n结果为：\ntensor([[[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]],\n\n        [[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]]])\n'''\n\n````\n\n### **`randn(a,b,c...)`函数**：\n\n> 从标准高斯（正态）分布中随机采样\n\n\n````python\ntorch.randn(3, 4) \n\n\n'''\n结果为：\ntensor([[-0.8709,  0.4194, -0.4292,  0.1850],\n        [-0.3330, -0.6349, -0.1422, -1.0355],\n        [-0.7531,  0.5163,  2.4913,  0.3060]])\n'''\n````\n\n通过提供包含数值的Python列表（或嵌套列表），来为所需张量中的每个元素赋予确定值\n\n\n````python\ntorch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n\n'''\n结果为：\ntensor([[2, 1, 4, 3],\n        [1, 2, 3, 4],\n        [4, 3, 2, 1]])\n'''\n````\n\n### **标准算术运算符`（+、-、*、/和**）`**：\n\n> 都可以被升级为按元素运算\n\n\n````python\nx = torch.tensor([1.0, 2, 4, 8])\ny = torch.tensor([2, 2, 2, 2])\nx + y, x - y, x * y, x / y, x ** y\n\n\n'''\n结果为：\n(tensor([ 3.,  4.,  6., 10.]),\n tensor([-1.,  0.,  2.,  6.]),\n tensor([ 2.,  4.,  8., 16.]),\n tensor([0.5000, 1.0000, 2.0000, 4.0000]),\n tensor([ 1.,  4., 16., 64.]))\n '''\n\n````\n\n### **求幂运算**\n\n\n````python\ntorch.exp(x)\n\n'''\n结果为：\ntensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])\n'''\n````\n\n### 张量连结（concatenate）：\n\n> 我们也可以把多个张量*连结*（concatenate）在一起\n\n\n````python\nX = torch.arange(12, dtype=torch.float32).reshape((3,4))\n# 定义一个12个元素的以为数组X，元素类型为浮点型，改变其形状为：（3,4）\n\nY = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n# 定义一个张量Y,并给元素赋值\n\ntorch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)\n# 将X和Y在0轴上连接合并并输出（0轴为高度方向）； 将X和Y在1轴上合并并输出（1轴为宽度方向）\n\n\n'''\n结果为：\n(tensor([[ 0.,  1.,  2.,  3.],\n         [ 4.,  5.,  6.,  7.],\n         [ 8.,  9., 10., 11.],\n         [ 2.,  1.,  4.,  3.],\n         [ 1.,  2.,  3.,  4.],\n         [ 4.,  3.,  2.,  1.]]),\n tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))\n'''\n````\n\n### 逻辑运算符\n\n> 使用***逻辑运算符***构建二元张量，`True` or `False`\n\n\n````python\nX > Y\n\n\n'''\n结果为：\ntensor([[False, False, False, False],\n        [ True,  True,  True,  True],\n        [ True,  True,  True,  True]])\n'''\n````\n\n对张量中的所有元素进行求和，会产生一个单元素张量\n\n\n```python\nX.sum()\n\n# 结果为：tensor(66.)\n```\n\n\n\n### 广播机制：\n\n> 即使形状不同，我们仍然可以通过调用***广播机制*（broadcasting mechanism）**来执行按元素操作。通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状。\n\n\n````python\na = torch.arange(9).reshape((3, 1,-1))   # \"-1\"表示该轴会自动计算长度\n# 定义一个包含（0-8）的张量a，形状为（3,1,3）,由最外层往内数\n\nb = torch.arange(3).reshape((1, 3,-1))\n# 定义一个包含（0-2）的张量b，形状为（1,3,1）\n\na, b\n# 输出张量a，输出张量b\n\n\n'''\n结果为：\n(tensor([[[0, 1, 2]],\n \n         [[3, 4, 5]],\n \n         [[6, 7, 8]]]),\n tensor([[[0],\n          [1],\n          [2]]]))\n   '''\n````\n\n不同形状的张量相加：\n\n\n````python\na + b\n# 由于a的形状为(3,1,3)\n\n'''\n结果为：\ntensor([[[ 0,  1,  2],\n         [ 1,  2,  3],\n         [ 2,  3,  4]],\n\n        [[ 3,  4,  5],\n         [ 4,  5,  6],\n         [ 5,  6,  7]],\n\n        [[ 6,  7,  8],\n         [ 7,  8,  9],\n         [ 8,  9, 10]]])\n'''\n````\n\n### **切片**：\n\n> 可以用`[-1]`选择最后一个元素，可以用`[1:3]`选择第二个和第三个元素\n\n\n````python\nX[-1], X[1:3]\n# 输出X中的最后一行元素，输出X中的第二行和第三行元素\n\n'''\n结果为：\n(tensor([ 8.,  9., 10., 11.]),\n tensor([[ 4.,  5.,  6.,  7.],\n         [ 8.,  9., 10., 11.]]))\n'''\n\n````\n\n### **通过索引修改元素的值**\n\n> 除读取外，我们还可以通过指定索引来将元素写入矩阵\n\n\n````python\nX[1, 2] = 9          # 将X中第二行第三列的元素值改为9\n\nX                    # 输出X\n\n'''\n结果为：\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  9.,  7.],\n        [ 8.,  9., 10., 11.]])\n'''\n\n````\n\n### **通过切片赋值**：\n\n> 为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值\n\n\n````python\nX[0:2, :] = 12 \n# 将12赋值给X中的第一行和第二行的每一个元素\n\nX  # 输出X\n\n'''\n结果为：\ntensor([[12., 12., 12., 12.],\n        [12., 12., 12., 12.],\n        [ 8.,  9., 10., 11.]])\n'''\n````\n\n### **内存分配问题：**\n\n运行一些操作可能会导致为新结果分配内存\n\n\n```python\nbefore = id(Y)\n# 将Y的id赋值给before变量\n\nY = Y + X\n# 执行矩阵相加\n\nid(Y) == before\n# 将新的Y的id和之前的Y的id进行逻辑比较，并输出逻辑运算结果\n\n# 结果为：False\n\n```\n\n执行原地操作\n\n\n````python\nZ = torch.zeros_like(Y)\n# 生成一个新的全0矩阵：Z，使得Z的形状与Y一样\n\nprint('id(Z):', id(Z))\n# 打印Z的id\n\nZ[:] = X + Y\n# 执行原地操作，Z的id不变\n\nprint('id(Z):', id(Z))\n# 打印Z的id\n\n\n'''\n结果为：\nid(Z): 139931132035296\nid(Z): 139931132035296\n'''\n````\n\n如果在后续计算中没有重复使用`X`，我们也可以使用`X[:] = X + Y`或`X += Y`来减少操作的内存开销\n\n\n````python\nbefore = id(X)\nX += Y\nid(X) == before\n\n'''\n结果为：\nTrue\n'''\n````\n\n### 转换为`NumPy`张量（`ndarray`）\n\n\n````python\nA = X.numpy()\n# 将X转换成Numpy数组，并赋值给A\n\nB = torch.tensor(A)\n# 将A转变成张量，并赋值给B\n\ntype(A), type(B)\n# 输出A的数据类型，输出B的数据类型\n\n\n'''\n结果为：\n(numpy.ndarray, torch.Tensor)\n'''\n````\n\n将大小为1的张量转换为Python标量\n\n\n````python\na = torch.tensor([3.5])\n# 定义一个大小为1，元素为3.5的张量a\na, a.item(), float(a), int(a)\n输出a，标量a，浮点数a，整数a的值\n\n\n'''\n结果为：\n(tensor([3.5000]), 3.5, 3.5, 3)\n'''\n````\n\n## 数据预处理\n\n### CSV（逗号分隔值）文件操作\n\n> 创建一个人工数据集，并存储在CSV（逗号分隔值）文件\n\n\n```python\nimport os\n\nos.makedirs(os.path.join('..', 'data'), exist_ok=True)\ndata_file = os.path.join('..', 'data', 'house_tiny.csv')\nwith open(data_file, 'w') as f:\n    f.write('NumRooms,Alley,Price\\n')\n    f.write('NA,Pave,127500\\n')\n    f.write('2,NA,106000\\n')\n    f.write('4,NA,178100\\n')\n    f.write('NA,NA,140000\\n')\n```\n\n> 从创建的CSV文件中加载原始数据集\n\n\n````python\nimport pandas as pd\n\ndata = pd.read_csv(data_file)\nprint(data)\n\n'''\n结果为：\n     NumRooms Alley   Price\n      NaN    Pave     127500\n      2.0     NaN     106000\n      4.0     NaN     178100\n      NaN     NaN     140000\n'''\n\n````\n\n> 为了处理缺失的数据，典型的方法包括***插值法***和***删除法***，这里，我们将考虑**插值法**\n\n\n````python\ninputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]\n# iloc是csv的索引方法\ninputs = inputs.fillna(inputs.mean())\n# 用同一列的均值替换NaN\nprint(inputs)\n\n'''\n结果为：\n     NumRooms Alley\n       3.0    Pave\n       2.0    NaN\n       4.0    NaN\n       3.0    NaN\n'''\n````\n\n> 对于`inputs`中的类别值或离散值，我们将`“NaN”`视为一个类别\n\n\n````python\ninputs = pd.get_dummies(inputs, dummy_na=True)\n#转换成pandas表格？？？\nprint(inputs)\n\n'''\n结果为：\n       NumRooms  Alley_Pave  Alley_nan\n       3.0           1          0\n       2.0           0          1\n       4.0           0          1\n       3.0           0          1\n'''\n````\n\n> 现在`inputs`和`outputs`中的所有条目都是数值类型，它们可以转换为张量格式\n\n\n````python\nimport torch\n\nX, y = torch.tensor(inputs.values), torch.tensor(outputs.values)\n# 将表格数据拆分，均转换为张量\nX, y\n\n结果为：\n'''\n(tensor([[3., 1., 0.],\n         [2., 0., 1.],\n         [4., 0., 1.],\n         [3., 0., 1.]], dtype=torch.float64),\n tensor([127500, 106000, 178100, 140000]))\n'''\n````\n\n## 线性代数\n\n### 标量与张量\n\n> 标量由只有一个元素的张量表示\n\n\n```python\nimport torch\n\nx = torch.tensor(3.0)\ny = torch.tensor(2.0)\n\nx + y, x * y, x / y, x**y\n\n# (tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))\n```\n\n> 向量可以被视为标量值组成的列表\n\n\n```python\nx = torch.arange(4)\nx\n\n# tensor([0, 1, 2, 3])\n```\n\n> 通过张量的索引来访问任一元素\n\n\n```python\nx[3]\n\n# tensor(3)\n```\n\n> 访问张量的长度\n\n\n```python\nlen(x)\n# 4\n```\n\n> 只有一个轴的张量，形状只有一个元素\n\n\n```python\nx.shape\n\n# torch.Size([4])\n```\n\n### 矩阵\n\n> 通过指定两个分量$m$和$n$来创建一个形状为$m \\times n$的**矩阵**\n\n\n```python\nA = torch.arange(20).reshape(5, 4)\nA\n\n'''\n结果为：\ntensor([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11],\n        [12, 13, 14, 15],\n        [16, 17, 18, 19]])\n \n'''\n```\n\n> 矩阵的转置\n\n\n```python\nA.T\n\n\n'''\n结果为：\ntensor([[ 0,  4,  8, 12, 16],\n        [ 1,  5,  9, 13, 17],\n        [ 2,  6, 10, 14, 18],\n        [ 3,  7, 11, 15, 19]])\n'''\n\n```\n\n> *对称矩阵*（symmetric matrix）$\\mathbf{A}$等于其转置：$\\mathbf{A} = \\mathbf{A}^\\top$\n\n\n```python\nB = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\nB\n\n'''\n结果为：\ntensor([[1, 2, 3],\n        [2, 0, 4],\n        [3, 4, 5]])\n'''\n\n```\n\n\n```python\nB == B.T\n\n'''\n结果为：\ntensor([[True, True, True],\n        [True, True, True],\n        [True, True, True]])\n'''\n```\n\n> 就像向量是标量的推广，矩阵是向量的推广一样，我们可以构建具有更多轴的数据结构\n\n\n```python\nX = torch.arange(24).reshape(2, 3, 4)\nX\n\n'''\n结果为：\ntensor([[[ 0,  1,  2,  3],\n         [ 4,  5,  6,  7],\n         [ 8,  9, 10, 11]],\n\n        [[12, 13, 14, 15],\n         [16, 17, 18, 19],\n         [20, 21, 22, 23]]])\n'''\n\n```\n\n> 给定具有**相同形状**的任意两个张量，任何按元素**二元运算**的结果都将是相同形状的张量\n\n\n```python\nA = torch.arange(20, dtype=torch.float32).reshape(5, 4)\nB = A.clone()\nA, A + B\n\n'''\n结果为：\n(tensor([[ 0.,  1.,  2.,  3.],\n         [ 4.,  5.,  6.,  7.],\n         [ 8.,  9., 10., 11.],\n         [12., 13., 14., 15.],\n         [16., 17., 18., 19.]]),\n tensor([[ 0.,  2.,  4.,  6.],\n         [ 8., 10., 12., 14.],\n         [16., 18., 20., 22.],\n         [24., 26., 28., 30.],\n         [32., 34., 36., 38.]]))\n'''\n```\n\n> 两个矩阵的按元素乘法称为*Hadamard积*（Hadamard product）（数学符号$\\odot$）\n\n\n```python\nA * B\n\n'''\n结果为：\ntensor([[  0.,   1.,   4.,   9.],\n        [ 16.,  25.,  36.,  49.],\n        [ 64.,  81., 100., 121.],\n        [144., 169., 196., 225.],\n        [256., 289., 324., 361.]])\n'''\n```\n\n\n```python\na = 2\nX = torch.arange(24).reshape(2, 3, 4)\na + X, (a * X).shape\n\n'''\n结果为：\n(tensor([[[ 2,  3,  4,  5],\n          [ 6,  7,  8,  9],\n          [10, 11, 12, 13]],\n \n         [[14, 15, 16, 17],\n          [18, 19, 20, 21],\n          [22, 23, 24, 25]]]),\n torch.Size([2, 3, 4]))\n'''\n```\n\n> 计算其元素的和\n\n\n```python\nx = torch.arange(4, dtype=torch.float32)\nx, x.sum()\n\n# (tensor([0., 1., 2., 3.]), tensor(6.))\n```\n\n> 表示任意形状张量的元素和\n\n\n```python\nA.shape, A.sum()\n\n# (torch.Size([5, 4]), tensor(190.))\n```\n\n### 降维\n\n> 指定张量沿哪一个轴来通过求和降低维度\n\n\n```python\nA_sum_axis0 = A.sum(axis=0)\n# 0轴上压缩，即压缩高度\nA_sum_axis0, A_sum_axis0.shape\n\n# (tensor([40., 45., 50., 55.]), torch.Size([4]))\n```\n\n\n```python\nA_sum_axis1 = A.sum(axis=1)\n# 1轴上压缩，即压缩宽度\nA_sum_axis1, A_sum_axis1.shape\n\n\n# (tensor([ 6., 22., 38., 54., 70.]), torch.Size([5]))\n```\n\n\n```python\nA.sum(axis=[0, 1])\n# 同时在0轴和1轴上压缩，压缩高宽\n\n# tensor(190.)\n```\n\n> 一个与求和相关的量是*平均值*（mean或average）\n\n\n```python\nA.mean(), A.sum() / A.numel()\n\n# (tensor(9.5000), tensor(9.5000))\n```\n\n\n```python\nA.mean(axis=0), A.sum(axis=0) / A.shape[0]\n\n\n# (tensor([ 8.,  9., 10., 11.]), tensor([ 8.,  9., 10., 11.]))\n```\n\n### 非降维求和\n\n> 计算总和或均值时**保持轴数不变**\n\n\n```python\nsum_A = A.sum(axis=1, keepdims=True)\nsum_A\n\n'''\n结果为：\ntensor([[ 6.],\n        [22.],\n        [38.],\n        [54.],\n        [70.]])\n'''\n```\n\n> 通过广播将`A`除以`sum_A`\n\n\n```python\nA / sum_A\n\n'''\n结果为：\ntensor([[0.0000, 0.1667, 0.3333, 0.5000],\n        [0.1818, 0.2273, 0.2727, 0.3182],\n        [0.2105, 0.2368, 0.2632, 0.2895],\n        [0.2222, 0.2407, 0.2593, 0.2778],\n        [0.2286, 0.2429, 0.2571, 0.2714]])\n'''\n```\n\n> 某个轴计算`A`元素的累积总和\n\n\n```python\nA.cumsum(axis=0)\n\n'''\n结果为：\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  6.,  8., 10.],\n        [12., 15., 18., 21.],\n        [24., 28., 32., 36.],\n        [40., 45., 50., 55.]])\n'''\n```\n\n### 点积\n\n> 点积是**相同位置的按元素乘积的和**\n\n\n```python\ny = torch.ones(4, dtype = torch.float32)\nx, y, torch.dot(x, y)\n\n# (tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))\n```\n\n我们可以通过执行按元素乘法，然后进行求和来表示两个向量的点积\n\n\n```python\ntorch.sum(x * y)\n\n# tensor(6.)\n```\n\n### 向量积\n\n> 矩阵向量积$\\mathbf{A}\\mathbf{x}$是一个长度为$m$的列向量，其第$i$个元素是点积$\\mathbf{a}^\\top_i \\mathbf{x}$\n\n\n```python\nA.shape, x.shape, torch.mv(A, x)\n\n# (torch.Size([5, 4]), torch.Size([4]), tensor([ 14.,  38.,  62.,  86., 110.]))\n```\n\n> 我们可以将矩阵-矩阵乘法$\\mathbf{AB}$看作简单地**执行$m$次矩阵-向量积**，并将结果拼接在一起，形成一个$n \\times m$矩阵\n\n\n```python\nB = torch.ones(4, 3)\ntorch.mm(A, B)\n\n'''\ntensor([[ 6.,  6.,  6.],\n        [22., 22., 22.],\n        [38., 38., 38.],\n        [54., 54., 54.],\n        [70., 70., 70.]])\n'''\n\n```\n\n### 范数\n\n> $L_2$*范数*是向量**元素平方和的平方根**：\n>\n>  $$\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}$$\n\n\n```python\nu = torch.tensor([3.0, -4.0])\ntorch.norm(u)\n\n# tensor(5.)\n```\n\n> $L_1$范数，它表示为**向量元素的绝对值之和**：\n>\n>   $$\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^n \\left|x_i \\right|$$\n\n\n```python\ntorch.abs(u).sum()\n\n# tensor(7.)\n```\n\n> 矩阵的*Frobenius（弗罗贝尼乌斯）范数*（Frobenius norm）是**矩阵元素平方和的平方根**：$$\\|\\mathbf{X}\\|_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n x_{ij}^2}$$\n\n\n```python\ntorch.norm(torch.ones((4, 9))) # 36开方\n\n# tensor(6.)\n```\n\n## 微积分\n\n> 如果$f$的*导数*存在，这个极限被定义为$$f'(x) = \\lim_{h \\rightarrow 0} \\frac{f(x+h) - f(x)}{h}$$，定义$u=f(x)=3x^2-4x$\n\n\n```python\n%matplotlib inline\nimport numpy as np\nfrom matplotlib_inline import backend_inline\nfrom d2l import torch as d2l\n\n\ndef f(x):\n    return 3 * x ** 2 - 4 * x\n```\n\n> 通过令$x=1$并让$h$接近$0$，$\\frac{f(x+h)-f(x)}{h}$的数值结果接近$2$\n\n\n```python\ndef numerical_lim(f, x, h):\n    return (f(x + h) - f(x)) / h\n\nh = 0.1\nfor i in range(5):\n    print(f'h={h:.5f}, numerical limit={numerical_lim(f, 1, h):.5f}')\n    h *= 0.1\n  \n'''\n结果为：\nh=0.10000, numerical limit=2.30000\nh=0.01000, numerical limit=2.03000\nh=0.00100, numerical limit=2.00300\nh=0.00010, numerical limit=2.00030\nh=0.00001, numerical limit=2.00003\n'''\n```\n\n> 为了对导数的这种解释进行可视化，我们将使用`matplotlib`定义几个函数\n\n\n```python\ndef use_svg_display():  \n    \"\"\"使用svg格式在Jupyter中显示绘图\"\"\"\n    backend_inline.set_matplotlib_formats('svg')\n\ndef set_figsize(figsize=(3.5, 2.5)):  \n    \"\"\"设置matplotlib的图表大小\"\"\"\n    use_svg_display()\n    d2l.plt.rcParams['figure.figsize'] = figsize\n\ndef set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n    \"\"\"设置matplotlib的轴\"\"\"\n    axes.set_xlabel(xlabel)\n    axes.set_ylabel(ylabel)\n    axes.set_xscale(xscale)\n    axes.set_yscale(yscale)\n    axes.set_xlim(xlim)\n    axes.set_ylim(ylim)\n    if legend:\n        axes.legend(legend)\n    axes.grid()\n\ndef plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None,\n         ylim=None, xscale='linear', yscale='linear',\n         fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):\n    \"\"\"绘制数据点\"\"\"\n    if legend is None:\n        legend = []\n\n    set_figsize(figsize)\n    axes = axes if axes else d2l.plt.gca()\n\n    def has_one_axis(X):\n        return (hasattr(X, \"ndim\") and X.ndim == 1 or isinstance(X, list)\n                and not hasattr(X[0], \"__len__\"))\n\n    if has_one_axis(X):\n        X = [X]\n    if Y is None:\n        X, Y = [[]] * len(X), X\n    elif has_one_axis(Y):\n        Y = [Y]\n    if len(X) != len(Y):\n        X = X * len(Y)\n    axes.cla()\n    for x, y, fmt in zip(X, Y, fmts):\n        if len(x):\n            axes.plot(x, y, fmt)\n        else:\n            axes.plot(y, fmt)\n    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n```\n\n> 绘制函数$u=f(x)$及其在$x=1$处的切线$y=2x-3$\n\n\n```python\nx = np.arange(0, 3, 0.1)\nplot(x, [f(x), 2 * x - 3], 'x', 'f(x)', legend=['f(x)', 'Tangent line (x=1)'])\n```\n\n\n![svg](D2L_ch02_预备知识/output_7_0-16794727634302.svg)\n\n## 自动微分\n\n> 假设我们想对函数$y=2\\mathbf{x}^{\\top}\\mathbf{x}$关于列向量$\\mathbf{x}$求导\n\n\n```python\nimport torch\n\nx = torch.arange(4.0)\nx\n\n'''tensor([0., 1., 2., 3.])'''\n```\n\n> 在我们计算$y$关于$\\mathbf{x}$的梯度之前，需要一个地方来存储梯度\n\n\n```python\nx.requires_grad_(True)\nx.grad\n```\n\n> 现在计算$y$\n\n\n```python\ny = 2 * torch.dot(x, x)\ny\n\n'''tensor(28., grad_fn=<MulBackward0>)'''\n```\n\n> 通过调用反向传播函数来自动计算`y`关于`x`每个分量的梯度\n\n\n```python\ny.backward()\nx.grad\n\n'''tensor([ 0.,  4.,  8., 12.])'''\n```\n\n\n\n\n```python\nx.grad == 4 * x\n\n'''tensor([True, True, True, True])'''\n```\n\n> 现在计算`x`的另一个函数\n\n\n```python\nx.grad.zero_()\ny = x.sum()\ny.backward()\nx.grad\n\n'''tensor([1., 1., 1., 1.])'''\n```\n\n> 深度学习中，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和\n\n\n```python\nx.grad.zero_()\ny = x * x\ny.sum().backward()\nx.grad\n\n'''tensor([0., 2., 4., 6.])'''\n```\n\n> 将某些计算移动到记录的计算图之外\n\n\n```python\nx.grad.zero_()\ny = x * x\nu = y.detach()\nz = u * x\n\nz.sum().backward()\nx.grad == u\n\n'''tensor([True, True, True, True])'''\n```\n\n\n```python\nx.grad.zero_()\ny.sum().backward()\nx.grad == 2 * x\n\n'''tensor([True, True, True, True])'''\n```\n\n> 即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度\n\n\n```python\ndef f(a):\n    b = a * 2\n    while b.norm() < 1000:\n        b = b * 2\n    if b.sum() > 0:\n        c = b\n    else:\n        c = 100 * b\n    return c\n\na = torch.randn(size=(), requires_grad=True)\nd = f(a)\nd.backward()\n\na.grad == d / a\n\n\n'''tensor(True)'''\n```\n\n\n\n","source":"_posts/学习记录/DataWhale/D2L_ch02_预备知识.md","raw":"---\ntitle: D2L_ch02_预备知识\ndate: 2023-03-22 16:17:58\nmathjax: true\ntags:\n  - DataWhale\n  - Deep Learning\ncategories:\n  - 学习记录\n  - DataWhale\n---\n\n\n## 数据操作\n\n\n\n首先，我们导入`torch`。请注意，虽然它被称为`PyTorch`，但是代码中使用`torch`而不是`pytorch`\n\n\n```python\nimport torch \n```\n\n### **张量**：\n\n> 表示一个由数值组成的数组，这个数组可能有多个维度\n\n\n```python\nx = torch.arange(12)\nx\n\n# 结果为：tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n```\n\n### **`shape`属性：**\n\n> 访问张量（沿每个轴的长度）的*形状*\n\n\n```python\nx.shape  # 获取张量的形状\n\n# 结果为： torch.Size([12])\n```\n\n### **`numel`函数：**\n\n> 获取张量中元素的总数\n\n\n```python\nx.numel() # 获取张量中元素的总数\n\n# 结果为： 12\n```\n\n### **`reshape`函数：**\n\n> 改变一个张量的形状而不改变元素数量和元素值\n\n\n````python\nX = x.reshape(3, 4) # 不改变元素数量和元素值的前提下，改变张量的形状。\nX\n\n\"\"\"\n结果为：\ntensor([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]])\n\"\"\"\n````\n\n### **`zeros((a,b,c...))`函数：**\n\n> 构造全0张量\n\n\n````python\ntorch.zeros((2, 3, 4))\n\n'''\n结果为：\ntensor([[[0., 0., 0., 0.],\n         [0., 0., 0., 0.],\n         [0., 0., 0., 0.]],\n\n        [[0., 0., 0., 0.],\n         [0., 0., 0., 0.],\n         [0., 0., 0., 0.]]])\n'''\n````\n\n### `ones((a,b,c...))`函数：\n\n> 构造全1张量\n\n\n````python\ntorch.ones((2, 3, 4)) \n\n'''\n结果为：\ntensor([[[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]],\n\n        [[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]]])\n'''\n\n````\n\n### **`randn(a,b,c...)`函数**：\n\n> 从标准高斯（正态）分布中随机采样\n\n\n````python\ntorch.randn(3, 4) \n\n\n'''\n结果为：\ntensor([[-0.8709,  0.4194, -0.4292,  0.1850],\n        [-0.3330, -0.6349, -0.1422, -1.0355],\n        [-0.7531,  0.5163,  2.4913,  0.3060]])\n'''\n````\n\n通过提供包含数值的Python列表（或嵌套列表），来为所需张量中的每个元素赋予确定值\n\n\n````python\ntorch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n\n'''\n结果为：\ntensor([[2, 1, 4, 3],\n        [1, 2, 3, 4],\n        [4, 3, 2, 1]])\n'''\n````\n\n### **标准算术运算符`（+、-、*、/和**）`**：\n\n> 都可以被升级为按元素运算\n\n\n````python\nx = torch.tensor([1.0, 2, 4, 8])\ny = torch.tensor([2, 2, 2, 2])\nx + y, x - y, x * y, x / y, x ** y\n\n\n'''\n结果为：\n(tensor([ 3.,  4.,  6., 10.]),\n tensor([-1.,  0.,  2.,  6.]),\n tensor([ 2.,  4.,  8., 16.]),\n tensor([0.5000, 1.0000, 2.0000, 4.0000]),\n tensor([ 1.,  4., 16., 64.]))\n '''\n\n````\n\n### **求幂运算**\n\n\n````python\ntorch.exp(x)\n\n'''\n结果为：\ntensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])\n'''\n````\n\n### 张量连结（concatenate）：\n\n> 我们也可以把多个张量*连结*（concatenate）在一起\n\n\n````python\nX = torch.arange(12, dtype=torch.float32).reshape((3,4))\n# 定义一个12个元素的以为数组X，元素类型为浮点型，改变其形状为：（3,4）\n\nY = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n# 定义一个张量Y,并给元素赋值\n\ntorch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)\n# 将X和Y在0轴上连接合并并输出（0轴为高度方向）； 将X和Y在1轴上合并并输出（1轴为宽度方向）\n\n\n'''\n结果为：\n(tensor([[ 0.,  1.,  2.,  3.],\n         [ 4.,  5.,  6.,  7.],\n         [ 8.,  9., 10., 11.],\n         [ 2.,  1.,  4.,  3.],\n         [ 1.,  2.,  3.,  4.],\n         [ 4.,  3.,  2.,  1.]]),\n tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))\n'''\n````\n\n### 逻辑运算符\n\n> 使用***逻辑运算符***构建二元张量，`True` or `False`\n\n\n````python\nX > Y\n\n\n'''\n结果为：\ntensor([[False, False, False, False],\n        [ True,  True,  True,  True],\n        [ True,  True,  True,  True]])\n'''\n````\n\n对张量中的所有元素进行求和，会产生一个单元素张量\n\n\n```python\nX.sum()\n\n# 结果为：tensor(66.)\n```\n\n\n\n### 广播机制：\n\n> 即使形状不同，我们仍然可以通过调用***广播机制*（broadcasting mechanism）**来执行按元素操作。通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状。\n\n\n````python\na = torch.arange(9).reshape((3, 1,-1))   # \"-1\"表示该轴会自动计算长度\n# 定义一个包含（0-8）的张量a，形状为（3,1,3）,由最外层往内数\n\nb = torch.arange(3).reshape((1, 3,-1))\n# 定义一个包含（0-2）的张量b，形状为（1,3,1）\n\na, b\n# 输出张量a，输出张量b\n\n\n'''\n结果为：\n(tensor([[[0, 1, 2]],\n \n         [[3, 4, 5]],\n \n         [[6, 7, 8]]]),\n tensor([[[0],\n          [1],\n          [2]]]))\n   '''\n````\n\n不同形状的张量相加：\n\n\n````python\na + b\n# 由于a的形状为(3,1,3)\n\n'''\n结果为：\ntensor([[[ 0,  1,  2],\n         [ 1,  2,  3],\n         [ 2,  3,  4]],\n\n        [[ 3,  4,  5],\n         [ 4,  5,  6],\n         [ 5,  6,  7]],\n\n        [[ 6,  7,  8],\n         [ 7,  8,  9],\n         [ 8,  9, 10]]])\n'''\n````\n\n### **切片**：\n\n> 可以用`[-1]`选择最后一个元素，可以用`[1:3]`选择第二个和第三个元素\n\n\n````python\nX[-1], X[1:3]\n# 输出X中的最后一行元素，输出X中的第二行和第三行元素\n\n'''\n结果为：\n(tensor([ 8.,  9., 10., 11.]),\n tensor([[ 4.,  5.,  6.,  7.],\n         [ 8.,  9., 10., 11.]]))\n'''\n\n````\n\n### **通过索引修改元素的值**\n\n> 除读取外，我们还可以通过指定索引来将元素写入矩阵\n\n\n````python\nX[1, 2] = 9          # 将X中第二行第三列的元素值改为9\n\nX                    # 输出X\n\n'''\n结果为：\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  9.,  7.],\n        [ 8.,  9., 10., 11.]])\n'''\n\n````\n\n### **通过切片赋值**：\n\n> 为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值\n\n\n````python\nX[0:2, :] = 12 \n# 将12赋值给X中的第一行和第二行的每一个元素\n\nX  # 输出X\n\n'''\n结果为：\ntensor([[12., 12., 12., 12.],\n        [12., 12., 12., 12.],\n        [ 8.,  9., 10., 11.]])\n'''\n````\n\n### **内存分配问题：**\n\n运行一些操作可能会导致为新结果分配内存\n\n\n```python\nbefore = id(Y)\n# 将Y的id赋值给before变量\n\nY = Y + X\n# 执行矩阵相加\n\nid(Y) == before\n# 将新的Y的id和之前的Y的id进行逻辑比较，并输出逻辑运算结果\n\n# 结果为：False\n\n```\n\n执行原地操作\n\n\n````python\nZ = torch.zeros_like(Y)\n# 生成一个新的全0矩阵：Z，使得Z的形状与Y一样\n\nprint('id(Z):', id(Z))\n# 打印Z的id\n\nZ[:] = X + Y\n# 执行原地操作，Z的id不变\n\nprint('id(Z):', id(Z))\n# 打印Z的id\n\n\n'''\n结果为：\nid(Z): 139931132035296\nid(Z): 139931132035296\n'''\n````\n\n如果在后续计算中没有重复使用`X`，我们也可以使用`X[:] = X + Y`或`X += Y`来减少操作的内存开销\n\n\n````python\nbefore = id(X)\nX += Y\nid(X) == before\n\n'''\n结果为：\nTrue\n'''\n````\n\n### 转换为`NumPy`张量（`ndarray`）\n\n\n````python\nA = X.numpy()\n# 将X转换成Numpy数组，并赋值给A\n\nB = torch.tensor(A)\n# 将A转变成张量，并赋值给B\n\ntype(A), type(B)\n# 输出A的数据类型，输出B的数据类型\n\n\n'''\n结果为：\n(numpy.ndarray, torch.Tensor)\n'''\n````\n\n将大小为1的张量转换为Python标量\n\n\n````python\na = torch.tensor([3.5])\n# 定义一个大小为1，元素为3.5的张量a\na, a.item(), float(a), int(a)\n输出a，标量a，浮点数a，整数a的值\n\n\n'''\n结果为：\n(tensor([3.5000]), 3.5, 3.5, 3)\n'''\n````\n\n## 数据预处理\n\n### CSV（逗号分隔值）文件操作\n\n> 创建一个人工数据集，并存储在CSV（逗号分隔值）文件\n\n\n```python\nimport os\n\nos.makedirs(os.path.join('..', 'data'), exist_ok=True)\ndata_file = os.path.join('..', 'data', 'house_tiny.csv')\nwith open(data_file, 'w') as f:\n    f.write('NumRooms,Alley,Price\\n')\n    f.write('NA,Pave,127500\\n')\n    f.write('2,NA,106000\\n')\n    f.write('4,NA,178100\\n')\n    f.write('NA,NA,140000\\n')\n```\n\n> 从创建的CSV文件中加载原始数据集\n\n\n````python\nimport pandas as pd\n\ndata = pd.read_csv(data_file)\nprint(data)\n\n'''\n结果为：\n     NumRooms Alley   Price\n      NaN    Pave     127500\n      2.0     NaN     106000\n      4.0     NaN     178100\n      NaN     NaN     140000\n'''\n\n````\n\n> 为了处理缺失的数据，典型的方法包括***插值法***和***删除法***，这里，我们将考虑**插值法**\n\n\n````python\ninputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]\n# iloc是csv的索引方法\ninputs = inputs.fillna(inputs.mean())\n# 用同一列的均值替换NaN\nprint(inputs)\n\n'''\n结果为：\n     NumRooms Alley\n       3.0    Pave\n       2.0    NaN\n       4.0    NaN\n       3.0    NaN\n'''\n````\n\n> 对于`inputs`中的类别值或离散值，我们将`“NaN”`视为一个类别\n\n\n````python\ninputs = pd.get_dummies(inputs, dummy_na=True)\n#转换成pandas表格？？？\nprint(inputs)\n\n'''\n结果为：\n       NumRooms  Alley_Pave  Alley_nan\n       3.0           1          0\n       2.0           0          1\n       4.0           0          1\n       3.0           0          1\n'''\n````\n\n> 现在`inputs`和`outputs`中的所有条目都是数值类型，它们可以转换为张量格式\n\n\n````python\nimport torch\n\nX, y = torch.tensor(inputs.values), torch.tensor(outputs.values)\n# 将表格数据拆分，均转换为张量\nX, y\n\n结果为：\n'''\n(tensor([[3., 1., 0.],\n         [2., 0., 1.],\n         [4., 0., 1.],\n         [3., 0., 1.]], dtype=torch.float64),\n tensor([127500, 106000, 178100, 140000]))\n'''\n````\n\n## 线性代数\n\n### 标量与张量\n\n> 标量由只有一个元素的张量表示\n\n\n```python\nimport torch\n\nx = torch.tensor(3.0)\ny = torch.tensor(2.0)\n\nx + y, x * y, x / y, x**y\n\n# (tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))\n```\n\n> 向量可以被视为标量值组成的列表\n\n\n```python\nx = torch.arange(4)\nx\n\n# tensor([0, 1, 2, 3])\n```\n\n> 通过张量的索引来访问任一元素\n\n\n```python\nx[3]\n\n# tensor(3)\n```\n\n> 访问张量的长度\n\n\n```python\nlen(x)\n# 4\n```\n\n> 只有一个轴的张量，形状只有一个元素\n\n\n```python\nx.shape\n\n# torch.Size([4])\n```\n\n### 矩阵\n\n> 通过指定两个分量$m$和$n$来创建一个形状为$m \\times n$的**矩阵**\n\n\n```python\nA = torch.arange(20).reshape(5, 4)\nA\n\n'''\n结果为：\ntensor([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11],\n        [12, 13, 14, 15],\n        [16, 17, 18, 19]])\n \n'''\n```\n\n> 矩阵的转置\n\n\n```python\nA.T\n\n\n'''\n结果为：\ntensor([[ 0,  4,  8, 12, 16],\n        [ 1,  5,  9, 13, 17],\n        [ 2,  6, 10, 14, 18],\n        [ 3,  7, 11, 15, 19]])\n'''\n\n```\n\n> *对称矩阵*（symmetric matrix）$\\mathbf{A}$等于其转置：$\\mathbf{A} = \\mathbf{A}^\\top$\n\n\n```python\nB = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\nB\n\n'''\n结果为：\ntensor([[1, 2, 3],\n        [2, 0, 4],\n        [3, 4, 5]])\n'''\n\n```\n\n\n```python\nB == B.T\n\n'''\n结果为：\ntensor([[True, True, True],\n        [True, True, True],\n        [True, True, True]])\n'''\n```\n\n> 就像向量是标量的推广，矩阵是向量的推广一样，我们可以构建具有更多轴的数据结构\n\n\n```python\nX = torch.arange(24).reshape(2, 3, 4)\nX\n\n'''\n结果为：\ntensor([[[ 0,  1,  2,  3],\n         [ 4,  5,  6,  7],\n         [ 8,  9, 10, 11]],\n\n        [[12, 13, 14, 15],\n         [16, 17, 18, 19],\n         [20, 21, 22, 23]]])\n'''\n\n```\n\n> 给定具有**相同形状**的任意两个张量，任何按元素**二元运算**的结果都将是相同形状的张量\n\n\n```python\nA = torch.arange(20, dtype=torch.float32).reshape(5, 4)\nB = A.clone()\nA, A + B\n\n'''\n结果为：\n(tensor([[ 0.,  1.,  2.,  3.],\n         [ 4.,  5.,  6.,  7.],\n         [ 8.,  9., 10., 11.],\n         [12., 13., 14., 15.],\n         [16., 17., 18., 19.]]),\n tensor([[ 0.,  2.,  4.,  6.],\n         [ 8., 10., 12., 14.],\n         [16., 18., 20., 22.],\n         [24., 26., 28., 30.],\n         [32., 34., 36., 38.]]))\n'''\n```\n\n> 两个矩阵的按元素乘法称为*Hadamard积*（Hadamard product）（数学符号$\\odot$）\n\n\n```python\nA * B\n\n'''\n结果为：\ntensor([[  0.,   1.,   4.,   9.],\n        [ 16.,  25.,  36.,  49.],\n        [ 64.,  81., 100., 121.],\n        [144., 169., 196., 225.],\n        [256., 289., 324., 361.]])\n'''\n```\n\n\n```python\na = 2\nX = torch.arange(24).reshape(2, 3, 4)\na + X, (a * X).shape\n\n'''\n结果为：\n(tensor([[[ 2,  3,  4,  5],\n          [ 6,  7,  8,  9],\n          [10, 11, 12, 13]],\n \n         [[14, 15, 16, 17],\n          [18, 19, 20, 21],\n          [22, 23, 24, 25]]]),\n torch.Size([2, 3, 4]))\n'''\n```\n\n> 计算其元素的和\n\n\n```python\nx = torch.arange(4, dtype=torch.float32)\nx, x.sum()\n\n# (tensor([0., 1., 2., 3.]), tensor(6.))\n```\n\n> 表示任意形状张量的元素和\n\n\n```python\nA.shape, A.sum()\n\n# (torch.Size([5, 4]), tensor(190.))\n```\n\n### 降维\n\n> 指定张量沿哪一个轴来通过求和降低维度\n\n\n```python\nA_sum_axis0 = A.sum(axis=0)\n# 0轴上压缩，即压缩高度\nA_sum_axis0, A_sum_axis0.shape\n\n# (tensor([40., 45., 50., 55.]), torch.Size([4]))\n```\n\n\n```python\nA_sum_axis1 = A.sum(axis=1)\n# 1轴上压缩，即压缩宽度\nA_sum_axis1, A_sum_axis1.shape\n\n\n# (tensor([ 6., 22., 38., 54., 70.]), torch.Size([5]))\n```\n\n\n```python\nA.sum(axis=[0, 1])\n# 同时在0轴和1轴上压缩，压缩高宽\n\n# tensor(190.)\n```\n\n> 一个与求和相关的量是*平均值*（mean或average）\n\n\n```python\nA.mean(), A.sum() / A.numel()\n\n# (tensor(9.5000), tensor(9.5000))\n```\n\n\n```python\nA.mean(axis=0), A.sum(axis=0) / A.shape[0]\n\n\n# (tensor([ 8.,  9., 10., 11.]), tensor([ 8.,  9., 10., 11.]))\n```\n\n### 非降维求和\n\n> 计算总和或均值时**保持轴数不变**\n\n\n```python\nsum_A = A.sum(axis=1, keepdims=True)\nsum_A\n\n'''\n结果为：\ntensor([[ 6.],\n        [22.],\n        [38.],\n        [54.],\n        [70.]])\n'''\n```\n\n> 通过广播将`A`除以`sum_A`\n\n\n```python\nA / sum_A\n\n'''\n结果为：\ntensor([[0.0000, 0.1667, 0.3333, 0.5000],\n        [0.1818, 0.2273, 0.2727, 0.3182],\n        [0.2105, 0.2368, 0.2632, 0.2895],\n        [0.2222, 0.2407, 0.2593, 0.2778],\n        [0.2286, 0.2429, 0.2571, 0.2714]])\n'''\n```\n\n> 某个轴计算`A`元素的累积总和\n\n\n```python\nA.cumsum(axis=0)\n\n'''\n结果为：\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  6.,  8., 10.],\n        [12., 15., 18., 21.],\n        [24., 28., 32., 36.],\n        [40., 45., 50., 55.]])\n'''\n```\n\n### 点积\n\n> 点积是**相同位置的按元素乘积的和**\n\n\n```python\ny = torch.ones(4, dtype = torch.float32)\nx, y, torch.dot(x, y)\n\n# (tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))\n```\n\n我们可以通过执行按元素乘法，然后进行求和来表示两个向量的点积\n\n\n```python\ntorch.sum(x * y)\n\n# tensor(6.)\n```\n\n### 向量积\n\n> 矩阵向量积$\\mathbf{A}\\mathbf{x}$是一个长度为$m$的列向量，其第$i$个元素是点积$\\mathbf{a}^\\top_i \\mathbf{x}$\n\n\n```python\nA.shape, x.shape, torch.mv(A, x)\n\n# (torch.Size([5, 4]), torch.Size([4]), tensor([ 14.,  38.,  62.,  86., 110.]))\n```\n\n> 我们可以将矩阵-矩阵乘法$\\mathbf{AB}$看作简单地**执行$m$次矩阵-向量积**，并将结果拼接在一起，形成一个$n \\times m$矩阵\n\n\n```python\nB = torch.ones(4, 3)\ntorch.mm(A, B)\n\n'''\ntensor([[ 6.,  6.,  6.],\n        [22., 22., 22.],\n        [38., 38., 38.],\n        [54., 54., 54.],\n        [70., 70., 70.]])\n'''\n\n```\n\n### 范数\n\n> $L_2$*范数*是向量**元素平方和的平方根**：\n>\n>  $$\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}$$\n\n\n```python\nu = torch.tensor([3.0, -4.0])\ntorch.norm(u)\n\n# tensor(5.)\n```\n\n> $L_1$范数，它表示为**向量元素的绝对值之和**：\n>\n>   $$\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^n \\left|x_i \\right|$$\n\n\n```python\ntorch.abs(u).sum()\n\n# tensor(7.)\n```\n\n> 矩阵的*Frobenius（弗罗贝尼乌斯）范数*（Frobenius norm）是**矩阵元素平方和的平方根**：$$\\|\\mathbf{X}\\|_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n x_{ij}^2}$$\n\n\n```python\ntorch.norm(torch.ones((4, 9))) # 36开方\n\n# tensor(6.)\n```\n\n## 微积分\n\n> 如果$f$的*导数*存在，这个极限被定义为$$f'(x) = \\lim_{h \\rightarrow 0} \\frac{f(x+h) - f(x)}{h}$$，定义$u=f(x)=3x^2-4x$\n\n\n```python\n%matplotlib inline\nimport numpy as np\nfrom matplotlib_inline import backend_inline\nfrom d2l import torch as d2l\n\n\ndef f(x):\n    return 3 * x ** 2 - 4 * x\n```\n\n> 通过令$x=1$并让$h$接近$0$，$\\frac{f(x+h)-f(x)}{h}$的数值结果接近$2$\n\n\n```python\ndef numerical_lim(f, x, h):\n    return (f(x + h) - f(x)) / h\n\nh = 0.1\nfor i in range(5):\n    print(f'h={h:.5f}, numerical limit={numerical_lim(f, 1, h):.5f}')\n    h *= 0.1\n  \n'''\n结果为：\nh=0.10000, numerical limit=2.30000\nh=0.01000, numerical limit=2.03000\nh=0.00100, numerical limit=2.00300\nh=0.00010, numerical limit=2.00030\nh=0.00001, numerical limit=2.00003\n'''\n```\n\n> 为了对导数的这种解释进行可视化，我们将使用`matplotlib`定义几个函数\n\n\n```python\ndef use_svg_display():  \n    \"\"\"使用svg格式在Jupyter中显示绘图\"\"\"\n    backend_inline.set_matplotlib_formats('svg')\n\ndef set_figsize(figsize=(3.5, 2.5)):  \n    \"\"\"设置matplotlib的图表大小\"\"\"\n    use_svg_display()\n    d2l.plt.rcParams['figure.figsize'] = figsize\n\ndef set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n    \"\"\"设置matplotlib的轴\"\"\"\n    axes.set_xlabel(xlabel)\n    axes.set_ylabel(ylabel)\n    axes.set_xscale(xscale)\n    axes.set_yscale(yscale)\n    axes.set_xlim(xlim)\n    axes.set_ylim(ylim)\n    if legend:\n        axes.legend(legend)\n    axes.grid()\n\ndef plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None,\n         ylim=None, xscale='linear', yscale='linear',\n         fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):\n    \"\"\"绘制数据点\"\"\"\n    if legend is None:\n        legend = []\n\n    set_figsize(figsize)\n    axes = axes if axes else d2l.plt.gca()\n\n    def has_one_axis(X):\n        return (hasattr(X, \"ndim\") and X.ndim == 1 or isinstance(X, list)\n                and not hasattr(X[0], \"__len__\"))\n\n    if has_one_axis(X):\n        X = [X]\n    if Y is None:\n        X, Y = [[]] * len(X), X\n    elif has_one_axis(Y):\n        Y = [Y]\n    if len(X) != len(Y):\n        X = X * len(Y)\n    axes.cla()\n    for x, y, fmt in zip(X, Y, fmts):\n        if len(x):\n            axes.plot(x, y, fmt)\n        else:\n            axes.plot(y, fmt)\n    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n```\n\n> 绘制函数$u=f(x)$及其在$x=1$处的切线$y=2x-3$\n\n\n```python\nx = np.arange(0, 3, 0.1)\nplot(x, [f(x), 2 * x - 3], 'x', 'f(x)', legend=['f(x)', 'Tangent line (x=1)'])\n```\n\n\n![svg](D2L_ch02_预备知识/output_7_0-16794727634302.svg)\n\n## 自动微分\n\n> 假设我们想对函数$y=2\\mathbf{x}^{\\top}\\mathbf{x}$关于列向量$\\mathbf{x}$求导\n\n\n```python\nimport torch\n\nx = torch.arange(4.0)\nx\n\n'''tensor([0., 1., 2., 3.])'''\n```\n\n> 在我们计算$y$关于$\\mathbf{x}$的梯度之前，需要一个地方来存储梯度\n\n\n```python\nx.requires_grad_(True)\nx.grad\n```\n\n> 现在计算$y$\n\n\n```python\ny = 2 * torch.dot(x, x)\ny\n\n'''tensor(28., grad_fn=<MulBackward0>)'''\n```\n\n> 通过调用反向传播函数来自动计算`y`关于`x`每个分量的梯度\n\n\n```python\ny.backward()\nx.grad\n\n'''tensor([ 0.,  4.,  8., 12.])'''\n```\n\n\n\n\n```python\nx.grad == 4 * x\n\n'''tensor([True, True, True, True])'''\n```\n\n> 现在计算`x`的另一个函数\n\n\n```python\nx.grad.zero_()\ny = x.sum()\ny.backward()\nx.grad\n\n'''tensor([1., 1., 1., 1.])'''\n```\n\n> 深度学习中，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和\n\n\n```python\nx.grad.zero_()\ny = x * x\ny.sum().backward()\nx.grad\n\n'''tensor([0., 2., 4., 6.])'''\n```\n\n> 将某些计算移动到记录的计算图之外\n\n\n```python\nx.grad.zero_()\ny = x * x\nu = y.detach()\nz = u * x\n\nz.sum().backward()\nx.grad == u\n\n'''tensor([True, True, True, True])'''\n```\n\n\n```python\nx.grad.zero_()\ny.sum().backward()\nx.grad == 2 * x\n\n'''tensor([True, True, True, True])'''\n```\n\n> 即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度\n\n\n```python\ndef f(a):\n    b = a * 2\n    while b.norm() < 1000:\n        b = b * 2\n    if b.sum() > 0:\n        c = b\n    else:\n        c = 100 * b\n    return c\n\na = torch.randn(size=(), requires_grad=True)\nd = f(a)\nd.backward()\n\na.grad == d / a\n\n\n'''tensor(True)'''\n```\n\n\n\n","slug":"学习记录/DataWhale/D2L_ch02_预备知识","published":1,"updated":"2023-03-22T08:52:05.435Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clfjkj0bt000e8gsz69jyfuie","content":"<h2 id=\"数据操作\"><a href=\"#数据操作\" class=\"headerlink\" title=\"数据操作\"></a>数据操作</h2><p>首先，我们导入<code>torch</code>。请注意，虽然它被称为<code>PyTorch</code>，但是代码中使用<code>torch</code>而不是<code>pytorch</code></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch </span><br></pre></td></tr></table></figure>\n<h3 id=\"张量：\"><a href=\"#张量：\" class=\"headerlink\" title=\"张量：\"></a><strong>张量</strong>：</h3><blockquote>\n<p>表示一个由数值组成的数组，这个数组可能有多个维度</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.arange(<span class=\"number\">12</span>)</span><br><span class=\"line\">x</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 结果为：tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"shape属性：\"><a href=\"#shape属性：\" class=\"headerlink\" title=\"shape属性：\"></a><strong><code>shape</code>属性：</strong></h3><blockquote>\n<p>访问张量（沿每个轴的长度）的<em>形状</em></p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x.shape  <span class=\"comment\"># 获取张量的形状</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 结果为： torch.Size([12])</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"numel函数：\"><a href=\"#numel函数：\" class=\"headerlink\" title=\"numel函数：\"></a><strong><code>numel</code>函数：</strong></h3><blockquote>\n<p>获取张量中元素的总数</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x.numel() <span class=\"comment\"># 获取张量中元素的总数</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 结果为： 12</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"reshape函数：\"><a href=\"#reshape函数：\" class=\"headerlink\" title=\"reshape函数：\"></a><strong><code>reshape</code>函数：</strong></h3><blockquote>\n<p>改变一个张量的形状而不改变元素数量和元素值</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = x.reshape(<span class=\"number\">3</span>, <span class=\"number\">4</span>) <span class=\"comment\"># 不改变元素数量和元素值的前提下，改变张量的形状。</span></span><br><span class=\"line\">X</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([[ 0,  1,  2,  3],</span></span><br><span class=\"line\"><span class=\"string\">        [ 4,  5,  6,  7],</span></span><br><span class=\"line\"><span class=\"string\">        [ 8,  9, 10, 11]])</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"zeros-a-b-c-函数：\"><a href=\"#zeros-a-b-c-函数：\" class=\"headerlink\" title=\"zeros((a,b,c...))函数：\"></a><strong><code>zeros((a,b,c...))</code>函数：</strong></h3><blockquote>\n<p>构造全0张量</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.zeros((<span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([[[0., 0., 0., 0.],</span></span><br><span class=\"line\"><span class=\"string\">         [0., 0., 0., 0.],</span></span><br><span class=\"line\"><span class=\"string\">         [0., 0., 0., 0.]],</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">        [[0., 0., 0., 0.],</span></span><br><span class=\"line\"><span class=\"string\">         [0., 0., 0., 0.],</span></span><br><span class=\"line\"><span class=\"string\">         [0., 0., 0., 0.]]])</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"ones-a-b-c-函数：\"><a href=\"#ones-a-b-c-函数：\" class=\"headerlink\" title=\"ones((a,b,c...))函数：\"></a><code>ones((a,b,c...))</code>函数：</h3><blockquote>\n<p>构造全1张量</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.ones((<span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>)) </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([[[1., 1., 1., 1.],</span></span><br><span class=\"line\"><span class=\"string\">         [1., 1., 1., 1.],</span></span><br><span class=\"line\"><span class=\"string\">         [1., 1., 1., 1.]],</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">        [[1., 1., 1., 1.],</span></span><br><span class=\"line\"><span class=\"string\">         [1., 1., 1., 1.],</span></span><br><span class=\"line\"><span class=\"string\">         [1., 1., 1., 1.]]])</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"randn-a-b-c-函数：\"><a href=\"#randn-a-b-c-函数：\" class=\"headerlink\" title=\"randn(a,b,c...)函数：\"></a><strong><code>randn(a,b,c...)</code>函数</strong>：</h3><blockquote>\n<p>从标准高斯（正态）分布中随机采样</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.randn(<span class=\"number\">3</span>, <span class=\"number\">4</span>) </span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([[-0.8709,  0.4194, -0.4292,  0.1850],</span></span><br><span class=\"line\"><span class=\"string\">        [-0.3330, -0.6349, -0.1422, -1.0355],</span></span><br><span class=\"line\"><span class=\"string\">        [-0.7531,  0.5163,  2.4913,  0.3060]])</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>通过提供包含数值的Python列表（或嵌套列表），来为所需张量中的每个元素赋予确定值</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.tensor([[<span class=\"number\">2</span>, <span class=\"number\">1</span>, <span class=\"number\">4</span>, <span class=\"number\">3</span>], [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>], [<span class=\"number\">4</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>]])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([[2, 1, 4, 3],</span></span><br><span class=\"line\"><span class=\"string\">        [1, 2, 3, 4],</span></span><br><span class=\"line\"><span class=\"string\">        [4, 3, 2, 1]])</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"标准算术运算符-（-、-、-、-和）-：\"><a href=\"#标准算术运算符-（-、-、-、-和）-：\" class=\"headerlink\" title=\"标准算术运算符`（+、-、*、/和）`**：\"></a><strong>标准算术运算符`（+、-、*、/和</strong>）`**：</h3><blockquote>\n<p>都可以被升级为按元素运算</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.tensor([<span class=\"number\">1.0</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">8</span>])</span><br><span class=\"line\">y = torch.tensor([<span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>])</span><br><span class=\"line\">x + y, x - y, x * y, x / y, x ** y</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">(tensor([ 3.,  4.,  6., 10.]),</span></span><br><span class=\"line\"><span class=\"string\"> tensor([-1.,  0.,  2.,  6.]),</span></span><br><span class=\"line\"><span class=\"string\"> tensor([ 2.,  4.,  8., 16.]),</span></span><br><span class=\"line\"><span class=\"string\"> tensor([0.5000, 1.0000, 2.0000, 4.0000]),</span></span><br><span class=\"line\"><span class=\"string\"> tensor([ 1.,  4., 16., 64.]))</span></span><br><span class=\"line\"><span class=\"string\"> &#x27;&#x27;&#x27;</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"求幂运算\"><a href=\"#求幂运算\" class=\"headerlink\" title=\"求幂运算\"></a><strong>求幂运算</strong></h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.exp(x)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"张量连结（concatenate）：\"><a href=\"#张量连结（concatenate）：\" class=\"headerlink\" title=\"张量连结（concatenate）：\"></a>张量连结（concatenate）：</h3><blockquote>\n<p>我们也可以把多个张量<em>连结</em>（concatenate）在一起</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = torch.arange(<span class=\"number\">12</span>, dtype=torch.float32).reshape((<span class=\"number\">3</span>,<span class=\"number\">4</span>))</span><br><span class=\"line\"><span class=\"comment\"># 定义一个12个元素的以为数组X，元素类型为浮点型，改变其形状为：（3,4）</span></span><br><span class=\"line\"></span><br><span class=\"line\">Y = torch.tensor([[<span class=\"number\">2.0</span>, <span class=\"number\">1</span>, <span class=\"number\">4</span>, <span class=\"number\">3</span>], [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>], [<span class=\"number\">4</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>]])</span><br><span class=\"line\"><span class=\"comment\"># 定义一个张量Y,并给元素赋值</span></span><br><span class=\"line\"></span><br><span class=\"line\">torch.cat((X, Y), dim=<span class=\"number\">0</span>), torch.cat((X, Y), dim=<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"comment\"># 将X和Y在0轴上连接合并并输出（0轴为高度方向）； 将X和Y在1轴上合并并输出（1轴为宽度方向）</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">(tensor([[ 0.,  1.,  2.,  3.],</span></span><br><span class=\"line\"><span class=\"string\">         [ 4.,  5.,  6.,  7.],</span></span><br><span class=\"line\"><span class=\"string\">         [ 8.,  9., 10., 11.],</span></span><br><span class=\"line\"><span class=\"string\">         [ 2.,  1.,  4.,  3.],</span></span><br><span class=\"line\"><span class=\"string\">         [ 1.,  2.,  3.,  4.],</span></span><br><span class=\"line\"><span class=\"string\">         [ 4.,  3.,  2.,  1.]]),</span></span><br><span class=\"line\"><span class=\"string\"> tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],</span></span><br><span class=\"line\"><span class=\"string\">         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],</span></span><br><span class=\"line\"><span class=\"string\">         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"逻辑运算符\"><a href=\"#逻辑运算符\" class=\"headerlink\" title=\"逻辑运算符\"></a>逻辑运算符</h3><blockquote>\n<p>使用<strong><em>逻辑运算符</em></strong>构建二元张量，<code>True</code> or <code>False</code></p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X &gt; Y</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([[False, False, False, False],</span></span><br><span class=\"line\"><span class=\"string\">        [ True,  True,  True,  True],</span></span><br><span class=\"line\"><span class=\"string\">        [ True,  True,  True,  True]])</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>对张量中的所有元素进行求和，会产生一个单元素张量</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X.<span class=\"built_in\">sum</span>()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 结果为：tensor(66.)</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"广播机制：\"><a href=\"#广播机制：\" class=\"headerlink\" title=\"广播机制：\"></a>广播机制：</h3><blockquote>\n<p>即使形状不同，我们仍然可以通过调用<strong><em>广播机制</em>（broadcasting mechanism）</strong>来执行按元素操作。通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状。</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a = torch.arange(<span class=\"number\">9</span>).reshape((<span class=\"number\">3</span>, <span class=\"number\">1</span>,-<span class=\"number\">1</span>))   <span class=\"comment\"># &quot;-1&quot;表示该轴会自动计算长度</span></span><br><span class=\"line\"><span class=\"comment\"># 定义一个包含（0-8）的张量a，形状为（3,1,3）,由最外层往内数</span></span><br><span class=\"line\"></span><br><span class=\"line\">b = torch.arange(<span class=\"number\">3</span>).reshape((<span class=\"number\">1</span>, <span class=\"number\">3</span>,-<span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"comment\"># 定义一个包含（0-2）的张量b，形状为（1,3,1）</span></span><br><span class=\"line\"></span><br><span class=\"line\">a, b</span><br><span class=\"line\"><span class=\"comment\"># 输出张量a，输出张量b</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">(tensor([[[0, 1, 2]],</span></span><br><span class=\"line\"><span class=\"string\"> </span></span><br><span class=\"line\"><span class=\"string\">         [[3, 4, 5]],</span></span><br><span class=\"line\"><span class=\"string\"> </span></span><br><span class=\"line\"><span class=\"string\">         [[6, 7, 8]]]),</span></span><br><span class=\"line\"><span class=\"string\"> tensor([[[0],</span></span><br><span class=\"line\"><span class=\"string\">          [1],</span></span><br><span class=\"line\"><span class=\"string\">          [2]]]))</span></span><br><span class=\"line\"><span class=\"string\">   &#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>不同形状的张量相加：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a + b</span><br><span class=\"line\"><span class=\"comment\"># 由于a的形状为(3,1,3)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([[[ 0,  1,  2],</span></span><br><span class=\"line\"><span class=\"string\">         [ 1,  2,  3],</span></span><br><span class=\"line\"><span class=\"string\">         [ 2,  3,  4]],</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">        [[ 3,  4,  5],</span></span><br><span class=\"line\"><span class=\"string\">         [ 4,  5,  6],</span></span><br><span class=\"line\"><span class=\"string\">         [ 5,  6,  7]],</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">        [[ 6,  7,  8],</span></span><br><span class=\"line\"><span class=\"string\">         [ 7,  8,  9],</span></span><br><span class=\"line\"><span class=\"string\">         [ 8,  9, 10]]])</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"切片：\"><a href=\"#切片：\" class=\"headerlink\" title=\"切片：\"></a><strong>切片</strong>：</h3><blockquote>\n<p>可以用<code>[-1]</code>选择最后一个元素，可以用<code>[1:3]</code>选择第二个和第三个元素</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X[-<span class=\"number\">1</span>], X[<span class=\"number\">1</span>:<span class=\"number\">3</span>]</span><br><span class=\"line\"><span class=\"comment\"># 输出X中的最后一行元素，输出X中的第二行和第三行元素</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">(tensor([ 8.,  9., 10., 11.]),</span></span><br><span class=\"line\"><span class=\"string\"> tensor([[ 4.,  5.,  6.,  7.],</span></span><br><span class=\"line\"><span class=\"string\">         [ 8.,  9., 10., 11.]]))</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"通过索引修改元素的值\"><a href=\"#通过索引修改元素的值\" class=\"headerlink\" title=\"通过索引修改元素的值\"></a><strong>通过索引修改元素的值</strong></h3><blockquote>\n<p>除读取外，我们还可以通过指定索引来将元素写入矩阵</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X[<span class=\"number\">1</span>, <span class=\"number\">2</span>] = <span class=\"number\">9</span>          <span class=\"comment\"># 将X中第二行第三列的元素值改为9</span></span><br><span class=\"line\"></span><br><span class=\"line\">X                    <span class=\"comment\"># 输出X</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([[ 0.,  1.,  2.,  3.],</span></span><br><span class=\"line\"><span class=\"string\">        [ 4.,  5.,  9.,  7.],</span></span><br><span class=\"line\"><span class=\"string\">        [ 8.,  9., 10., 11.]])</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"通过切片赋值：\"><a href=\"#通过切片赋值：\" class=\"headerlink\" title=\"通过切片赋值：\"></a><strong>通过切片赋值</strong>：</h3><blockquote>\n<p>为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X[<span class=\"number\">0</span>:<span class=\"number\">2</span>, :] = <span class=\"number\">12</span> </span><br><span class=\"line\"><span class=\"comment\"># 将12赋值给X中的第一行和第二行的每一个元素</span></span><br><span class=\"line\"></span><br><span class=\"line\">X  <span class=\"comment\"># 输出X</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([[12., 12., 12., 12.],</span></span><br><span class=\"line\"><span class=\"string\">        [12., 12., 12., 12.],</span></span><br><span class=\"line\"><span class=\"string\">        [ 8.,  9., 10., 11.]])</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"内存分配问题：\"><a href=\"#内存分配问题：\" class=\"headerlink\" title=\"内存分配问题：\"></a><strong>内存分配问题：</strong></h3><p>运行一些操作可能会导致为新结果分配内存</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">before = <span class=\"built_in\">id</span>(Y)</span><br><span class=\"line\"><span class=\"comment\"># 将Y的id赋值给before变量</span></span><br><span class=\"line\"></span><br><span class=\"line\">Y = Y + X</span><br><span class=\"line\"><span class=\"comment\"># 执行矩阵相加</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">id</span>(Y) == before</span><br><span class=\"line\"><span class=\"comment\"># 将新的Y的id和之前的Y的id进行逻辑比较，并输出逻辑运算结果</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 结果为：False</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>执行原地操作</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Z = torch.zeros_like(Y)</span><br><span class=\"line\"><span class=\"comment\"># 生成一个新的全0矩阵：Z，使得Z的形状与Y一样</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;id(Z):&#x27;</span>, <span class=\"built_in\">id</span>(Z))</span><br><span class=\"line\"><span class=\"comment\"># 打印Z的id</span></span><br><span class=\"line\"></span><br><span class=\"line\">Z[:] = X + Y</span><br><span class=\"line\"><span class=\"comment\"># 执行原地操作，Z的id不变</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;id(Z):&#x27;</span>, <span class=\"built_in\">id</span>(Z))</span><br><span class=\"line\"><span class=\"comment\"># 打印Z的id</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">id(Z): 139931132035296</span></span><br><span class=\"line\"><span class=\"string\">id(Z): 139931132035296</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>如果在后续计算中没有重复使用<code>X</code>，我们也可以使用<code>X[:] = X + Y</code>或<code>X += Y</code>来减少操作的内存开销</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">before = <span class=\"built_in\">id</span>(X)</span><br><span class=\"line\">X += Y</span><br><span class=\"line\"><span class=\"built_in\">id</span>(X) == before</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">True</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"转换为NumPy张量（ndarray）\"><a href=\"#转换为NumPy张量（ndarray）\" class=\"headerlink\" title=\"转换为NumPy张量（ndarray）\"></a>转换为<code>NumPy</code>张量（<code>ndarray</code>）</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A = X.numpy()</span><br><span class=\"line\"><span class=\"comment\"># 将X转换成Numpy数组，并赋值给A</span></span><br><span class=\"line\"></span><br><span class=\"line\">B = torch.tensor(A)</span><br><span class=\"line\"><span class=\"comment\"># 将A转变成张量，并赋值给B</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">type</span>(A), <span class=\"built_in\">type</span>(B)</span><br><span class=\"line\"><span class=\"comment\"># 输出A的数据类型，输出B的数据类型</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">(numpy.ndarray, torch.Tensor)</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>将大小为1的张量转换为Python标量</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a = torch.tensor([<span class=\"number\">3.5</span>])</span><br><span class=\"line\"><span class=\"comment\"># 定义一个大小为1，元素为3.5的张量a</span></span><br><span class=\"line\">a, a.item(), <span class=\"built_in\">float</span>(a), <span class=\"built_in\">int</span>(a)</span><br><span class=\"line\">输出a，标量a，浮点数a，整数a的值</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">(tensor([3.5000]), 3.5, 3.5, 3)</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"数据预处理\"><a href=\"#数据预处理\" class=\"headerlink\" title=\"数据预处理\"></a>数据预处理</h2><h3 id=\"CSV（逗号分隔值）文件操作\"><a href=\"#CSV（逗号分隔值）文件操作\" class=\"headerlink\" title=\"CSV（逗号分隔值）文件操作\"></a>CSV（逗号分隔值）文件操作</h3><blockquote>\n<p>创建一个人工数据集，并存储在CSV（逗号分隔值）文件</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"></span><br><span class=\"line\">os.makedirs(os.path.join(<span class=\"string\">&#x27;..&#x27;</span>, <span class=\"string\">&#x27;data&#x27;</span>), exist_ok=<span class=\"literal\">True</span>)</span><br><span class=\"line\">data_file = os.path.join(<span class=\"string\">&#x27;..&#x27;</span>, <span class=\"string\">&#x27;data&#x27;</span>, <span class=\"string\">&#x27;house_tiny.csv&#x27;</span>)</span><br><span class=\"line\"><span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(data_file, <span class=\"string\">&#x27;w&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">    f.write(<span class=\"string\">&#x27;NumRooms,Alley,Price\\n&#x27;</span>)</span><br><span class=\"line\">    f.write(<span class=\"string\">&#x27;NA,Pave,127500\\n&#x27;</span>)</span><br><span class=\"line\">    f.write(<span class=\"string\">&#x27;2,NA,106000\\n&#x27;</span>)</span><br><span class=\"line\">    f.write(<span class=\"string\">&#x27;4,NA,178100\\n&#x27;</span>)</span><br><span class=\"line\">    f.write(<span class=\"string\">&#x27;NA,NA,140000\\n&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>从创建的CSV文件中加载原始数据集</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"></span><br><span class=\"line\">data = pd.read_csv(data_file)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(data)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">     NumRooms Alley   Price</span></span><br><span class=\"line\"><span class=\"string\">      NaN    Pave     127500</span></span><br><span class=\"line\"><span class=\"string\">      2.0     NaN     106000</span></span><br><span class=\"line\"><span class=\"string\">      4.0     NaN     178100</span></span><br><span class=\"line\"><span class=\"string\">      NaN     NaN     140000</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>为了处理缺失的数据，典型的方法包括<strong><em>插值法</em></strong>和<strong><em>删除法</em></strong>，这里，我们将考虑<strong>插值法</strong></p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inputs, outputs = data.iloc[:, <span class=\"number\">0</span>:<span class=\"number\">2</span>], data.iloc[:, <span class=\"number\">2</span>]</span><br><span class=\"line\"><span class=\"comment\"># iloc是csv的索引方法</span></span><br><span class=\"line\">inputs = inputs.fillna(inputs.mean())</span><br><span class=\"line\"><span class=\"comment\"># 用同一列的均值替换NaN</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(inputs)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">     NumRooms Alley</span></span><br><span class=\"line\"><span class=\"string\">       3.0    Pave</span></span><br><span class=\"line\"><span class=\"string\">       2.0    NaN</span></span><br><span class=\"line\"><span class=\"string\">       4.0    NaN</span></span><br><span class=\"line\"><span class=\"string\">       3.0    NaN</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>对于<code>inputs</code>中的类别值或离散值，我们将<code>“NaN”</code>视为一个类别</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inputs = pd.get_dummies(inputs, dummy_na=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"comment\">#转换成pandas表格？？？</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(inputs)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">       NumRooms  Alley_Pave  Alley_nan</span></span><br><span class=\"line\"><span class=\"string\">       3.0           1          0</span></span><br><span class=\"line\"><span class=\"string\">       2.0           0          1</span></span><br><span class=\"line\"><span class=\"string\">       4.0           0          1</span></span><br><span class=\"line\"><span class=\"string\">       3.0           0          1</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>现在<code>inputs</code>和<code>outputs</code>中的所有条目都是数值类型，它们可以转换为张量格式</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"></span><br><span class=\"line\">X, y = torch.tensor(inputs.values), torch.tensor(outputs.values)</span><br><span class=\"line\"><span class=\"comment\"># 将表格数据拆分，均转换为张量</span></span><br><span class=\"line\">X, y</span><br><span class=\"line\"></span><br><span class=\"line\">结果为：</span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">(tensor([[3., 1., 0.],</span></span><br><span class=\"line\"><span class=\"string\">         [2., 0., 1.],</span></span><br><span class=\"line\"><span class=\"string\">         [4., 0., 1.],</span></span><br><span class=\"line\"><span class=\"string\">         [3., 0., 1.]], dtype=torch.float64),</span></span><br><span class=\"line\"><span class=\"string\"> tensor([127500, 106000, 178100, 140000]))</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"线性代数\"><a href=\"#线性代数\" class=\"headerlink\" title=\"线性代数\"></a>线性代数</h2><h3 id=\"标量与张量\"><a href=\"#标量与张量\" class=\"headerlink\" title=\"标量与张量\"></a>标量与张量</h3><blockquote>\n<p>标量由只有一个元素的张量表示</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"></span><br><span class=\"line\">x = torch.tensor(<span class=\"number\">3.0</span>)</span><br><span class=\"line\">y = torch.tensor(<span class=\"number\">2.0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">x + y, x * y, x / y, x**y</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># (tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>向量可以被视为标量值组成的列表</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.arange(<span class=\"number\">4</span>)</span><br><span class=\"line\">x</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># tensor([0, 1, 2, 3])</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>通过张量的索引来访问任一元素</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x[<span class=\"number\">3</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># tensor(3)</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>访问张量的长度</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">len</span>(x)</span><br><span class=\"line\"><span class=\"comment\"># 4</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>只有一个轴的张量，形状只有一个元素</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x.shape</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch.Size([4])</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"矩阵\"><a href=\"#矩阵\" class=\"headerlink\" title=\"矩阵\"></a>矩阵</h3><blockquote>\n<p>通过指定两个分量$m$和$n$来创建一个形状为$m \\times n$的<strong>矩阵</strong></p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A = torch.arange(<span class=\"number\">20</span>).reshape(<span class=\"number\">5</span>, <span class=\"number\">4</span>)</span><br><span class=\"line\">A</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([[ 0,  1,  2,  3],</span></span><br><span class=\"line\"><span class=\"string\">        [ 4,  5,  6,  7],</span></span><br><span class=\"line\"><span class=\"string\">        [ 8,  9, 10, 11],</span></span><br><span class=\"line\"><span class=\"string\">        [12, 13, 14, 15],</span></span><br><span class=\"line\"><span class=\"string\">        [16, 17, 18, 19]])</span></span><br><span class=\"line\"><span class=\"string\"> </span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>矩阵的转置</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A.T</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([[ 0,  4,  8, 12, 16],</span></span><br><span class=\"line\"><span class=\"string\">        [ 1,  5,  9, 13, 17],</span></span><br><span class=\"line\"><span class=\"string\">        [ 2,  6, 10, 14, 18],</span></span><br><span class=\"line\"><span class=\"string\">        [ 3,  7, 11, 15, 19]])</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p><em>对称矩阵</em>（symmetric matrix）$\\mathbf{A}$等于其转置：$\\mathbf{A} = \\mathbf{A}^\\top$</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">B = torch.tensor([[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>], [<span class=\"number\">2</span>, <span class=\"number\">0</span>, <span class=\"number\">4</span>], [<span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>]])</span><br><span class=\"line\">B</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([[1, 2, 3],</span></span><br><span class=\"line\"><span class=\"string\">        [2, 0, 4],</span></span><br><span class=\"line\"><span class=\"string\">        [3, 4, 5]])</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">B == B.T</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([[True, True, True],</span></span><br><span class=\"line\"><span class=\"string\">        [True, True, True],</span></span><br><span class=\"line\"><span class=\"string\">        [True, True, True]])</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>就像向量是标量的推广，矩阵是向量的推广一样，我们可以构建具有更多轴的数据结构</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = torch.arange(<span class=\"number\">24</span>).reshape(<span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>)</span><br><span class=\"line\">X</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([[[ 0,  1,  2,  3],</span></span><br><span class=\"line\"><span class=\"string\">         [ 4,  5,  6,  7],</span></span><br><span class=\"line\"><span class=\"string\">         [ 8,  9, 10, 11]],</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">        [[12, 13, 14, 15],</span></span><br><span class=\"line\"><span class=\"string\">         [16, 17, 18, 19],</span></span><br><span class=\"line\"><span class=\"string\">         [20, 21, 22, 23]]])</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>给定具有<strong>相同形状</strong>的任意两个张量，任何按元素<strong>二元运算</strong>的结果都将是相同形状的张量</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A = torch.arange(<span class=\"number\">20</span>, dtype=torch.float32).reshape(<span class=\"number\">5</span>, <span class=\"number\">4</span>)</span><br><span class=\"line\">B = A.clone()</span><br><span class=\"line\">A, A + B</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">(tensor([[ 0.,  1.,  2.,  3.],</span></span><br><span class=\"line\"><span class=\"string\">         [ 4.,  5.,  6.,  7.],</span></span><br><span class=\"line\"><span class=\"string\">         [ 8.,  9., 10., 11.],</span></span><br><span class=\"line\"><span class=\"string\">         [12., 13., 14., 15.],</span></span><br><span class=\"line\"><span class=\"string\">         [16., 17., 18., 19.]]),</span></span><br><span class=\"line\"><span class=\"string\"> tensor([[ 0.,  2.,  4.,  6.],</span></span><br><span class=\"line\"><span class=\"string\">         [ 8., 10., 12., 14.],</span></span><br><span class=\"line\"><span class=\"string\">         [16., 18., 20., 22.],</span></span><br><span class=\"line\"><span class=\"string\">         [24., 26., 28., 30.],</span></span><br><span class=\"line\"><span class=\"string\">         [32., 34., 36., 38.]]))</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>两个矩阵的按元素乘法称为<em>Hadamard积</em>（Hadamard product）（数学符号$\\odot$）</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A * B</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([[  0.,   1.,   4.,   9.],</span></span><br><span class=\"line\"><span class=\"string\">        [ 16.,  25.,  36.,  49.],</span></span><br><span class=\"line\"><span class=\"string\">        [ 64.,  81., 100., 121.],</span></span><br><span class=\"line\"><span class=\"string\">        [144., 169., 196., 225.],</span></span><br><span class=\"line\"><span class=\"string\">        [256., 289., 324., 361.]])</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a = <span class=\"number\">2</span></span><br><span class=\"line\">X = torch.arange(<span class=\"number\">24</span>).reshape(<span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>)</span><br><span class=\"line\">a + X, (a * X).shape</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">(tensor([[[ 2,  3,  4,  5],</span></span><br><span class=\"line\"><span class=\"string\">          [ 6,  7,  8,  9],</span></span><br><span class=\"line\"><span class=\"string\">          [10, 11, 12, 13]],</span></span><br><span class=\"line\"><span class=\"string\"> </span></span><br><span class=\"line\"><span class=\"string\">         [[14, 15, 16, 17],</span></span><br><span class=\"line\"><span class=\"string\">          [18, 19, 20, 21],</span></span><br><span class=\"line\"><span class=\"string\">          [22, 23, 24, 25]]]),</span></span><br><span class=\"line\"><span class=\"string\"> torch.Size([2, 3, 4]))</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>计算其元素的和</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.arange(<span class=\"number\">4</span>, dtype=torch.float32)</span><br><span class=\"line\">x, x.<span class=\"built_in\">sum</span>()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># (tensor([0., 1., 2., 3.]), tensor(6.))</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>表示任意形状张量的元素和</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A.shape, A.<span class=\"built_in\">sum</span>()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># (torch.Size([5, 4]), tensor(190.))</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"降维\"><a href=\"#降维\" class=\"headerlink\" title=\"降维\"></a>降维</h3><blockquote>\n<p>指定张量沿哪一个轴来通过求和降低维度</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A_sum_axis0 = A.<span class=\"built_in\">sum</span>(axis=<span class=\"number\">0</span>)</span><br><span class=\"line\"><span class=\"comment\"># 0轴上压缩，即压缩高度</span></span><br><span class=\"line\">A_sum_axis0, A_sum_axis0.shape</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># (tensor([40., 45., 50., 55.]), torch.Size([4]))</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A_sum_axis1 = A.<span class=\"built_in\">sum</span>(axis=<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"comment\"># 1轴上压缩，即压缩宽度</span></span><br><span class=\"line\">A_sum_axis1, A_sum_axis1.shape</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># (tensor([ 6., 22., 38., 54., 70.]), torch.Size([5]))</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A.<span class=\"built_in\">sum</span>(axis=[<span class=\"number\">0</span>, <span class=\"number\">1</span>])</span><br><span class=\"line\"><span class=\"comment\"># 同时在0轴和1轴上压缩，压缩高宽</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># tensor(190.)</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>一个与求和相关的量是<em>平均值</em>（mean或average）</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A.mean(), A.<span class=\"built_in\">sum</span>() / A.numel()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># (tensor(9.5000), tensor(9.5000))</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A.mean(axis=<span class=\"number\">0</span>), A.<span class=\"built_in\">sum</span>(axis=<span class=\"number\">0</span>) / A.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># (tensor([ 8.,  9., 10., 11.]), tensor([ 8.,  9., 10., 11.]))</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"非降维求和\"><a href=\"#非降维求和\" class=\"headerlink\" title=\"非降维求和\"></a>非降维求和</h3><blockquote>\n<p>计算总和或均值时<strong>保持轴数不变</strong></p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sum_A = A.<span class=\"built_in\">sum</span>(axis=<span class=\"number\">1</span>, keepdims=<span class=\"literal\">True</span>)</span><br><span class=\"line\">sum_A</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([[ 6.],</span></span><br><span class=\"line\"><span class=\"string\">        [22.],</span></span><br><span class=\"line\"><span class=\"string\">        [38.],</span></span><br><span class=\"line\"><span class=\"string\">        [54.],</span></span><br><span class=\"line\"><span class=\"string\">        [70.]])</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>通过广播将<code>A</code>除以<code>sum_A</code></p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A / sum_A</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([[0.0000, 0.1667, 0.3333, 0.5000],</span></span><br><span class=\"line\"><span class=\"string\">        [0.1818, 0.2273, 0.2727, 0.3182],</span></span><br><span class=\"line\"><span class=\"string\">        [0.2105, 0.2368, 0.2632, 0.2895],</span></span><br><span class=\"line\"><span class=\"string\">        [0.2222, 0.2407, 0.2593, 0.2778],</span></span><br><span class=\"line\"><span class=\"string\">        [0.2286, 0.2429, 0.2571, 0.2714]])</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>某个轴计算<code>A</code>元素的累积总和</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A.cumsum(axis=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([[ 0.,  1.,  2.,  3.],</span></span><br><span class=\"line\"><span class=\"string\">        [ 4.,  6.,  8., 10.],</span></span><br><span class=\"line\"><span class=\"string\">        [12., 15., 18., 21.],</span></span><br><span class=\"line\"><span class=\"string\">        [24., 28., 32., 36.],</span></span><br><span class=\"line\"><span class=\"string\">        [40., 45., 50., 55.]])</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"点积\"><a href=\"#点积\" class=\"headerlink\" title=\"点积\"></a>点积</h3><blockquote>\n<p>点积是<strong>相同位置的按元素乘积的和</strong></p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y = torch.ones(<span class=\"number\">4</span>, dtype = torch.float32)</span><br><span class=\"line\">x, y, torch.dot(x, y)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># (tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))</span></span><br></pre></td></tr></table></figure>\n<p>我们可以通过执行按元素乘法，然后进行求和来表示两个向量的点积</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.<span class=\"built_in\">sum</span>(x * y)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># tensor(6.)</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"向量积\"><a href=\"#向量积\" class=\"headerlink\" title=\"向量积\"></a>向量积</h3><blockquote>\n<p>矩阵向量积$\\mathbf{A}\\mathbf{x}$是一个长度为$m$的列向量，其第$i$个元素是点积$\\mathbf{a}^\\top_i \\mathbf{x}$</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A.shape, x.shape, torch.mv(A, x)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># (torch.Size([5, 4]), torch.Size([4]), tensor([ 14.,  38.,  62.,  86., 110.]))</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>我们可以将矩阵-矩阵乘法$\\mathbf{AB}$看作简单地<strong>执行$m$次矩阵-向量积</strong>，并将结果拼接在一起，形成一个$n \\times m$矩阵</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">B = torch.ones(<span class=\"number\">4</span>, <span class=\"number\">3</span>)</span><br><span class=\"line\">torch.mm(A, B)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">tensor([[ 6.,  6.,  6.],</span></span><br><span class=\"line\"><span class=\"string\">        [22., 22., 22.],</span></span><br><span class=\"line\"><span class=\"string\">        [38., 38., 38.],</span></span><br><span class=\"line\"><span class=\"string\">        [54., 54., 54.],</span></span><br><span class=\"line\"><span class=\"string\">        [70., 70., 70.]])</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"范数\"><a href=\"#范数\" class=\"headerlink\" title=\"范数\"></a>范数</h3><blockquote>\n<p>$L_2$<em>范数</em>是向量<strong>元素平方和的平方根</strong>：</p>\n<script type=\"math/tex; mode=display\">\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}</script></blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">u = torch.tensor([<span class=\"number\">3.0</span>, -<span class=\"number\">4.0</span>])</span><br><span class=\"line\">torch.norm(u)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># tensor(5.)</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>$L_1$范数，它表示为<strong>向量元素的绝对值之和</strong>：</p>\n<script type=\"math/tex; mode=display\">\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^n \\left|x_i \\right|</script></blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.<span class=\"built_in\">abs</span>(u).<span class=\"built_in\">sum</span>()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># tensor(7.)</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>矩阵的<em>Frobenius（弗罗贝尼乌斯）范数</em>（Frobenius norm）是<strong>矩阵元素平方和的平方根</strong>：<script type=\"math/tex\">\\|\\mathbf{X}\\|_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n x_{ij}^2}</script></p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.norm(torch.ones((<span class=\"number\">4</span>, <span class=\"number\">9</span>))) <span class=\"comment\"># 36开方</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># tensor(6.)</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"微积分\"><a href=\"#微积分\" class=\"headerlink\" title=\"微积分\"></a>微积分</h2><blockquote>\n<p>如果$f$的<em>导数</em>存在，这个极限被定义为<script type=\"math/tex\">f'(x) = \\lim_{h \\rightarrow 0} \\frac{f(x+h) - f(x)}{h}</script>，定义$u=f(x)=3x^2-4x$</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">%matplotlib inline</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> matplotlib_inline <span class=\"keyword\">import</span> backend_inline</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">f</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">3</span> * x ** <span class=\"number\">2</span> - <span class=\"number\">4</span> * x</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>通过令$x=1$并让$h$接近$0$，$\\frac{f(x+h)-f(x)}{h}$的数值结果接近$2$</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">numerical_lim</span>(<span class=\"params\">f, x, h</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> (f(x + h) - f(x)) / h</span><br><span class=\"line\"></span><br><span class=\"line\">h = <span class=\"number\">0.1</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">5</span>):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;h=<span class=\"subst\">&#123;h:<span class=\"number\">.5</span>f&#125;</span>, numerical limit=<span class=\"subst\">&#123;numerical_lim(f, <span class=\"number\">1</span>, h):<span class=\"number\">.5</span>f&#125;</span>&#x27;</span>)</span><br><span class=\"line\">    h *= <span class=\"number\">0.1</span></span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">h=0.10000, numerical limit=2.30000</span></span><br><span class=\"line\"><span class=\"string\">h=0.01000, numerical limit=2.03000</span></span><br><span class=\"line\"><span class=\"string\">h=0.00100, numerical limit=2.00300</span></span><br><span class=\"line\"><span class=\"string\">h=0.00010, numerical limit=2.00030</span></span><br><span class=\"line\"><span class=\"string\">h=0.00001, numerical limit=2.00003</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>为了对导数的这种解释进行可视化，我们将使用<code>matplotlib</code>定义几个函数</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">use_svg_display</span>():  </span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;使用svg格式在Jupyter中显示绘图&quot;&quot;&quot;</span></span><br><span class=\"line\">    backend_inline.set_matplotlib_formats(<span class=\"string\">&#x27;svg&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">set_figsize</span>(<span class=\"params\">figsize=(<span class=\"params\"><span class=\"number\">3.5</span>, <span class=\"number\">2.5</span></span>)</span>):  </span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;设置matplotlib的图表大小&quot;&quot;&quot;</span></span><br><span class=\"line\">    use_svg_display()</span><br><span class=\"line\">    d2l.plt.rcParams[<span class=\"string\">&#x27;figure.figsize&#x27;</span>] = figsize</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">set_axes</span>(<span class=\"params\">axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;设置matplotlib的轴&quot;&quot;&quot;</span></span><br><span class=\"line\">    axes.set_xlabel(xlabel)</span><br><span class=\"line\">    axes.set_ylabel(ylabel)</span><br><span class=\"line\">    axes.set_xscale(xscale)</span><br><span class=\"line\">    axes.set_yscale(yscale)</span><br><span class=\"line\">    axes.set_xlim(xlim)</span><br><span class=\"line\">    axes.set_ylim(ylim)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> legend:</span><br><span class=\"line\">        axes.legend(legend)</span><br><span class=\"line\">    axes.grid()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">plot</span>(<span class=\"params\">X, Y=<span class=\"literal\">None</span>, xlabel=<span class=\"literal\">None</span>, ylabel=<span class=\"literal\">None</span>, legend=<span class=\"literal\">None</span>, xlim=<span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">         ylim=<span class=\"literal\">None</span>, xscale=<span class=\"string\">&#x27;linear&#x27;</span>, yscale=<span class=\"string\">&#x27;linear&#x27;</span>,</span></span><br><span class=\"line\"><span class=\"params\">         fmts=(<span class=\"params\"><span class=\"string\">&#x27;-&#x27;</span>, <span class=\"string\">&#x27;m--&#x27;</span>, <span class=\"string\">&#x27;g-.&#x27;</span>, <span class=\"string\">&#x27;r:&#x27;</span></span>), figsize=(<span class=\"params\"><span class=\"number\">3.5</span>, <span class=\"number\">2.5</span></span>), axes=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;绘制数据点&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> legend <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        legend = []</span><br><span class=\"line\"></span><br><span class=\"line\">    set_figsize(figsize)</span><br><span class=\"line\">    axes = axes <span class=\"keyword\">if</span> axes <span class=\"keyword\">else</span> d2l.plt.gca()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">has_one_axis</span>(<span class=\"params\">X</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> (<span class=\"built_in\">hasattr</span>(X, <span class=\"string\">&quot;ndim&quot;</span>) <span class=\"keyword\">and</span> X.ndim == <span class=\"number\">1</span> <span class=\"keyword\">or</span> <span class=\"built_in\">isinstance</span>(X, <span class=\"built_in\">list</span>)</span><br><span class=\"line\">                <span class=\"keyword\">and</span> <span class=\"keyword\">not</span> <span class=\"built_in\">hasattr</span>(X[<span class=\"number\">0</span>], <span class=\"string\">&quot;__len__&quot;</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> has_one_axis(X):</span><br><span class=\"line\">        X = [X]</span><br><span class=\"line\">    <span class=\"keyword\">if</span> Y <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        X, Y = [[]] * <span class=\"built_in\">len</span>(X), X</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> has_one_axis(Y):</span><br><span class=\"line\">        Y = [Y]</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(X) != <span class=\"built_in\">len</span>(Y):</span><br><span class=\"line\">        X = X * <span class=\"built_in\">len</span>(Y)</span><br><span class=\"line\">    axes.cla()</span><br><span class=\"line\">    <span class=\"keyword\">for</span> x, y, fmt <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(X, Y, fmts):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(x):</span><br><span class=\"line\">            axes.plot(x, y, fmt)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            axes.plot(y, fmt)</span><br><span class=\"line\">    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>绘制函数$u=f(x)$及其在$x=1$处的切线$y=2x-3$</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = np.arange(<span class=\"number\">0</span>, <span class=\"number\">3</span>, <span class=\"number\">0.1</span>)</span><br><span class=\"line\">plot(x, [f(x), <span class=\"number\">2</span> * x - <span class=\"number\">3</span>], <span class=\"string\">&#x27;x&#x27;</span>, <span class=\"string\">&#x27;f(x)&#x27;</span>, legend=[<span class=\"string\">&#x27;f(x)&#x27;</span>, <span class=\"string\">&#x27;Tangent line (x=1)&#x27;</span>])</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2023/03/22/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/DataWhale/D2L_ch02_%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/output_7_0-16794727634302.svg\" alt=\"svg\"></p>\n<h2 id=\"自动微分\"><a href=\"#自动微分\" class=\"headerlink\" title=\"自动微分\"></a>自动微分</h2><blockquote>\n<p>假设我们想对函数$y=2\\mathbf{x}^{\\top}\\mathbf{x}$关于列向量$\\mathbf{x}$求导</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"></span><br><span class=\"line\">x = torch.arange(<span class=\"number\">4.0</span>)</span><br><span class=\"line\">x</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;tensor([0., 1., 2., 3.])&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>在我们计算$y$关于$\\mathbf{x}$的梯度之前，需要一个地方来存储梯度</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x.requires_grad_(<span class=\"literal\">True</span>)</span><br><span class=\"line\">x.grad</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>现在计算$y$</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y = <span class=\"number\">2</span> * torch.dot(x, x)</span><br><span class=\"line\">y</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;tensor(28., grad_fn=&lt;MulBackward0&gt;)&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>通过调用反向传播函数来自动计算<code>y</code>关于<code>x</code>每个分量的梯度</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y.backward()</span><br><span class=\"line\">x.grad</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;tensor([ 0.,  4.,  8., 12.])&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x.grad == <span class=\"number\">4</span> * x</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;tensor([True, True, True, True])&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>现在计算<code>x</code>的另一个函数</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x.grad.zero_()</span><br><span class=\"line\">y = x.<span class=\"built_in\">sum</span>()</span><br><span class=\"line\">y.backward()</span><br><span class=\"line\">x.grad</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;tensor([1., 1., 1., 1.])&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>深度学习中，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x.grad.zero_()</span><br><span class=\"line\">y = x * x</span><br><span class=\"line\">y.<span class=\"built_in\">sum</span>().backward()</span><br><span class=\"line\">x.grad</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;tensor([0., 2., 4., 6.])&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>将某些计算移动到记录的计算图之外</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x.grad.zero_()</span><br><span class=\"line\">y = x * x</span><br><span class=\"line\">u = y.detach()</span><br><span class=\"line\">z = u * x</span><br><span class=\"line\"></span><br><span class=\"line\">z.<span class=\"built_in\">sum</span>().backward()</span><br><span class=\"line\">x.grad == u</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;tensor([True, True, True, True])&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x.grad.zero_()</span><br><span class=\"line\">y.<span class=\"built_in\">sum</span>().backward()</span><br><span class=\"line\">x.grad == <span class=\"number\">2</span> * x</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;tensor([True, True, True, True])&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">f</span>(<span class=\"params\">a</span>):</span><br><span class=\"line\">    b = a * <span class=\"number\">2</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> b.norm() &lt; <span class=\"number\">1000</span>:</span><br><span class=\"line\">        b = b * <span class=\"number\">2</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> b.<span class=\"built_in\">sum</span>() &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">        c = b</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        c = <span class=\"number\">100</span> * b</span><br><span class=\"line\">    <span class=\"keyword\">return</span> c</span><br><span class=\"line\"></span><br><span class=\"line\">a = torch.randn(size=(), requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">d = f(a)</span><br><span class=\"line\">d.backward()</span><br><span class=\"line\"></span><br><span class=\"line\">a.grad == d / a</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;tensor(True)&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"cover":"/img/covers/2.jpg","cover_type":"img","excerpt":"","more":"<h2 id=\"数据操作\"><a href=\"#数据操作\" class=\"headerlink\" title=\"数据操作\"></a>数据操作</h2><p>首先，我们导入<code>torch</code>。请注意，虽然它被称为<code>PyTorch</code>，但是代码中使用<code>torch</code>而不是<code>pytorch</code></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch </span><br></pre></td></tr></table></figure>\n<h3 id=\"张量：\"><a href=\"#张量：\" class=\"headerlink\" title=\"张量：\"></a><strong>张量</strong>：</h3><blockquote>\n<p>表示一个由数值组成的数组，这个数组可能有多个维度</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.arange(<span class=\"number\">12</span>)</span><br><span class=\"line\">x</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 结果为：tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"shape属性：\"><a href=\"#shape属性：\" class=\"headerlink\" title=\"shape属性：\"></a><strong><code>shape</code>属性：</strong></h3><blockquote>\n<p>访问张量（沿每个轴的长度）的<em>形状</em></p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x.shape  <span class=\"comment\"># 获取张量的形状</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 结果为： torch.Size([12])</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"numel函数：\"><a href=\"#numel函数：\" class=\"headerlink\" title=\"numel函数：\"></a><strong><code>numel</code>函数：</strong></h3><blockquote>\n<p>获取张量中元素的总数</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x.numel() <span class=\"comment\"># 获取张量中元素的总数</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 结果为： 12</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"reshape函数：\"><a href=\"#reshape函数：\" class=\"headerlink\" title=\"reshape函数：\"></a><strong><code>reshape</code>函数：</strong></h3><blockquote>\n<p>改变一个张量的形状而不改变元素数量和元素值</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = x.reshape(<span class=\"number\">3</span>, <span class=\"number\">4</span>) <span class=\"comment\"># 不改变元素数量和元素值的前提下，改变张量的形状。</span></span><br><span class=\"line\">X</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([[ 0,  1,  2,  3],</span></span><br><span class=\"line\"><span class=\"string\">        [ 4,  5,  6,  7],</span></span><br><span class=\"line\"><span class=\"string\">        [ 8,  9, 10, 11]])</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"zeros-a-b-c-函数：\"><a href=\"#zeros-a-b-c-函数：\" class=\"headerlink\" title=\"zeros((a,b,c...))函数：\"></a><strong><code>zeros((a,b,c...))</code>函数：</strong></h3><blockquote>\n<p>构造全0张量</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.zeros((<span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([[[0., 0., 0., 0.],</span></span><br><span class=\"line\"><span class=\"string\">         [0., 0., 0., 0.],</span></span><br><span class=\"line\"><span class=\"string\">         [0., 0., 0., 0.]],</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">        [[0., 0., 0., 0.],</span></span><br><span class=\"line\"><span class=\"string\">         [0., 0., 0., 0.],</span></span><br><span class=\"line\"><span class=\"string\">         [0., 0., 0., 0.]]])</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"ones-a-b-c-函数：\"><a href=\"#ones-a-b-c-函数：\" class=\"headerlink\" title=\"ones((a,b,c...))函数：\"></a><code>ones((a,b,c...))</code>函数：</h3><blockquote>\n<p>构造全1张量</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.ones((<span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>)) </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([[[1., 1., 1., 1.],</span></span><br><span class=\"line\"><span class=\"string\">         [1., 1., 1., 1.],</span></span><br><span class=\"line\"><span class=\"string\">         [1., 1., 1., 1.]],</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">        [[1., 1., 1., 1.],</span></span><br><span class=\"line\"><span class=\"string\">         [1., 1., 1., 1.],</span></span><br><span class=\"line\"><span class=\"string\">         [1., 1., 1., 1.]]])</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"randn-a-b-c-函数：\"><a href=\"#randn-a-b-c-函数：\" class=\"headerlink\" title=\"randn(a,b,c...)函数：\"></a><strong><code>randn(a,b,c...)</code>函数</strong>：</h3><blockquote>\n<p>从标准高斯（正态）分布中随机采样</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.randn(<span class=\"number\">3</span>, <span class=\"number\">4</span>) </span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([[-0.8709,  0.4194, -0.4292,  0.1850],</span></span><br><span class=\"line\"><span class=\"string\">        [-0.3330, -0.6349, -0.1422, -1.0355],</span></span><br><span class=\"line\"><span class=\"string\">        [-0.7531,  0.5163,  2.4913,  0.3060]])</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>通过提供包含数值的Python列表（或嵌套列表），来为所需张量中的每个元素赋予确定值</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.tensor([[<span class=\"number\">2</span>, <span class=\"number\">1</span>, <span class=\"number\">4</span>, <span class=\"number\">3</span>], [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>], [<span class=\"number\">4</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>]])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([[2, 1, 4, 3],</span></span><br><span class=\"line\"><span class=\"string\">        [1, 2, 3, 4],</span></span><br><span class=\"line\"><span class=\"string\">        [4, 3, 2, 1]])</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"标准算术运算符-（-、-、-、-和）-：\"><a href=\"#标准算术运算符-（-、-、-、-和）-：\" class=\"headerlink\" title=\"标准算术运算符`（+、-、*、/和）`**：\"></a><strong>标准算术运算符`（+、-、*、/和</strong>）`**：</h3><blockquote>\n<p>都可以被升级为按元素运算</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.tensor([<span class=\"number\">1.0</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">8</span>])</span><br><span class=\"line\">y = torch.tensor([<span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>])</span><br><span class=\"line\">x + y, x - y, x * y, x / y, x ** y</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">(tensor([ 3.,  4.,  6., 10.]),</span></span><br><span class=\"line\"><span class=\"string\"> tensor([-1.,  0.,  2.,  6.]),</span></span><br><span class=\"line\"><span class=\"string\"> tensor([ 2.,  4.,  8., 16.]),</span></span><br><span class=\"line\"><span class=\"string\"> tensor([0.5000, 1.0000, 2.0000, 4.0000]),</span></span><br><span class=\"line\"><span class=\"string\"> tensor([ 1.,  4., 16., 64.]))</span></span><br><span class=\"line\"><span class=\"string\"> &#x27;&#x27;&#x27;</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"求幂运算\"><a href=\"#求幂运算\" class=\"headerlink\" title=\"求幂运算\"></a><strong>求幂运算</strong></h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.exp(x)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"张量连结（concatenate）：\"><a href=\"#张量连结（concatenate）：\" class=\"headerlink\" title=\"张量连结（concatenate）：\"></a>张量连结（concatenate）：</h3><blockquote>\n<p>我们也可以把多个张量<em>连结</em>（concatenate）在一起</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = torch.arange(<span class=\"number\">12</span>, dtype=torch.float32).reshape((<span class=\"number\">3</span>,<span class=\"number\">4</span>))</span><br><span class=\"line\"><span class=\"comment\"># 定义一个12个元素的以为数组X，元素类型为浮点型，改变其形状为：（3,4）</span></span><br><span class=\"line\"></span><br><span class=\"line\">Y = torch.tensor([[<span class=\"number\">2.0</span>, <span class=\"number\">1</span>, <span class=\"number\">4</span>, <span class=\"number\">3</span>], [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>], [<span class=\"number\">4</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>]])</span><br><span class=\"line\"><span class=\"comment\"># 定义一个张量Y,并给元素赋值</span></span><br><span class=\"line\"></span><br><span class=\"line\">torch.cat((X, Y), dim=<span class=\"number\">0</span>), torch.cat((X, Y), dim=<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"comment\"># 将X和Y在0轴上连接合并并输出（0轴为高度方向）； 将X和Y在1轴上合并并输出（1轴为宽度方向）</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">(tensor([[ 0.,  1.,  2.,  3.],</span></span><br><span class=\"line\"><span class=\"string\">         [ 4.,  5.,  6.,  7.],</span></span><br><span class=\"line\"><span class=\"string\">         [ 8.,  9., 10., 11.],</span></span><br><span class=\"line\"><span class=\"string\">         [ 2.,  1.,  4.,  3.],</span></span><br><span class=\"line\"><span class=\"string\">         [ 1.,  2.,  3.,  4.],</span></span><br><span class=\"line\"><span class=\"string\">         [ 4.,  3.,  2.,  1.]]),</span></span><br><span class=\"line\"><span class=\"string\"> tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],</span></span><br><span class=\"line\"><span class=\"string\">         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],</span></span><br><span class=\"line\"><span class=\"string\">         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"逻辑运算符\"><a href=\"#逻辑运算符\" class=\"headerlink\" title=\"逻辑运算符\"></a>逻辑运算符</h3><blockquote>\n<p>使用<strong><em>逻辑运算符</em></strong>构建二元张量，<code>True</code> or <code>False</code></p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X &gt; Y</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([[False, False, False, False],</span></span><br><span class=\"line\"><span class=\"string\">        [ True,  True,  True,  True],</span></span><br><span class=\"line\"><span class=\"string\">        [ True,  True,  True,  True]])</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>对张量中的所有元素进行求和，会产生一个单元素张量</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X.<span class=\"built_in\">sum</span>()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 结果为：tensor(66.)</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"广播机制：\"><a href=\"#广播机制：\" class=\"headerlink\" title=\"广播机制：\"></a>广播机制：</h3><blockquote>\n<p>即使形状不同，我们仍然可以通过调用<strong><em>广播机制</em>（broadcasting mechanism）</strong>来执行按元素操作。通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状。</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a = torch.arange(<span class=\"number\">9</span>).reshape((<span class=\"number\">3</span>, <span class=\"number\">1</span>,-<span class=\"number\">1</span>))   <span class=\"comment\"># &quot;-1&quot;表示该轴会自动计算长度</span></span><br><span class=\"line\"><span class=\"comment\"># 定义一个包含（0-8）的张量a，形状为（3,1,3）,由最外层往内数</span></span><br><span class=\"line\"></span><br><span class=\"line\">b = torch.arange(<span class=\"number\">3</span>).reshape((<span class=\"number\">1</span>, <span class=\"number\">3</span>,-<span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"comment\"># 定义一个包含（0-2）的张量b，形状为（1,3,1）</span></span><br><span class=\"line\"></span><br><span class=\"line\">a, b</span><br><span class=\"line\"><span class=\"comment\"># 输出张量a，输出张量b</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">(tensor([[[0, 1, 2]],</span></span><br><span class=\"line\"><span class=\"string\"> </span></span><br><span class=\"line\"><span class=\"string\">         [[3, 4, 5]],</span></span><br><span class=\"line\"><span class=\"string\"> </span></span><br><span class=\"line\"><span class=\"string\">         [[6, 7, 8]]]),</span></span><br><span class=\"line\"><span class=\"string\"> tensor([[[0],</span></span><br><span class=\"line\"><span class=\"string\">          [1],</span></span><br><span class=\"line\"><span class=\"string\">          [2]]]))</span></span><br><span class=\"line\"><span class=\"string\">   &#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>不同形状的张量相加：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a + b</span><br><span class=\"line\"><span class=\"comment\"># 由于a的形状为(3,1,3)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([[[ 0,  1,  2],</span></span><br><span class=\"line\"><span class=\"string\">         [ 1,  2,  3],</span></span><br><span class=\"line\"><span class=\"string\">         [ 2,  3,  4]],</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">        [[ 3,  4,  5],</span></span><br><span class=\"line\"><span class=\"string\">         [ 4,  5,  6],</span></span><br><span class=\"line\"><span class=\"string\">         [ 5,  6,  7]],</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">        [[ 6,  7,  8],</span></span><br><span class=\"line\"><span class=\"string\">         [ 7,  8,  9],</span></span><br><span class=\"line\"><span class=\"string\">         [ 8,  9, 10]]])</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"切片：\"><a href=\"#切片：\" class=\"headerlink\" title=\"切片：\"></a><strong>切片</strong>：</h3><blockquote>\n<p>可以用<code>[-1]</code>选择最后一个元素，可以用<code>[1:3]</code>选择第二个和第三个元素</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X[-<span class=\"number\">1</span>], X[<span class=\"number\">1</span>:<span class=\"number\">3</span>]</span><br><span class=\"line\"><span class=\"comment\"># 输出X中的最后一行元素，输出X中的第二行和第三行元素</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">(tensor([ 8.,  9., 10., 11.]),</span></span><br><span class=\"line\"><span class=\"string\"> tensor([[ 4.,  5.,  6.,  7.],</span></span><br><span class=\"line\"><span class=\"string\">         [ 8.,  9., 10., 11.]]))</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"通过索引修改元素的值\"><a href=\"#通过索引修改元素的值\" class=\"headerlink\" title=\"通过索引修改元素的值\"></a><strong>通过索引修改元素的值</strong></h3><blockquote>\n<p>除读取外，我们还可以通过指定索引来将元素写入矩阵</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X[<span class=\"number\">1</span>, <span class=\"number\">2</span>] = <span class=\"number\">9</span>          <span class=\"comment\"># 将X中第二行第三列的元素值改为9</span></span><br><span class=\"line\"></span><br><span class=\"line\">X                    <span class=\"comment\"># 输出X</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([[ 0.,  1.,  2.,  3.],</span></span><br><span class=\"line\"><span class=\"string\">        [ 4.,  5.,  9.,  7.],</span></span><br><span class=\"line\"><span class=\"string\">        [ 8.,  9., 10., 11.]])</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"通过切片赋值：\"><a href=\"#通过切片赋值：\" class=\"headerlink\" title=\"通过切片赋值：\"></a><strong>通过切片赋值</strong>：</h3><blockquote>\n<p>为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X[<span class=\"number\">0</span>:<span class=\"number\">2</span>, :] = <span class=\"number\">12</span> </span><br><span class=\"line\"><span class=\"comment\"># 将12赋值给X中的第一行和第二行的每一个元素</span></span><br><span class=\"line\"></span><br><span class=\"line\">X  <span class=\"comment\"># 输出X</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([[12., 12., 12., 12.],</span></span><br><span class=\"line\"><span class=\"string\">        [12., 12., 12., 12.],</span></span><br><span class=\"line\"><span class=\"string\">        [ 8.,  9., 10., 11.]])</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"内存分配问题：\"><a href=\"#内存分配问题：\" class=\"headerlink\" title=\"内存分配问题：\"></a><strong>内存分配问题：</strong></h3><p>运行一些操作可能会导致为新结果分配内存</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">before = <span class=\"built_in\">id</span>(Y)</span><br><span class=\"line\"><span class=\"comment\"># 将Y的id赋值给before变量</span></span><br><span class=\"line\"></span><br><span class=\"line\">Y = Y + X</span><br><span class=\"line\"><span class=\"comment\"># 执行矩阵相加</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">id</span>(Y) == before</span><br><span class=\"line\"><span class=\"comment\"># 将新的Y的id和之前的Y的id进行逻辑比较，并输出逻辑运算结果</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 结果为：False</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>执行原地操作</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Z = torch.zeros_like(Y)</span><br><span class=\"line\"><span class=\"comment\"># 生成一个新的全0矩阵：Z，使得Z的形状与Y一样</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;id(Z):&#x27;</span>, <span class=\"built_in\">id</span>(Z))</span><br><span class=\"line\"><span class=\"comment\"># 打印Z的id</span></span><br><span class=\"line\"></span><br><span class=\"line\">Z[:] = X + Y</span><br><span class=\"line\"><span class=\"comment\"># 执行原地操作，Z的id不变</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;id(Z):&#x27;</span>, <span class=\"built_in\">id</span>(Z))</span><br><span class=\"line\"><span class=\"comment\"># 打印Z的id</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">id(Z): 139931132035296</span></span><br><span class=\"line\"><span class=\"string\">id(Z): 139931132035296</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>如果在后续计算中没有重复使用<code>X</code>，我们也可以使用<code>X[:] = X + Y</code>或<code>X += Y</code>来减少操作的内存开销</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">before = <span class=\"built_in\">id</span>(X)</span><br><span class=\"line\">X += Y</span><br><span class=\"line\"><span class=\"built_in\">id</span>(X) == before</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">True</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"转换为NumPy张量（ndarray）\"><a href=\"#转换为NumPy张量（ndarray）\" class=\"headerlink\" title=\"转换为NumPy张量（ndarray）\"></a>转换为<code>NumPy</code>张量（<code>ndarray</code>）</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A = X.numpy()</span><br><span class=\"line\"><span class=\"comment\"># 将X转换成Numpy数组，并赋值给A</span></span><br><span class=\"line\"></span><br><span class=\"line\">B = torch.tensor(A)</span><br><span class=\"line\"><span class=\"comment\"># 将A转变成张量，并赋值给B</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">type</span>(A), <span class=\"built_in\">type</span>(B)</span><br><span class=\"line\"><span class=\"comment\"># 输出A的数据类型，输出B的数据类型</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">(numpy.ndarray, torch.Tensor)</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>将大小为1的张量转换为Python标量</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a = torch.tensor([<span class=\"number\">3.5</span>])</span><br><span class=\"line\"><span class=\"comment\"># 定义一个大小为1，元素为3.5的张量a</span></span><br><span class=\"line\">a, a.item(), <span class=\"built_in\">float</span>(a), <span class=\"built_in\">int</span>(a)</span><br><span class=\"line\">输出a，标量a，浮点数a，整数a的值</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">(tensor([3.5000]), 3.5, 3.5, 3)</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"数据预处理\"><a href=\"#数据预处理\" class=\"headerlink\" title=\"数据预处理\"></a>数据预处理</h2><h3 id=\"CSV（逗号分隔值）文件操作\"><a href=\"#CSV（逗号分隔值）文件操作\" class=\"headerlink\" title=\"CSV（逗号分隔值）文件操作\"></a>CSV（逗号分隔值）文件操作</h3><blockquote>\n<p>创建一个人工数据集，并存储在CSV（逗号分隔值）文件</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"></span><br><span class=\"line\">os.makedirs(os.path.join(<span class=\"string\">&#x27;..&#x27;</span>, <span class=\"string\">&#x27;data&#x27;</span>), exist_ok=<span class=\"literal\">True</span>)</span><br><span class=\"line\">data_file = os.path.join(<span class=\"string\">&#x27;..&#x27;</span>, <span class=\"string\">&#x27;data&#x27;</span>, <span class=\"string\">&#x27;house_tiny.csv&#x27;</span>)</span><br><span class=\"line\"><span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(data_file, <span class=\"string\">&#x27;w&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">    f.write(<span class=\"string\">&#x27;NumRooms,Alley,Price\\n&#x27;</span>)</span><br><span class=\"line\">    f.write(<span class=\"string\">&#x27;NA,Pave,127500\\n&#x27;</span>)</span><br><span class=\"line\">    f.write(<span class=\"string\">&#x27;2,NA,106000\\n&#x27;</span>)</span><br><span class=\"line\">    f.write(<span class=\"string\">&#x27;4,NA,178100\\n&#x27;</span>)</span><br><span class=\"line\">    f.write(<span class=\"string\">&#x27;NA,NA,140000\\n&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>从创建的CSV文件中加载原始数据集</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"></span><br><span class=\"line\">data = pd.read_csv(data_file)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(data)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">     NumRooms Alley   Price</span></span><br><span class=\"line\"><span class=\"string\">      NaN    Pave     127500</span></span><br><span class=\"line\"><span class=\"string\">      2.0     NaN     106000</span></span><br><span class=\"line\"><span class=\"string\">      4.0     NaN     178100</span></span><br><span class=\"line\"><span class=\"string\">      NaN     NaN     140000</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>为了处理缺失的数据，典型的方法包括<strong><em>插值法</em></strong>和<strong><em>删除法</em></strong>，这里，我们将考虑<strong>插值法</strong></p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inputs, outputs = data.iloc[:, <span class=\"number\">0</span>:<span class=\"number\">2</span>], data.iloc[:, <span class=\"number\">2</span>]</span><br><span class=\"line\"><span class=\"comment\"># iloc是csv的索引方法</span></span><br><span class=\"line\">inputs = inputs.fillna(inputs.mean())</span><br><span class=\"line\"><span class=\"comment\"># 用同一列的均值替换NaN</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(inputs)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">     NumRooms Alley</span></span><br><span class=\"line\"><span class=\"string\">       3.0    Pave</span></span><br><span class=\"line\"><span class=\"string\">       2.0    NaN</span></span><br><span class=\"line\"><span class=\"string\">       4.0    NaN</span></span><br><span class=\"line\"><span class=\"string\">       3.0    NaN</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>对于<code>inputs</code>中的类别值或离散值，我们将<code>“NaN”</code>视为一个类别</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inputs = pd.get_dummies(inputs, dummy_na=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"comment\">#转换成pandas表格？？？</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(inputs)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">       NumRooms  Alley_Pave  Alley_nan</span></span><br><span class=\"line\"><span class=\"string\">       3.0           1          0</span></span><br><span class=\"line\"><span class=\"string\">       2.0           0          1</span></span><br><span class=\"line\"><span class=\"string\">       4.0           0          1</span></span><br><span class=\"line\"><span class=\"string\">       3.0           0          1</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>现在<code>inputs</code>和<code>outputs</code>中的所有条目都是数值类型，它们可以转换为张量格式</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"></span><br><span class=\"line\">X, y = torch.tensor(inputs.values), torch.tensor(outputs.values)</span><br><span class=\"line\"><span class=\"comment\"># 将表格数据拆分，均转换为张量</span></span><br><span class=\"line\">X, y</span><br><span class=\"line\"></span><br><span class=\"line\">结果为：</span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">(tensor([[3., 1., 0.],</span></span><br><span class=\"line\"><span class=\"string\">         [2., 0., 1.],</span></span><br><span class=\"line\"><span class=\"string\">         [4., 0., 1.],</span></span><br><span class=\"line\"><span class=\"string\">         [3., 0., 1.]], dtype=torch.float64),</span></span><br><span class=\"line\"><span class=\"string\"> tensor([127500, 106000, 178100, 140000]))</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"线性代数\"><a href=\"#线性代数\" class=\"headerlink\" title=\"线性代数\"></a>线性代数</h2><h3 id=\"标量与张量\"><a href=\"#标量与张量\" class=\"headerlink\" title=\"标量与张量\"></a>标量与张量</h3><blockquote>\n<p>标量由只有一个元素的张量表示</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"></span><br><span class=\"line\">x = torch.tensor(<span class=\"number\">3.0</span>)</span><br><span class=\"line\">y = torch.tensor(<span class=\"number\">2.0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">x + y, x * y, x / y, x**y</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># (tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>向量可以被视为标量值组成的列表</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.arange(<span class=\"number\">4</span>)</span><br><span class=\"line\">x</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># tensor([0, 1, 2, 3])</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>通过张量的索引来访问任一元素</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x[<span class=\"number\">3</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># tensor(3)</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>访问张量的长度</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">len</span>(x)</span><br><span class=\"line\"><span class=\"comment\"># 4</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>只有一个轴的张量，形状只有一个元素</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x.shape</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># torch.Size([4])</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"矩阵\"><a href=\"#矩阵\" class=\"headerlink\" title=\"矩阵\"></a>矩阵</h3><blockquote>\n<p>通过指定两个分量$m$和$n$来创建一个形状为$m \\times n$的<strong>矩阵</strong></p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A = torch.arange(<span class=\"number\">20</span>).reshape(<span class=\"number\">5</span>, <span class=\"number\">4</span>)</span><br><span class=\"line\">A</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([[ 0,  1,  2,  3],</span></span><br><span class=\"line\"><span class=\"string\">        [ 4,  5,  6,  7],</span></span><br><span class=\"line\"><span class=\"string\">        [ 8,  9, 10, 11],</span></span><br><span class=\"line\"><span class=\"string\">        [12, 13, 14, 15],</span></span><br><span class=\"line\"><span class=\"string\">        [16, 17, 18, 19]])</span></span><br><span class=\"line\"><span class=\"string\"> </span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>矩阵的转置</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A.T</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([[ 0,  4,  8, 12, 16],</span></span><br><span class=\"line\"><span class=\"string\">        [ 1,  5,  9, 13, 17],</span></span><br><span class=\"line\"><span class=\"string\">        [ 2,  6, 10, 14, 18],</span></span><br><span class=\"line\"><span class=\"string\">        [ 3,  7, 11, 15, 19]])</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p><em>对称矩阵</em>（symmetric matrix）$\\mathbf{A}$等于其转置：$\\mathbf{A} = \\mathbf{A}^\\top$</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">B = torch.tensor([[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>], [<span class=\"number\">2</span>, <span class=\"number\">0</span>, <span class=\"number\">4</span>], [<span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>]])</span><br><span class=\"line\">B</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([[1, 2, 3],</span></span><br><span class=\"line\"><span class=\"string\">        [2, 0, 4],</span></span><br><span class=\"line\"><span class=\"string\">        [3, 4, 5]])</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">B == B.T</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([[True, True, True],</span></span><br><span class=\"line\"><span class=\"string\">        [True, True, True],</span></span><br><span class=\"line\"><span class=\"string\">        [True, True, True]])</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>就像向量是标量的推广，矩阵是向量的推广一样，我们可以构建具有更多轴的数据结构</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = torch.arange(<span class=\"number\">24</span>).reshape(<span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>)</span><br><span class=\"line\">X</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([[[ 0,  1,  2,  3],</span></span><br><span class=\"line\"><span class=\"string\">         [ 4,  5,  6,  7],</span></span><br><span class=\"line\"><span class=\"string\">         [ 8,  9, 10, 11]],</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">        [[12, 13, 14, 15],</span></span><br><span class=\"line\"><span class=\"string\">         [16, 17, 18, 19],</span></span><br><span class=\"line\"><span class=\"string\">         [20, 21, 22, 23]]])</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>给定具有<strong>相同形状</strong>的任意两个张量，任何按元素<strong>二元运算</strong>的结果都将是相同形状的张量</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A = torch.arange(<span class=\"number\">20</span>, dtype=torch.float32).reshape(<span class=\"number\">5</span>, <span class=\"number\">4</span>)</span><br><span class=\"line\">B = A.clone()</span><br><span class=\"line\">A, A + B</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">(tensor([[ 0.,  1.,  2.,  3.],</span></span><br><span class=\"line\"><span class=\"string\">         [ 4.,  5.,  6.,  7.],</span></span><br><span class=\"line\"><span class=\"string\">         [ 8.,  9., 10., 11.],</span></span><br><span class=\"line\"><span class=\"string\">         [12., 13., 14., 15.],</span></span><br><span class=\"line\"><span class=\"string\">         [16., 17., 18., 19.]]),</span></span><br><span class=\"line\"><span class=\"string\"> tensor([[ 0.,  2.,  4.,  6.],</span></span><br><span class=\"line\"><span class=\"string\">         [ 8., 10., 12., 14.],</span></span><br><span class=\"line\"><span class=\"string\">         [16., 18., 20., 22.],</span></span><br><span class=\"line\"><span class=\"string\">         [24., 26., 28., 30.],</span></span><br><span class=\"line\"><span class=\"string\">         [32., 34., 36., 38.]]))</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>两个矩阵的按元素乘法称为<em>Hadamard积</em>（Hadamard product）（数学符号$\\odot$）</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A * B</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([[  0.,   1.,   4.,   9.],</span></span><br><span class=\"line\"><span class=\"string\">        [ 16.,  25.,  36.,  49.],</span></span><br><span class=\"line\"><span class=\"string\">        [ 64.,  81., 100., 121.],</span></span><br><span class=\"line\"><span class=\"string\">        [144., 169., 196., 225.],</span></span><br><span class=\"line\"><span class=\"string\">        [256., 289., 324., 361.]])</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a = <span class=\"number\">2</span></span><br><span class=\"line\">X = torch.arange(<span class=\"number\">24</span>).reshape(<span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>)</span><br><span class=\"line\">a + X, (a * X).shape</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">(tensor([[[ 2,  3,  4,  5],</span></span><br><span class=\"line\"><span class=\"string\">          [ 6,  7,  8,  9],</span></span><br><span class=\"line\"><span class=\"string\">          [10, 11, 12, 13]],</span></span><br><span class=\"line\"><span class=\"string\"> </span></span><br><span class=\"line\"><span class=\"string\">         [[14, 15, 16, 17],</span></span><br><span class=\"line\"><span class=\"string\">          [18, 19, 20, 21],</span></span><br><span class=\"line\"><span class=\"string\">          [22, 23, 24, 25]]]),</span></span><br><span class=\"line\"><span class=\"string\"> torch.Size([2, 3, 4]))</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>计算其元素的和</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.arange(<span class=\"number\">4</span>, dtype=torch.float32)</span><br><span class=\"line\">x, x.<span class=\"built_in\">sum</span>()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># (tensor([0., 1., 2., 3.]), tensor(6.))</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>表示任意形状张量的元素和</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A.shape, A.<span class=\"built_in\">sum</span>()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># (torch.Size([5, 4]), tensor(190.))</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"降维\"><a href=\"#降维\" class=\"headerlink\" title=\"降维\"></a>降维</h3><blockquote>\n<p>指定张量沿哪一个轴来通过求和降低维度</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A_sum_axis0 = A.<span class=\"built_in\">sum</span>(axis=<span class=\"number\">0</span>)</span><br><span class=\"line\"><span class=\"comment\"># 0轴上压缩，即压缩高度</span></span><br><span class=\"line\">A_sum_axis0, A_sum_axis0.shape</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># (tensor([40., 45., 50., 55.]), torch.Size([4]))</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A_sum_axis1 = A.<span class=\"built_in\">sum</span>(axis=<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"comment\"># 1轴上压缩，即压缩宽度</span></span><br><span class=\"line\">A_sum_axis1, A_sum_axis1.shape</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># (tensor([ 6., 22., 38., 54., 70.]), torch.Size([5]))</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A.<span class=\"built_in\">sum</span>(axis=[<span class=\"number\">0</span>, <span class=\"number\">1</span>])</span><br><span class=\"line\"><span class=\"comment\"># 同时在0轴和1轴上压缩，压缩高宽</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># tensor(190.)</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>一个与求和相关的量是<em>平均值</em>（mean或average）</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A.mean(), A.<span class=\"built_in\">sum</span>() / A.numel()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># (tensor(9.5000), tensor(9.5000))</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A.mean(axis=<span class=\"number\">0</span>), A.<span class=\"built_in\">sum</span>(axis=<span class=\"number\">0</span>) / A.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># (tensor([ 8.,  9., 10., 11.]), tensor([ 8.,  9., 10., 11.]))</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"非降维求和\"><a href=\"#非降维求和\" class=\"headerlink\" title=\"非降维求和\"></a>非降维求和</h3><blockquote>\n<p>计算总和或均值时<strong>保持轴数不变</strong></p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sum_A = A.<span class=\"built_in\">sum</span>(axis=<span class=\"number\">1</span>, keepdims=<span class=\"literal\">True</span>)</span><br><span class=\"line\">sum_A</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([[ 6.],</span></span><br><span class=\"line\"><span class=\"string\">        [22.],</span></span><br><span class=\"line\"><span class=\"string\">        [38.],</span></span><br><span class=\"line\"><span class=\"string\">        [54.],</span></span><br><span class=\"line\"><span class=\"string\">        [70.]])</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>通过广播将<code>A</code>除以<code>sum_A</code></p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A / sum_A</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([[0.0000, 0.1667, 0.3333, 0.5000],</span></span><br><span class=\"line\"><span class=\"string\">        [0.1818, 0.2273, 0.2727, 0.3182],</span></span><br><span class=\"line\"><span class=\"string\">        [0.2105, 0.2368, 0.2632, 0.2895],</span></span><br><span class=\"line\"><span class=\"string\">        [0.2222, 0.2407, 0.2593, 0.2778],</span></span><br><span class=\"line\"><span class=\"string\">        [0.2286, 0.2429, 0.2571, 0.2714]])</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>某个轴计算<code>A</code>元素的累积总和</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A.cumsum(axis=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">tensor([[ 0.,  1.,  2.,  3.],</span></span><br><span class=\"line\"><span class=\"string\">        [ 4.,  6.,  8., 10.],</span></span><br><span class=\"line\"><span class=\"string\">        [12., 15., 18., 21.],</span></span><br><span class=\"line\"><span class=\"string\">        [24., 28., 32., 36.],</span></span><br><span class=\"line\"><span class=\"string\">        [40., 45., 50., 55.]])</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"点积\"><a href=\"#点积\" class=\"headerlink\" title=\"点积\"></a>点积</h3><blockquote>\n<p>点积是<strong>相同位置的按元素乘积的和</strong></p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y = torch.ones(<span class=\"number\">4</span>, dtype = torch.float32)</span><br><span class=\"line\">x, y, torch.dot(x, y)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># (tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))</span></span><br></pre></td></tr></table></figure>\n<p>我们可以通过执行按元素乘法，然后进行求和来表示两个向量的点积</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.<span class=\"built_in\">sum</span>(x * y)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># tensor(6.)</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"向量积\"><a href=\"#向量积\" class=\"headerlink\" title=\"向量积\"></a>向量积</h3><blockquote>\n<p>矩阵向量积$\\mathbf{A}\\mathbf{x}$是一个长度为$m$的列向量，其第$i$个元素是点积$\\mathbf{a}^\\top_i \\mathbf{x}$</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A.shape, x.shape, torch.mv(A, x)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># (torch.Size([5, 4]), torch.Size([4]), tensor([ 14.,  38.,  62.,  86., 110.]))</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>我们可以将矩阵-矩阵乘法$\\mathbf{AB}$看作简单地<strong>执行$m$次矩阵-向量积</strong>，并将结果拼接在一起，形成一个$n \\times m$矩阵</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">B = torch.ones(<span class=\"number\">4</span>, <span class=\"number\">3</span>)</span><br><span class=\"line\">torch.mm(A, B)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">tensor([[ 6.,  6.,  6.],</span></span><br><span class=\"line\"><span class=\"string\">        [22., 22., 22.],</span></span><br><span class=\"line\"><span class=\"string\">        [38., 38., 38.],</span></span><br><span class=\"line\"><span class=\"string\">        [54., 54., 54.],</span></span><br><span class=\"line\"><span class=\"string\">        [70., 70., 70.]])</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"范数\"><a href=\"#范数\" class=\"headerlink\" title=\"范数\"></a>范数</h3><blockquote>\n<p>$L_2$<em>范数</em>是向量<strong>元素平方和的平方根</strong>：</p>\n<script type=\"math/tex; mode=display\">\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}</script></blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">u = torch.tensor([<span class=\"number\">3.0</span>, -<span class=\"number\">4.0</span>])</span><br><span class=\"line\">torch.norm(u)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># tensor(5.)</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>$L_1$范数，它表示为<strong>向量元素的绝对值之和</strong>：</p>\n<script type=\"math/tex; mode=display\">\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^n \\left|x_i \\right|</script></blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.<span class=\"built_in\">abs</span>(u).<span class=\"built_in\">sum</span>()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># tensor(7.)</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>矩阵的<em>Frobenius（弗罗贝尼乌斯）范数</em>（Frobenius norm）是<strong>矩阵元素平方和的平方根</strong>：<script type=\"math/tex\">\\|\\mathbf{X}\\|_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n x_{ij}^2}</script></p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.norm(torch.ones((<span class=\"number\">4</span>, <span class=\"number\">9</span>))) <span class=\"comment\"># 36开方</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># tensor(6.)</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"微积分\"><a href=\"#微积分\" class=\"headerlink\" title=\"微积分\"></a>微积分</h2><blockquote>\n<p>如果$f$的<em>导数</em>存在，这个极限被定义为<script type=\"math/tex\">f'(x) = \\lim_{h \\rightarrow 0} \\frac{f(x+h) - f(x)}{h}</script>，定义$u=f(x)=3x^2-4x$</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">%matplotlib inline</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> matplotlib_inline <span class=\"keyword\">import</span> backend_inline</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">f</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">3</span> * x ** <span class=\"number\">2</span> - <span class=\"number\">4</span> * x</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>通过令$x=1$并让$h$接近$0$，$\\frac{f(x+h)-f(x)}{h}$的数值结果接近$2$</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">numerical_lim</span>(<span class=\"params\">f, x, h</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> (f(x + h) - f(x)) / h</span><br><span class=\"line\"></span><br><span class=\"line\">h = <span class=\"number\">0.1</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">5</span>):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;h=<span class=\"subst\">&#123;h:<span class=\"number\">.5</span>f&#125;</span>, numerical limit=<span class=\"subst\">&#123;numerical_lim(f, <span class=\"number\">1</span>, h):<span class=\"number\">.5</span>f&#125;</span>&#x27;</span>)</span><br><span class=\"line\">    h *= <span class=\"number\">0.1</span></span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">结果为：</span></span><br><span class=\"line\"><span class=\"string\">h=0.10000, numerical limit=2.30000</span></span><br><span class=\"line\"><span class=\"string\">h=0.01000, numerical limit=2.03000</span></span><br><span class=\"line\"><span class=\"string\">h=0.00100, numerical limit=2.00300</span></span><br><span class=\"line\"><span class=\"string\">h=0.00010, numerical limit=2.00030</span></span><br><span class=\"line\"><span class=\"string\">h=0.00001, numerical limit=2.00003</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>为了对导数的这种解释进行可视化，我们将使用<code>matplotlib</code>定义几个函数</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">use_svg_display</span>():  </span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;使用svg格式在Jupyter中显示绘图&quot;&quot;&quot;</span></span><br><span class=\"line\">    backend_inline.set_matplotlib_formats(<span class=\"string\">&#x27;svg&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">set_figsize</span>(<span class=\"params\">figsize=(<span class=\"params\"><span class=\"number\">3.5</span>, <span class=\"number\">2.5</span></span>)</span>):  </span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;设置matplotlib的图表大小&quot;&quot;&quot;</span></span><br><span class=\"line\">    use_svg_display()</span><br><span class=\"line\">    d2l.plt.rcParams[<span class=\"string\">&#x27;figure.figsize&#x27;</span>] = figsize</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">set_axes</span>(<span class=\"params\">axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;设置matplotlib的轴&quot;&quot;&quot;</span></span><br><span class=\"line\">    axes.set_xlabel(xlabel)</span><br><span class=\"line\">    axes.set_ylabel(ylabel)</span><br><span class=\"line\">    axes.set_xscale(xscale)</span><br><span class=\"line\">    axes.set_yscale(yscale)</span><br><span class=\"line\">    axes.set_xlim(xlim)</span><br><span class=\"line\">    axes.set_ylim(ylim)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> legend:</span><br><span class=\"line\">        axes.legend(legend)</span><br><span class=\"line\">    axes.grid()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">plot</span>(<span class=\"params\">X, Y=<span class=\"literal\">None</span>, xlabel=<span class=\"literal\">None</span>, ylabel=<span class=\"literal\">None</span>, legend=<span class=\"literal\">None</span>, xlim=<span class=\"literal\">None</span>,</span></span><br><span class=\"line\"><span class=\"params\">         ylim=<span class=\"literal\">None</span>, xscale=<span class=\"string\">&#x27;linear&#x27;</span>, yscale=<span class=\"string\">&#x27;linear&#x27;</span>,</span></span><br><span class=\"line\"><span class=\"params\">         fmts=(<span class=\"params\"><span class=\"string\">&#x27;-&#x27;</span>, <span class=\"string\">&#x27;m--&#x27;</span>, <span class=\"string\">&#x27;g-.&#x27;</span>, <span class=\"string\">&#x27;r:&#x27;</span></span>), figsize=(<span class=\"params\"><span class=\"number\">3.5</span>, <span class=\"number\">2.5</span></span>), axes=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;绘制数据点&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> legend <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        legend = []</span><br><span class=\"line\"></span><br><span class=\"line\">    set_figsize(figsize)</span><br><span class=\"line\">    axes = axes <span class=\"keyword\">if</span> axes <span class=\"keyword\">else</span> d2l.plt.gca()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">has_one_axis</span>(<span class=\"params\">X</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> (<span class=\"built_in\">hasattr</span>(X, <span class=\"string\">&quot;ndim&quot;</span>) <span class=\"keyword\">and</span> X.ndim == <span class=\"number\">1</span> <span class=\"keyword\">or</span> <span class=\"built_in\">isinstance</span>(X, <span class=\"built_in\">list</span>)</span><br><span class=\"line\">                <span class=\"keyword\">and</span> <span class=\"keyword\">not</span> <span class=\"built_in\">hasattr</span>(X[<span class=\"number\">0</span>], <span class=\"string\">&quot;__len__&quot;</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> has_one_axis(X):</span><br><span class=\"line\">        X = [X]</span><br><span class=\"line\">    <span class=\"keyword\">if</span> Y <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        X, Y = [[]] * <span class=\"built_in\">len</span>(X), X</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> has_one_axis(Y):</span><br><span class=\"line\">        Y = [Y]</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(X) != <span class=\"built_in\">len</span>(Y):</span><br><span class=\"line\">        X = X * <span class=\"built_in\">len</span>(Y)</span><br><span class=\"line\">    axes.cla()</span><br><span class=\"line\">    <span class=\"keyword\">for</span> x, y, fmt <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(X, Y, fmts):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(x):</span><br><span class=\"line\">            axes.plot(x, y, fmt)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            axes.plot(y, fmt)</span><br><span class=\"line\">    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>绘制函数$u=f(x)$及其在$x=1$处的切线$y=2x-3$</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = np.arange(<span class=\"number\">0</span>, <span class=\"number\">3</span>, <span class=\"number\">0.1</span>)</span><br><span class=\"line\">plot(x, [f(x), <span class=\"number\">2</span> * x - <span class=\"number\">3</span>], <span class=\"string\">&#x27;x&#x27;</span>, <span class=\"string\">&#x27;f(x)&#x27;</span>, legend=[<span class=\"string\">&#x27;f(x)&#x27;</span>, <span class=\"string\">&#x27;Tangent line (x=1)&#x27;</span>])</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2023/03/22/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/DataWhale/D2L_ch02_%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/output_7_0-16794727634302.svg\" alt=\"svg\"></p>\n<h2 id=\"自动微分\"><a href=\"#自动微分\" class=\"headerlink\" title=\"自动微分\"></a>自动微分</h2><blockquote>\n<p>假设我们想对函数$y=2\\mathbf{x}^{\\top}\\mathbf{x}$关于列向量$\\mathbf{x}$求导</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"></span><br><span class=\"line\">x = torch.arange(<span class=\"number\">4.0</span>)</span><br><span class=\"line\">x</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;tensor([0., 1., 2., 3.])&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>在我们计算$y$关于$\\mathbf{x}$的梯度之前，需要一个地方来存储梯度</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x.requires_grad_(<span class=\"literal\">True</span>)</span><br><span class=\"line\">x.grad</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>现在计算$y$</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y = <span class=\"number\">2</span> * torch.dot(x, x)</span><br><span class=\"line\">y</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;tensor(28., grad_fn=&lt;MulBackward0&gt;)&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>通过调用反向传播函数来自动计算<code>y</code>关于<code>x</code>每个分量的梯度</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y.backward()</span><br><span class=\"line\">x.grad</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;tensor([ 0.,  4.,  8., 12.])&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x.grad == <span class=\"number\">4</span> * x</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;tensor([True, True, True, True])&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>现在计算<code>x</code>的另一个函数</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x.grad.zero_()</span><br><span class=\"line\">y = x.<span class=\"built_in\">sum</span>()</span><br><span class=\"line\">y.backward()</span><br><span class=\"line\">x.grad</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;tensor([1., 1., 1., 1.])&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>深度学习中，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x.grad.zero_()</span><br><span class=\"line\">y = x * x</span><br><span class=\"line\">y.<span class=\"built_in\">sum</span>().backward()</span><br><span class=\"line\">x.grad</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;tensor([0., 2., 4., 6.])&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>将某些计算移动到记录的计算图之外</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x.grad.zero_()</span><br><span class=\"line\">y = x * x</span><br><span class=\"line\">u = y.detach()</span><br><span class=\"line\">z = u * x</span><br><span class=\"line\"></span><br><span class=\"line\">z.<span class=\"built_in\">sum</span>().backward()</span><br><span class=\"line\">x.grad == u</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;tensor([True, True, True, True])&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x.grad.zero_()</span><br><span class=\"line\">y.<span class=\"built_in\">sum</span>().backward()</span><br><span class=\"line\">x.grad == <span class=\"number\">2</span> * x</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;tensor([True, True, True, True])&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">f</span>(<span class=\"params\">a</span>):</span><br><span class=\"line\">    b = a * <span class=\"number\">2</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> b.norm() &lt; <span class=\"number\">1000</span>:</span><br><span class=\"line\">        b = b * <span class=\"number\">2</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> b.<span class=\"built_in\">sum</span>() &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">        c = b</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        c = <span class=\"number\">100</span> * b</span><br><span class=\"line\">    <span class=\"keyword\">return</span> c</span><br><span class=\"line\"></span><br><span class=\"line\">a = torch.randn(size=(), requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">d = f(a)</span><br><span class=\"line\">d.backward()</span><br><span class=\"line\"></span><br><span class=\"line\">a.grad == d / a</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;tensor(True)&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n"},{"title":"D2L_Ch01_深度学习的相关概念","abbrlink":"c3bae6c0","date":"2023-03-19T08:49:36.000Z","_content":"\n**机器学习训练的主要流程：**\n\n![image-20230319170611582](D2L_Ch01_深度学习的相关概念/image-20230319170611582.png)\n\n## 机器学习中的关键组件⚙\n\n<table>\n    <tr>\n        <td>⚙组件</td> \n        <td>注释</td> \n   </tr>\n         <tr>\n  \t\t <td>📊数据</td> \n      \t <td colspan=\"2\">数据集由一个个样本（example, sample）组成；遵循独立同分布(independently and identically distributed, i.i.d.)；每个样本由一组称为特征（features，或协变量（covariates））的属性组成。 机器学习模型会据此性进行预测；数据集包括训练集（training dataset）、验证集（val dataset）和测试集（test dataset）。</td>    \n    </tr>\n        <tr>\n        <td>📐 模型</td>\n        <td colspan=\"2\">模型是用来转换数据的，由神经网络错综复杂地交织在一起。</td>    \n    </tr>\n        <tr>\n        <td>🎯 目标函数</td>\n        <td colspan=\"2\">也被称为“损失函数”，用来量化模型的有效性。大多数情况是“可优化”的；<br />\n                        预测数值时，最常见的损失函数是平方误差（squared error），即预测值与实际值之差的平方；<br />\n                        解决分类问题时，最常见的目标函数是最小化错误率，即预测与实际情况不符的样本比例。</td>    \n    </tr>\n        <tr>\n        <td>🗝 优化算法</td>\n        <td colspan=\"2\">用于搜索最佳参数，以最小化损失函数。基本方法——梯度下降法（gradient descent）</td>    \n    </tr>\n</table>\n\n\n\n\n\n## 各种机器学习问题\n\n### 监督学习（supervised learning）\n\n<img src=\"D2L_Ch01_深度学习的相关概念/image-20230319174247282.png\">\n\n* **回归问题**（regression）\n\n    > 任何有关**“有多少”**的问题很可能是**回归问题**，其目标是生成一个模型，使它的预测值非常接近实际标签值。\n    >\n    > 例如：你让人修理了排水管，承包商花了3小时清除污水管道中的污物，然后他寄给你一张350美元的账单。 而你的朋友雇了同一个承包商2小时，他收到了250美元的账单。 如果有人请你估算清理污物的费用，你可以假设承包商收取一些基本费用，然后按小时收费。 如果这些假设成立，那么给出这两个数据样本，你就已经可以确定承包商的定价结构：50美元上门服务费，另外每小时100美元。 在不经意间，你就已经理解并应用了**线性回归算法。**\n\n* **分类问题**（classification）\n\n    > 有关“哪一个”的问题很可能是**分类问题**，其目标是训练出一个分类器来预测样本属于哪个类别。\n    >\n    > 这类问题可以分为**二项分类（binomial classification）**和**多项分类（binomial classification）**两大类，其常见的损失函数被称为**交叉熵（cross-entropy）**\n\n* **标注问题**（Annotation）\n\n    > 有时我们希望模型描绘输入图像中的内容，比如:包含一只鸡、一只狗和一只猫。\n    >\n    > 学习预测不相互排次的类别的问题成为多标签分类（multi-label classification）\n\n* **搜索问题**（search）\n\n    > 搜索引擎会使用机器学习和用户行为来获取网页相关性的评分。\n\n* **推荐系统**（recommender system）\n\n    > 对于任何给定的用户，**推荐系统**都可以检索得分最高的对象集，然后推荐给用户。其缺陷在于：因为用户更倾向于给感觉强烈的参评打分，但所以单独使用其作为预测模型会导致数据中**只包含“审查后的反馈”**；因为系统会优先推送购买量较大的产品，所以可能会形成**反馈循环**。\n\n* **序列学习**（Sequence）\n\n    > 序列学习的输入样本是连续的，如：音频、视频等。我们期望训练的模型**具有“记忆”功能**，如视频样本，通过前一帧的图像，我们可能对后一帧中发生的事情更有把握。\n\n    <table>\n        <tr>\n            <td>❓问题</td> \n            <td>❕注解</td> \n       </tr>\n             <tr>\n      \t\t <td>标记和解析</td> \n          \t <td colspan=\"2\">基于文本结构和语法假设对文本进行分解，以获得一些注释</td>    \n        </tr>\n            <tr>\n            <td>自动语言识别</td>\n            <td colspan=\"2\">输入序列是说话人的录音，输出序列是说话人所说内容的文本记录</td>    \n        </tr>\n            <tr>\n            <td>文本到语音</td>\n            <td colspan=\"2\">输入是文本，输出是音频文件</td>    \n        </tr>\n            <tr>\n            <td>机器翻译</td>\n                <td colspan=\"2\">不同于语音识别，在机器翻译中颠倒输入输出的顺序非常重要。\n                虽然机器翻译也是将一个序转换成另一个序列，但输入和输出的数量以及相应序列的顺序大都不会相同 </td>    \n        </tr>\n    </table>\n\n### 无监督学习（unsupervised Learning）\n\n<table>\n    <tr>\n        <td>⚙问题</td> \n        <td>注解</td> \n   </tr>\n         <tr>\n  \t\t <td>聚类（clustering）</td> \n      \t <td colspan=\"2\">在没有标签的情况下，将样本分成某几大类，如：风景、动物、工具等。</td>    \n    </tr>\n        <tr>\n        <td>主成分分析（PCA）</td>\n        <td colspan=\"2\">找少量的参数来准确捕捉数据的线性相关属性</td>    \n    </tr>\n        <tr>\n        <td>因果关系（CP）</td>\n        <td colspan=\"2\">根据统计数据，来发现各类数据之间的关系</td>    \n    </tr>\n        <tr>\n        <td>生成对抗网络（GAN）</td>\n            <td colspan=\"2\">一种合成数据的方法，甚至对于像图像和音频这样复杂的非结构化数据。</br>\n                            潜在的统计机制是检查真实数据和虚假数据是否相同的测试</td>    \n    </tr>\n</table>\n\n\n### 强化学习（Reinforcement Learning）\n\n![image-20230319192839845](D2L_Ch01_深度学习的相关概念/image-20230319192839845.png)\n\n* 环境被完全观测的强化学习问题，称为**马尔可夫决策过程（Markov decision process）**\n\n* 状态不依赖之前的动作，这类强化学习问题，称为**上下文老虎机（contextual bandit problem）**\n\n* 只有一组最初未知奖励的可用动作的强化学习问题称为**多臂老虎机（multi-armed bandit problem）**","source":"_posts/学习记录/DataWhale/D2L_Ch01_深度学习的相关概念.md","raw":"---\ntitle: D2L_Ch01_深度学习的相关概念\ncategories:\n  - 学习记录\n  - DataWhale\ntags:\n  - Deep Learning\n  - DataWhale\nabbrlink: c3bae6c0\ndate: 2023-03-19 16:49:36\n \n---\n\n**机器学习训练的主要流程：**\n\n![image-20230319170611582](D2L_Ch01_深度学习的相关概念/image-20230319170611582.png)\n\n## 机器学习中的关键组件⚙\n\n<table>\n    <tr>\n        <td>⚙组件</td> \n        <td>注释</td> \n   </tr>\n         <tr>\n  \t\t <td>📊数据</td> \n      \t <td colspan=\"2\">数据集由一个个样本（example, sample）组成；遵循独立同分布(independently and identically distributed, i.i.d.)；每个样本由一组称为特征（features，或协变量（covariates））的属性组成。 机器学习模型会据此性进行预测；数据集包括训练集（training dataset）、验证集（val dataset）和测试集（test dataset）。</td>    \n    </tr>\n        <tr>\n        <td>📐 模型</td>\n        <td colspan=\"2\">模型是用来转换数据的，由神经网络错综复杂地交织在一起。</td>    \n    </tr>\n        <tr>\n        <td>🎯 目标函数</td>\n        <td colspan=\"2\">也被称为“损失函数”，用来量化模型的有效性。大多数情况是“可优化”的；<br />\n                        预测数值时，最常见的损失函数是平方误差（squared error），即预测值与实际值之差的平方；<br />\n                        解决分类问题时，最常见的目标函数是最小化错误率，即预测与实际情况不符的样本比例。</td>    \n    </tr>\n        <tr>\n        <td>🗝 优化算法</td>\n        <td colspan=\"2\">用于搜索最佳参数，以最小化损失函数。基本方法——梯度下降法（gradient descent）</td>    \n    </tr>\n</table>\n\n\n\n\n\n## 各种机器学习问题\n\n### 监督学习（supervised learning）\n\n<img src=\"D2L_Ch01_深度学习的相关概念/image-20230319174247282.png\">\n\n* **回归问题**（regression）\n\n    > 任何有关**“有多少”**的问题很可能是**回归问题**，其目标是生成一个模型，使它的预测值非常接近实际标签值。\n    >\n    > 例如：你让人修理了排水管，承包商花了3小时清除污水管道中的污物，然后他寄给你一张350美元的账单。 而你的朋友雇了同一个承包商2小时，他收到了250美元的账单。 如果有人请你估算清理污物的费用，你可以假设承包商收取一些基本费用，然后按小时收费。 如果这些假设成立，那么给出这两个数据样本，你就已经可以确定承包商的定价结构：50美元上门服务费，另外每小时100美元。 在不经意间，你就已经理解并应用了**线性回归算法。**\n\n* **分类问题**（classification）\n\n    > 有关“哪一个”的问题很可能是**分类问题**，其目标是训练出一个分类器来预测样本属于哪个类别。\n    >\n    > 这类问题可以分为**二项分类（binomial classification）**和**多项分类（binomial classification）**两大类，其常见的损失函数被称为**交叉熵（cross-entropy）**\n\n* **标注问题**（Annotation）\n\n    > 有时我们希望模型描绘输入图像中的内容，比如:包含一只鸡、一只狗和一只猫。\n    >\n    > 学习预测不相互排次的类别的问题成为多标签分类（multi-label classification）\n\n* **搜索问题**（search）\n\n    > 搜索引擎会使用机器学习和用户行为来获取网页相关性的评分。\n\n* **推荐系统**（recommender system）\n\n    > 对于任何给定的用户，**推荐系统**都可以检索得分最高的对象集，然后推荐给用户。其缺陷在于：因为用户更倾向于给感觉强烈的参评打分，但所以单独使用其作为预测模型会导致数据中**只包含“审查后的反馈”**；因为系统会优先推送购买量较大的产品，所以可能会形成**反馈循环**。\n\n* **序列学习**（Sequence）\n\n    > 序列学习的输入样本是连续的，如：音频、视频等。我们期望训练的模型**具有“记忆”功能**，如视频样本，通过前一帧的图像，我们可能对后一帧中发生的事情更有把握。\n\n    <table>\n        <tr>\n            <td>❓问题</td> \n            <td>❕注解</td> \n       </tr>\n             <tr>\n      \t\t <td>标记和解析</td> \n          \t <td colspan=\"2\">基于文本结构和语法假设对文本进行分解，以获得一些注释</td>    \n        </tr>\n            <tr>\n            <td>自动语言识别</td>\n            <td colspan=\"2\">输入序列是说话人的录音，输出序列是说话人所说内容的文本记录</td>    \n        </tr>\n            <tr>\n            <td>文本到语音</td>\n            <td colspan=\"2\">输入是文本，输出是音频文件</td>    \n        </tr>\n            <tr>\n            <td>机器翻译</td>\n                <td colspan=\"2\">不同于语音识别，在机器翻译中颠倒输入输出的顺序非常重要。\n                虽然机器翻译也是将一个序转换成另一个序列，但输入和输出的数量以及相应序列的顺序大都不会相同 </td>    \n        </tr>\n    </table>\n\n### 无监督学习（unsupervised Learning）\n\n<table>\n    <tr>\n        <td>⚙问题</td> \n        <td>注解</td> \n   </tr>\n         <tr>\n  \t\t <td>聚类（clustering）</td> \n      \t <td colspan=\"2\">在没有标签的情况下，将样本分成某几大类，如：风景、动物、工具等。</td>    \n    </tr>\n        <tr>\n        <td>主成分分析（PCA）</td>\n        <td colspan=\"2\">找少量的参数来准确捕捉数据的线性相关属性</td>    \n    </tr>\n        <tr>\n        <td>因果关系（CP）</td>\n        <td colspan=\"2\">根据统计数据，来发现各类数据之间的关系</td>    \n    </tr>\n        <tr>\n        <td>生成对抗网络（GAN）</td>\n            <td colspan=\"2\">一种合成数据的方法，甚至对于像图像和音频这样复杂的非结构化数据。</br>\n                            潜在的统计机制是检查真实数据和虚假数据是否相同的测试</td>    \n    </tr>\n</table>\n\n\n### 强化学习（Reinforcement Learning）\n\n![image-20230319192839845](D2L_Ch01_深度学习的相关概念/image-20230319192839845.png)\n\n* 环境被完全观测的强化学习问题，称为**马尔可夫决策过程（Markov decision process）**\n\n* 状态不依赖之前的动作，这类强化学习问题，称为**上下文老虎机（contextual bandit problem）**\n\n* 只有一组最初未知奖励的可用动作的强化学习问题称为**多臂老虎机（multi-armed bandit problem）**","slug":"学习记录/DataWhale/D2L_Ch01_深度学习的相关概念","published":1,"updated":"2023-03-22T08:25:23.962Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clfjkj0bw000h8gszc81rdz2k","content":"<p><strong>机器学习训练的主要流程：</strong></p>\n<p><img src=\"/2023/03/19/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/DataWhale/D2L_Ch01_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-20230319170611582.png\" alt=\"image-20230319170611582\"></p>\n<h2 id=\"机器学习中的关键组件⚙\"><a href=\"#机器学习中的关键组件⚙\" class=\"headerlink\" title=\"机器学习中的关键组件⚙\"></a>机器学习中的关键组件⚙</h2><table>\n    <tr>\n        <td>⚙组件</td> \n        <td>注释</td> \n   </tr>\n         <tr>\n           <td>📊数据</td> \n           <td colspan=\"2\">数据集由一个个样本（example, sample）组成；遵循独立同分布(independently and identically distributed, i.i.d.)；每个样本由一组称为特征（features，或协变量（covariates））的属性组成。 机器学习模型会据此性进行预测；数据集包括训练集（training dataset）、验证集（val dataset）和测试集（test dataset）。</td>    \n    </tr>\n        <tr>\n        <td>📐 模型</td>\n        <td colspan=\"2\">模型是用来转换数据的，由神经网络错综复杂地交织在一起。</td>    \n    </tr>\n        <tr>\n        <td>🎯 目标函数</td>\n        <td colspan=\"2\">也被称为“损失函数”，用来量化模型的有效性。大多数情况是“可优化”的；<br>\n                        预测数值时，最常见的损失函数是平方误差（squared error），即预测值与实际值之差的平方；<br>\n                        解决分类问题时，最常见的目标函数是最小化错误率，即预测与实际情况不符的样本比例。</td>    \n    </tr>\n        <tr>\n        <td>🗝 优化算法</td>\n        <td colspan=\"2\">用于搜索最佳参数，以最小化损失函数。基本方法——梯度下降法（gradient descent）</td>    \n    </tr>\n</table>\n\n\n\n\n\n<h2 id=\"各种机器学习问题\"><a href=\"#各种机器学习问题\" class=\"headerlink\" title=\"各种机器学习问题\"></a>各种机器学习问题</h2><h3 id=\"监督学习（supervised-learning）\"><a href=\"#监督学习（supervised-learning）\" class=\"headerlink\" title=\"监督学习（supervised learning）\"></a>监督学习（supervised learning）</h3><p><img src=\"/2023/03/19/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/DataWhale/D2L_Ch01_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-20230319174247282.png\"></p>\n<ul>\n<li><p><strong>回归问题</strong>（regression）</p>\n<blockquote>\n<p>任何有关<strong>“有多少”</strong>的问题很可能是<strong>回归问题</strong>，其目标是生成一个模型，使它的预测值非常接近实际标签值。</p>\n<p>例如：你让人修理了排水管，承包商花了3小时清除污水管道中的污物，然后他寄给你一张350美元的账单。 而你的朋友雇了同一个承包商2小时，他收到了250美元的账单。 如果有人请你估算清理污物的费用，你可以假设承包商收取一些基本费用，然后按小时收费。 如果这些假设成立，那么给出这两个数据样本，你就已经可以确定承包商的定价结构：50美元上门服务费，另外每小时100美元。 在不经意间，你就已经理解并应用了<strong>线性回归算法。</strong></p>\n</blockquote>\n</li>\n<li><p><strong>分类问题</strong>（classification）</p>\n<blockquote>\n<p>有关“哪一个”的问题很可能是<strong>分类问题</strong>，其目标是训练出一个分类器来预测样本属于哪个类别。</p>\n<p>这类问题可以分为<strong>二项分类（binomial classification）</strong>和<strong>多项分类（binomial classification）</strong>两大类，其常见的损失函数被称为<strong>交叉熵（cross-entropy）</strong></p>\n</blockquote>\n</li>\n<li><p><strong>标注问题</strong>（Annotation）</p>\n<blockquote>\n<p>有时我们希望模型描绘输入图像中的内容，比如:包含一只鸡、一只狗和一只猫。</p>\n<p>学习预测不相互排次的类别的问题成为多标签分类（multi-label classification）</p>\n</blockquote>\n</li>\n<li><p><strong>搜索问题</strong>（search）</p>\n<blockquote>\n<p>搜索引擎会使用机器学习和用户行为来获取网页相关性的评分。</p>\n</blockquote>\n</li>\n<li><p><strong>推荐系统</strong>（recommender system）</p>\n<blockquote>\n<p>对于任何给定的用户，<strong>推荐系统</strong>都可以检索得分最高的对象集，然后推荐给用户。其缺陷在于：因为用户更倾向于给感觉强烈的参评打分，但所以单独使用其作为预测模型会导致数据中<strong>只包含“审查后的反馈”</strong>；因为系统会优先推送购买量较大的产品，所以可能会形成<strong>反馈循环</strong>。</p>\n</blockquote>\n</li>\n<li><p><strong>序列学习</strong>（Sequence）</p>\n<blockquote>\n<p>序列学习的输入样本是连续的，如：音频、视频等。我们期望训练的模型<strong>具有“记忆”功能</strong>，如视频样本，通过前一帧的图像，我们可能对后一帧中发生的事情更有把握。</p>\n</blockquote>\n  <table>\n      <tr>\n          <td>❓问题</td> \n          <td>❕注解</td> \n     </tr>\n           <tr>\n             <td>标记和解析</td> \n             <td colspan=\"2\">基于文本结构和语法假设对文本进行分解，以获得一些注释</td>    \n      </tr>\n          <tr>\n          <td>自动语言识别</td>\n          <td colspan=\"2\">输入序列是说话人的录音，输出序列是说话人所说内容的文本记录</td>    \n      </tr>\n          <tr>\n          <td>文本到语音</td>\n          <td colspan=\"2\">输入是文本，输出是音频文件</td>    \n      </tr>\n          <tr>\n          <td>机器翻译</td>\n              <td colspan=\"2\">不同于语音识别，在机器翻译中颠倒输入输出的顺序非常重要。\n              虽然机器翻译也是将一个序转换成另一个序列，但输入和输出的数量以及相应序列的顺序大都不会相同 </td>    \n      </tr>\n  </table>\n\n</li>\n</ul>\n<h3 id=\"无监督学习（unsupervised-Learning）\"><a href=\"#无监督学习（unsupervised-Learning）\" class=\"headerlink\" title=\"无监督学习（unsupervised Learning）\"></a>无监督学习（unsupervised Learning）</h3><table>\n    <tr>\n        <td>⚙问题</td> \n        <td>注解</td> \n   </tr>\n         <tr>\n           <td>聚类（clustering）</td> \n           <td colspan=\"2\">在没有标签的情况下，将样本分成某几大类，如：风景、动物、工具等。</td>    \n    </tr>\n        <tr>\n        <td>主成分分析（PCA）</td>\n        <td colspan=\"2\">找少量的参数来准确捕捉数据的线性相关属性</td>    \n    </tr>\n        <tr>\n        <td>因果关系（CP）</td>\n        <td colspan=\"2\">根据统计数据，来发现各类数据之间的关系</td>    \n    </tr>\n        <tr>\n        <td>生成对抗网络（GAN）</td>\n            <td colspan=\"2\">一种合成数据的方法，甚至对于像图像和音频这样复杂的非结构化数据。<br>\n                            潜在的统计机制是检查真实数据和虚假数据是否相同的测试</td>    \n    </tr>\n</table>\n\n\n<h3 id=\"强化学习（Reinforcement-Learning）\"><a href=\"#强化学习（Reinforcement-Learning）\" class=\"headerlink\" title=\"强化学习（Reinforcement Learning）\"></a>强化学习（Reinforcement Learning）</h3><p><img src=\"/2023/03/19/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/DataWhale/D2L_Ch01_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-20230319192839845.png\" alt=\"image-20230319192839845\"></p>\n<ul>\n<li><p>环境被完全观测的强化学习问题，称为<strong>马尔可夫决策过程（Markov decision process）</strong></p>\n</li>\n<li><p>状态不依赖之前的动作，这类强化学习问题，称为<strong>上下文老虎机（contextual bandit problem）</strong></p>\n</li>\n<li><p>只有一组最初未知奖励的可用动作的强化学习问题称为<strong>多臂老虎机（multi-armed bandit problem）</strong></p>\n</li>\n</ul>\n","site":{"data":{}},"cover":"/img/covers/3.jpg","cover_type":"img","excerpt":"","more":"<p><strong>机器学习训练的主要流程：</strong></p>\n<p><img src=\"/2023/03/19/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/DataWhale/D2L_Ch01_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-20230319170611582.png\" alt=\"image-20230319170611582\"></p>\n<h2 id=\"机器学习中的关键组件⚙\"><a href=\"#机器学习中的关键组件⚙\" class=\"headerlink\" title=\"机器学习中的关键组件⚙\"></a>机器学习中的关键组件⚙</h2><table>\n    <tr>\n        <td>⚙组件</td> \n        <td>注释</td> \n   </tr>\n         <tr>\n           <td>📊数据</td> \n           <td colspan=\"2\">数据集由一个个样本（example, sample）组成；遵循独立同分布(independently and identically distributed, i.i.d.)；每个样本由一组称为特征（features，或协变量（covariates））的属性组成。 机器学习模型会据此性进行预测；数据集包括训练集（training dataset）、验证集（val dataset）和测试集（test dataset）。</td>    \n    </tr>\n        <tr>\n        <td>📐 模型</td>\n        <td colspan=\"2\">模型是用来转换数据的，由神经网络错综复杂地交织在一起。</td>    \n    </tr>\n        <tr>\n        <td>🎯 目标函数</td>\n        <td colspan=\"2\">也被称为“损失函数”，用来量化模型的有效性。大多数情况是“可优化”的；<br>\n                        预测数值时，最常见的损失函数是平方误差（squared error），即预测值与实际值之差的平方；<br>\n                        解决分类问题时，最常见的目标函数是最小化错误率，即预测与实际情况不符的样本比例。</td>    \n    </tr>\n        <tr>\n        <td>🗝 优化算法</td>\n        <td colspan=\"2\">用于搜索最佳参数，以最小化损失函数。基本方法——梯度下降法（gradient descent）</td>    \n    </tr>\n</table>\n\n\n\n\n\n<h2 id=\"各种机器学习问题\"><a href=\"#各种机器学习问题\" class=\"headerlink\" title=\"各种机器学习问题\"></a>各种机器学习问题</h2><h3 id=\"监督学习（supervised-learning）\"><a href=\"#监督学习（supervised-learning）\" class=\"headerlink\" title=\"监督学习（supervised learning）\"></a>监督学习（supervised learning）</h3><p><img src=\"/2023/03/19/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/DataWhale/D2L_Ch01_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-20230319174247282.png\"></p>\n<ul>\n<li><p><strong>回归问题</strong>（regression）</p>\n<blockquote>\n<p>任何有关<strong>“有多少”</strong>的问题很可能是<strong>回归问题</strong>，其目标是生成一个模型，使它的预测值非常接近实际标签值。</p>\n<p>例如：你让人修理了排水管，承包商花了3小时清除污水管道中的污物，然后他寄给你一张350美元的账单。 而你的朋友雇了同一个承包商2小时，他收到了250美元的账单。 如果有人请你估算清理污物的费用，你可以假设承包商收取一些基本费用，然后按小时收费。 如果这些假设成立，那么给出这两个数据样本，你就已经可以确定承包商的定价结构：50美元上门服务费，另外每小时100美元。 在不经意间，你就已经理解并应用了<strong>线性回归算法。</strong></p>\n</blockquote>\n</li>\n<li><p><strong>分类问题</strong>（classification）</p>\n<blockquote>\n<p>有关“哪一个”的问题很可能是<strong>分类问题</strong>，其目标是训练出一个分类器来预测样本属于哪个类别。</p>\n<p>这类问题可以分为<strong>二项分类（binomial classification）</strong>和<strong>多项分类（binomial classification）</strong>两大类，其常见的损失函数被称为<strong>交叉熵（cross-entropy）</strong></p>\n</blockquote>\n</li>\n<li><p><strong>标注问题</strong>（Annotation）</p>\n<blockquote>\n<p>有时我们希望模型描绘输入图像中的内容，比如:包含一只鸡、一只狗和一只猫。</p>\n<p>学习预测不相互排次的类别的问题成为多标签分类（multi-label classification）</p>\n</blockquote>\n</li>\n<li><p><strong>搜索问题</strong>（search）</p>\n<blockquote>\n<p>搜索引擎会使用机器学习和用户行为来获取网页相关性的评分。</p>\n</blockquote>\n</li>\n<li><p><strong>推荐系统</strong>（recommender system）</p>\n<blockquote>\n<p>对于任何给定的用户，<strong>推荐系统</strong>都可以检索得分最高的对象集，然后推荐给用户。其缺陷在于：因为用户更倾向于给感觉强烈的参评打分，但所以单独使用其作为预测模型会导致数据中<strong>只包含“审查后的反馈”</strong>；因为系统会优先推送购买量较大的产品，所以可能会形成<strong>反馈循环</strong>。</p>\n</blockquote>\n</li>\n<li><p><strong>序列学习</strong>（Sequence）</p>\n<blockquote>\n<p>序列学习的输入样本是连续的，如：音频、视频等。我们期望训练的模型<strong>具有“记忆”功能</strong>，如视频样本，通过前一帧的图像，我们可能对后一帧中发生的事情更有把握。</p>\n</blockquote>\n  <table>\n      <tr>\n          <td>❓问题</td> \n          <td>❕注解</td> \n     </tr>\n           <tr>\n             <td>标记和解析</td> \n             <td colspan=\"2\">基于文本结构和语法假设对文本进行分解，以获得一些注释</td>    \n      </tr>\n          <tr>\n          <td>自动语言识别</td>\n          <td colspan=\"2\">输入序列是说话人的录音，输出序列是说话人所说内容的文本记录</td>    \n      </tr>\n          <tr>\n          <td>文本到语音</td>\n          <td colspan=\"2\">输入是文本，输出是音频文件</td>    \n      </tr>\n          <tr>\n          <td>机器翻译</td>\n              <td colspan=\"2\">不同于语音识别，在机器翻译中颠倒输入输出的顺序非常重要。\n              虽然机器翻译也是将一个序转换成另一个序列，但输入和输出的数量以及相应序列的顺序大都不会相同 </td>    \n      </tr>\n  </table>\n\n</li>\n</ul>\n<h3 id=\"无监督学习（unsupervised-Learning）\"><a href=\"#无监督学习（unsupervised-Learning）\" class=\"headerlink\" title=\"无监督学习（unsupervised Learning）\"></a>无监督学习（unsupervised Learning）</h3><table>\n    <tr>\n        <td>⚙问题</td> \n        <td>注解</td> \n   </tr>\n         <tr>\n           <td>聚类（clustering）</td> \n           <td colspan=\"2\">在没有标签的情况下，将样本分成某几大类，如：风景、动物、工具等。</td>    \n    </tr>\n        <tr>\n        <td>主成分分析（PCA）</td>\n        <td colspan=\"2\">找少量的参数来准确捕捉数据的线性相关属性</td>    \n    </tr>\n        <tr>\n        <td>因果关系（CP）</td>\n        <td colspan=\"2\">根据统计数据，来发现各类数据之间的关系</td>    \n    </tr>\n        <tr>\n        <td>生成对抗网络（GAN）</td>\n            <td colspan=\"2\">一种合成数据的方法，甚至对于像图像和音频这样复杂的非结构化数据。<br>\n                            潜在的统计机制是检查真实数据和虚假数据是否相同的测试</td>    \n    </tr>\n</table>\n\n\n<h3 id=\"强化学习（Reinforcement-Learning）\"><a href=\"#强化学习（Reinforcement-Learning）\" class=\"headerlink\" title=\"强化学习（Reinforcement Learning）\"></a>强化学习（Reinforcement Learning）</h3><p><img src=\"/2023/03/19/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/DataWhale/D2L_Ch01_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-20230319192839845.png\" alt=\"image-20230319192839845\"></p>\n<ul>\n<li><p>环境被完全观测的强化学习问题，称为<strong>马尔可夫决策过程（Markov decision process）</strong></p>\n</li>\n<li><p>状态不依赖之前的动作，这类强化学习问题，称为<strong>上下文老虎机（contextual bandit problem）</strong></p>\n</li>\n<li><p>只有一组最初未知奖励的可用动作的强化学习问题称为<strong>多臂老虎机（multi-armed bandit problem）</strong></p>\n</li>\n</ul>\n"},{"title":"7x24 Hours 无人监守直播间","abbrlink":"334c93db","date":"2023-03-02T07:13:36.000Z","_content":"\n# 7x24小时无人监守直播间 配置方法\n\n\n\n## **安装screen**\n\n\n```bash\n$ mkdir /home/lighthouse\n$ cd /home/lighthouse\n$ yum -y install screen\n```\n\n## **新建窗口**\n\n\n```bash\n$ screen -S game\n```\n\n## **直播循环推流脚本**\n\n\n```python\n#!/bin/bash\nPATH=/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin:~/bin\nexport PATH\n#=================================================================#\n#   System Required: CentOS7 X86_64                               #\n#   Description: FFmpeg Stream Media Server                       #\n#   Author: LALA                                    #\n#   Website: https://www.lala.im                                  #\n#=================================================================#\n\n# 颜色选择\nred='\\033[0;31m'\ngreen='\\033[0;32m'\nyellow='\\033[0;33m'\nfont=\"\\033[0m\"\n\nffmpeg_install(){\n# 安装FFMPEG\nread -p \"你的机器内是否已经安装过FFmpeg4.x?安装FFmpeg才能正常推流,是否现在安装FFmpeg?(yes/no):\" Choose\nif [ $Choose = \"yes\" ];then\n\tyum -y install wget\n\twget --no-check-certificate https://www.johnvansickle.com/ffmpeg/old-releases/ffmpeg-4.0.3-64bit-static.tar.xz\n\ttar -xJf ffmpeg-4.0.3-64bit-static.tar.xz\n\tcd ffmpeg-4.0.3-64bit-static\n\tmv ffmpeg /usr/bin && mv ffprobe /usr/bin && mv qt-faststart /usr/bin && mv ffmpeg-10bit /usr/bin\nfi\nif [ $Choose = \"no\" ]\nthen\n    echo -e \"${yellow} 你选择不安装FFmpeg,请确定你的机器内已经自行安装过FFmpeg,否则程序无法正常工作! ${font}\"\n    sleep 2\nfi\n\t}\n\nstream_start(){\n# 定义推流地址和推流码\nread -p \"输入你的推流地址和推流码(rtmp协议):\" rtmp\n\n# 判断用户输入的地址是否合法\nif [[ $rtmp =~ \"rtmp://\" ]];then\n\techo -e \"${green} 推流地址输入正确,程序将进行下一步操作. ${font}\"\n  \tsleep 2\n\telse  \n  \techo -e \"${red} 你输入的地址不合法,请重新运行程序并输入! ${font}\"\n  \texit 1\nfi \n\n# 定义视频存放目录\nread -p \"输入你的视频存放目录 (格式仅支持mp4,并且要绝对路径,例如/opt/video):\" folder\n\n# 判断是否需要添加水印\nread -p \"是否需要为视频添加水印?水印位置默认在右上方,需要较好CPU支持(yes/no):\" watermark\nif [ $watermark = \"yes\" ];then\n\tread -p \"输入你的水印图片存放绝对路径,例如/opt/image/watermark.jpg (格式支持jpg/png/bmp):\" image\n\techo -e \"${yellow} 添加水印完成,程序将开始推流. ${font}\"\n\t# 循环\n\twhile true\n\tdo\n\t\tcd $folder\n\t\tfor video in $(ls *.mp4)\n\t\tdo\n\t\tffmpeg -re -i \"$video\" -i \"$image\" -filter_complex overlay=W-w-5:5 -c:v libx264 -c:a aac -b:a 192k -strict -2 -f flv ${rtmp}\n\t\tdone\n\tdone\nfi\nif [ $watermark = \"no\" ]\nthen\n    echo -e \"${yellow} 你选择不添加水印,程序将开始推流. ${font}\"\n    # 循环\n\twhile true\n\tdo\n\t\tcd $folder\n\t\tvideo=$(find ./ -type f | shuf -n 1)\n\t\tffmpeg -re -i \"$video\" -preset ultrafast -vcodec libx264 -g 60 -b:v 1500k -c:a aac -b:a 92k -strict -2 -f flv ${rtmp}\n \tdone\nfi\n \t}\n\n# 停止推流\nstream_stop(){\n\tscreen -S stream -X quit\n\tkillall ffmpeg\n\t}\n\n# 开始菜单设置\necho -e \"${yellow} CentOS7 X86_64 FFmpeg无人值守循环推流 For LALA.IM ${font}\"\necho -e \"${red} 请确定此脚本目前是在screen窗口内运行的! ${font}\"\necho -e \"${green} 1.安装FFmpeg (机器要安装FFmpeg才能正常推流) ${font}\"\necho -e \"${green} 2.开始无人值守循环推流 ${font}\"\necho -e \"${green} 3.停止推流 ${font}\"\nstart_menu(){\n    read -p \"请输入数字(1-3),选择你要进行的操作:\" num\n    case \"$num\" in\n        1)\n        ffmpeg_install\n        ;;\n        2)\n        stream_start\n        ;;\n        3)\n        stream_stop\n        ;;\n        *)\n        echo -e \"${red} 请输入正确的数字 (1-3) ${font}\"\n        ;;\n    esac\n\t}\n\n# 运行开始菜单\nstart_menu\n```\n\n## **运行脚本**\n\n\n```linux\n$ bash ./直播循环推流脚本.sh\n1：安装ffmpg\n2：输入直播间的直播网址和直播密钥\n3：停止推流\n```\n\n## **detach: 使脚本脱离窗口束缚**\n\n\n```bash\n$ screen -d 进程名称\n```\n\n## **关闭进程**\n\n\n```bash\n$ screen -X -S 进程名称 quit\n```\n\n","source":"_posts/踩坑日记/Tips/7x24-Hours-无人监守直播间.md","raw":"---\ntitle: 7x24 Hours 无人监守直播间\ncategories:\n  - 踩坑日记\n  - Tips\ntags:\n  - bash脚本\nabbrlink: 334c93db\ndate: 2023-03-02 15:13:36\n---\n\n# 7x24小时无人监守直播间 配置方法\n\n\n\n## **安装screen**\n\n\n```bash\n$ mkdir /home/lighthouse\n$ cd /home/lighthouse\n$ yum -y install screen\n```\n\n## **新建窗口**\n\n\n```bash\n$ screen -S game\n```\n\n## **直播循环推流脚本**\n\n\n```python\n#!/bin/bash\nPATH=/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin:~/bin\nexport PATH\n#=================================================================#\n#   System Required: CentOS7 X86_64                               #\n#   Description: FFmpeg Stream Media Server                       #\n#   Author: LALA                                    #\n#   Website: https://www.lala.im                                  #\n#=================================================================#\n\n# 颜色选择\nred='\\033[0;31m'\ngreen='\\033[0;32m'\nyellow='\\033[0;33m'\nfont=\"\\033[0m\"\n\nffmpeg_install(){\n# 安装FFMPEG\nread -p \"你的机器内是否已经安装过FFmpeg4.x?安装FFmpeg才能正常推流,是否现在安装FFmpeg?(yes/no):\" Choose\nif [ $Choose = \"yes\" ];then\n\tyum -y install wget\n\twget --no-check-certificate https://www.johnvansickle.com/ffmpeg/old-releases/ffmpeg-4.0.3-64bit-static.tar.xz\n\ttar -xJf ffmpeg-4.0.3-64bit-static.tar.xz\n\tcd ffmpeg-4.0.3-64bit-static\n\tmv ffmpeg /usr/bin && mv ffprobe /usr/bin && mv qt-faststart /usr/bin && mv ffmpeg-10bit /usr/bin\nfi\nif [ $Choose = \"no\" ]\nthen\n    echo -e \"${yellow} 你选择不安装FFmpeg,请确定你的机器内已经自行安装过FFmpeg,否则程序无法正常工作! ${font}\"\n    sleep 2\nfi\n\t}\n\nstream_start(){\n# 定义推流地址和推流码\nread -p \"输入你的推流地址和推流码(rtmp协议):\" rtmp\n\n# 判断用户输入的地址是否合法\nif [[ $rtmp =~ \"rtmp://\" ]];then\n\techo -e \"${green} 推流地址输入正确,程序将进行下一步操作. ${font}\"\n  \tsleep 2\n\telse  \n  \techo -e \"${red} 你输入的地址不合法,请重新运行程序并输入! ${font}\"\n  \texit 1\nfi \n\n# 定义视频存放目录\nread -p \"输入你的视频存放目录 (格式仅支持mp4,并且要绝对路径,例如/opt/video):\" folder\n\n# 判断是否需要添加水印\nread -p \"是否需要为视频添加水印?水印位置默认在右上方,需要较好CPU支持(yes/no):\" watermark\nif [ $watermark = \"yes\" ];then\n\tread -p \"输入你的水印图片存放绝对路径,例如/opt/image/watermark.jpg (格式支持jpg/png/bmp):\" image\n\techo -e \"${yellow} 添加水印完成,程序将开始推流. ${font}\"\n\t# 循环\n\twhile true\n\tdo\n\t\tcd $folder\n\t\tfor video in $(ls *.mp4)\n\t\tdo\n\t\tffmpeg -re -i \"$video\" -i \"$image\" -filter_complex overlay=W-w-5:5 -c:v libx264 -c:a aac -b:a 192k -strict -2 -f flv ${rtmp}\n\t\tdone\n\tdone\nfi\nif [ $watermark = \"no\" ]\nthen\n    echo -e \"${yellow} 你选择不添加水印,程序将开始推流. ${font}\"\n    # 循环\n\twhile true\n\tdo\n\t\tcd $folder\n\t\tvideo=$(find ./ -type f | shuf -n 1)\n\t\tffmpeg -re -i \"$video\" -preset ultrafast -vcodec libx264 -g 60 -b:v 1500k -c:a aac -b:a 92k -strict -2 -f flv ${rtmp}\n \tdone\nfi\n \t}\n\n# 停止推流\nstream_stop(){\n\tscreen -S stream -X quit\n\tkillall ffmpeg\n\t}\n\n# 开始菜单设置\necho -e \"${yellow} CentOS7 X86_64 FFmpeg无人值守循环推流 For LALA.IM ${font}\"\necho -e \"${red} 请确定此脚本目前是在screen窗口内运行的! ${font}\"\necho -e \"${green} 1.安装FFmpeg (机器要安装FFmpeg才能正常推流) ${font}\"\necho -e \"${green} 2.开始无人值守循环推流 ${font}\"\necho -e \"${green} 3.停止推流 ${font}\"\nstart_menu(){\n    read -p \"请输入数字(1-3),选择你要进行的操作:\" num\n    case \"$num\" in\n        1)\n        ffmpeg_install\n        ;;\n        2)\n        stream_start\n        ;;\n        3)\n        stream_stop\n        ;;\n        *)\n        echo -e \"${red} 请输入正确的数字 (1-3) ${font}\"\n        ;;\n    esac\n\t}\n\n# 运行开始菜单\nstart_menu\n```\n\n## **运行脚本**\n\n\n```linux\n$ bash ./直播循环推流脚本.sh\n1：安装ffmpg\n2：输入直播间的直播网址和直播密钥\n3：停止推流\n```\n\n## **detach: 使脚本脱离窗口束缚**\n\n\n```bash\n$ screen -d 进程名称\n```\n\n## **关闭进程**\n\n\n```bash\n$ screen -X -S 进程名称 quit\n```\n\n","slug":"踩坑日记/Tips/7x24-Hours-无人监守直播间","published":1,"updated":"2023-03-21T11:13:47.616Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clfjkj0bx000j8gsz0pzga2oz","content":"<h1 id=\"7x24小时无人监守直播间-配置方法\"><a href=\"#7x24小时无人监守直播间-配置方法\" class=\"headerlink\" title=\"7x24小时无人监守直播间 配置方法\"></a>7x24小时无人监守直播间 配置方法</h1><h2 id=\"安装screen\"><a href=\"#安装screen\" class=\"headerlink\" title=\"安装screen\"></a><strong>安装screen</strong></h2><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ <span class=\"built_in\">mkdir</span> /home/lighthouse</span><br><span class=\"line\">$ <span class=\"built_in\">cd</span> /home/lighthouse</span><br><span class=\"line\">$ yum -y install screen</span><br></pre></td></tr></table></figure>\n<h2 id=\"新建窗口\"><a href=\"#新建窗口\" class=\"headerlink\" title=\"新建窗口\"></a><strong>新建窗口</strong></h2><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ screen -S game</span><br></pre></td></tr></table></figure>\n<h2 id=\"直播循环推流脚本\"><a href=\"#直播循环推流脚本\" class=\"headerlink\" title=\"直播循环推流脚本\"></a><strong>直播循环推流脚本</strong></h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#!/bin/bash</span></span><br><span class=\"line\">PATH=/<span class=\"built_in\">bin</span>:/sbin:/usr/<span class=\"built_in\">bin</span>:/usr/sbin:/usr/local/<span class=\"built_in\">bin</span>:/usr/local/sbin:~/<span class=\"built_in\">bin</span></span><br><span class=\"line\">export PATH</span><br><span class=\"line\"><span class=\"comment\">#=================================================================#</span></span><br><span class=\"line\"><span class=\"comment\">#   System Required: CentOS7 X86_64                               #</span></span><br><span class=\"line\"><span class=\"comment\">#   Description: FFmpeg Stream Media Server                       #</span></span><br><span class=\"line\"><span class=\"comment\">#   Author: LALA                                    #</span></span><br><span class=\"line\"><span class=\"comment\">#   Website: https://www.lala.im                                  #</span></span><br><span class=\"line\"><span class=\"comment\">#=================================================================#</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 颜色选择</span></span><br><span class=\"line\">red=<span class=\"string\">&#x27;\\033[0;31m&#x27;</span></span><br><span class=\"line\">green=<span class=\"string\">&#x27;\\033[0;32m&#x27;</span></span><br><span class=\"line\">yellow=<span class=\"string\">&#x27;\\033[0;33m&#x27;</span></span><br><span class=\"line\">font=<span class=\"string\">&quot;\\033[0m&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">ffmpeg_install()&#123;</span><br><span class=\"line\"><span class=\"comment\"># 安装FFMPEG</span></span><br><span class=\"line\">read -p <span class=\"string\">&quot;你的机器内是否已经安装过FFmpeg4.x?安装FFmpeg才能正常推流,是否现在安装FFmpeg?(yes/no):&quot;</span> Choose</span><br><span class=\"line\"><span class=\"keyword\">if</span> [ $Choose = <span class=\"string\">&quot;yes&quot;</span> ];then</span><br><span class=\"line\">\tyum -y install wget</span><br><span class=\"line\">\twget --no-check-certificate https://www.johnvansickle.com/ffmpeg/old-releases/ffmpeg-<span class=\"number\">4.0</span><span class=\"number\">.3</span>-64bit-static.tar.xz</span><br><span class=\"line\">\ttar -xJf ffmpeg-<span class=\"number\">4.0</span><span class=\"number\">.3</span>-64bit-static.tar.xz</span><br><span class=\"line\">\tcd ffmpeg-<span class=\"number\">4.0</span><span class=\"number\">.3</span>-64bit-static</span><br><span class=\"line\">\tmv ffmpeg /usr/<span class=\"built_in\">bin</span> &amp;&amp; mv ffprobe /usr/<span class=\"built_in\">bin</span> &amp;&amp; mv qt-faststart /usr/<span class=\"built_in\">bin</span> &amp;&amp; mv ffmpeg-10bit /usr/<span class=\"built_in\">bin</span></span><br><span class=\"line\">fi</span><br><span class=\"line\"><span class=\"keyword\">if</span> [ $Choose = <span class=\"string\">&quot;no&quot;</span> ]</span><br><span class=\"line\">then</span><br><span class=\"line\">    echo -e <span class=\"string\">&quot;$&#123;yellow&#125; 你选择不安装FFmpeg,请确定你的机器内已经自行安装过FFmpeg,否则程序无法正常工作! $&#123;font&#125;&quot;</span></span><br><span class=\"line\">    sleep <span class=\"number\">2</span></span><br><span class=\"line\">fi</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">stream_start()&#123;</span><br><span class=\"line\"><span class=\"comment\"># 定义推流地址和推流码</span></span><br><span class=\"line\">read -p <span class=\"string\">&quot;输入你的推流地址和推流码(rtmp协议):&quot;</span> rtmp</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 判断用户输入的地址是否合法</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> [[ $rtmp =~ <span class=\"string\">&quot;rtmp://&quot;</span> ]];then</span><br><span class=\"line\">\techo -e <span class=\"string\">&quot;$&#123;green&#125; 推流地址输入正确,程序将进行下一步操作. $&#123;font&#125;&quot;</span></span><br><span class=\"line\">  \tsleep <span class=\"number\">2</span></span><br><span class=\"line\">\t<span class=\"keyword\">else</span>  </span><br><span class=\"line\">  \techo -e <span class=\"string\">&quot;$&#123;red&#125; 你输入的地址不合法,请重新运行程序并输入! $&#123;font&#125;&quot;</span></span><br><span class=\"line\">  \texit <span class=\"number\">1</span></span><br><span class=\"line\">fi </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义视频存放目录</span></span><br><span class=\"line\">read -p <span class=\"string\">&quot;输入你的视频存放目录 (格式仅支持mp4,并且要绝对路径,例如/opt/video):&quot;</span> folder</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 判断是否需要添加水印</span></span><br><span class=\"line\">read -p <span class=\"string\">&quot;是否需要为视频添加水印?水印位置默认在右上方,需要较好CPU支持(yes/no):&quot;</span> watermark</span><br><span class=\"line\"><span class=\"keyword\">if</span> [ $watermark = <span class=\"string\">&quot;yes&quot;</span> ];then</span><br><span class=\"line\">\tread -p <span class=\"string\">&quot;输入你的水印图片存放绝对路径,例如/opt/image/watermark.jpg (格式支持jpg/png/bmp):&quot;</span> image</span><br><span class=\"line\">\techo -e <span class=\"string\">&quot;$&#123;yellow&#125; 添加水印完成,程序将开始推流. $&#123;font&#125;&quot;</span></span><br><span class=\"line\">\t<span class=\"comment\"># 循环</span></span><br><span class=\"line\">\t<span class=\"keyword\">while</span> true</span><br><span class=\"line\">\tdo</span><br><span class=\"line\">\t\tcd $folder</span><br><span class=\"line\">\t\t<span class=\"keyword\">for</span> video <span class=\"keyword\">in</span> $(ls *.mp4)</span><br><span class=\"line\">\t\tdo</span><br><span class=\"line\">\t\tffmpeg -re -i <span class=\"string\">&quot;$video&quot;</span> -i <span class=\"string\">&quot;$image&quot;</span> -filter_complex overlay=W-w-<span class=\"number\">5</span>:<span class=\"number\">5</span> -c:v libx264 -c:a aac -b:a 192k -strict -<span class=\"number\">2</span> -f flv $&#123;rtmp&#125;</span><br><span class=\"line\">\t\tdone</span><br><span class=\"line\">\tdone</span><br><span class=\"line\">fi</span><br><span class=\"line\"><span class=\"keyword\">if</span> [ $watermark = <span class=\"string\">&quot;no&quot;</span> ]</span><br><span class=\"line\">then</span><br><span class=\"line\">    echo -e <span class=\"string\">&quot;$&#123;yellow&#125; 你选择不添加水印,程序将开始推流. $&#123;font&#125;&quot;</span></span><br><span class=\"line\">    <span class=\"comment\"># 循环</span></span><br><span class=\"line\">\t<span class=\"keyword\">while</span> true</span><br><span class=\"line\">\tdo</span><br><span class=\"line\">\t\tcd $folder</span><br><span class=\"line\">\t\tvideo=$(find ./ -<span class=\"built_in\">type</span> f | shuf -n <span class=\"number\">1</span>)</span><br><span class=\"line\">\t\tffmpeg -re -i <span class=\"string\">&quot;$video&quot;</span> -preset ultrafast -vcodec libx264 -g <span class=\"number\">60</span> -b:v 1500k -c:a aac -b:a 92k -strict -<span class=\"number\">2</span> -f flv $&#123;rtmp&#125;</span><br><span class=\"line\"> \tdone</span><br><span class=\"line\">fi</span><br><span class=\"line\"> \t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 停止推流</span></span><br><span class=\"line\">stream_stop()&#123;</span><br><span class=\"line\">\tscreen -S stream -X quit</span><br><span class=\"line\">\tkillall ffmpeg</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 开始菜单设置</span></span><br><span class=\"line\">echo -e <span class=\"string\">&quot;$&#123;yellow&#125; CentOS7 X86_64 FFmpeg无人值守循环推流 For LALA.IM $&#123;font&#125;&quot;</span></span><br><span class=\"line\">echo -e <span class=\"string\">&quot;$&#123;red&#125; 请确定此脚本目前是在screen窗口内运行的! $&#123;font&#125;&quot;</span></span><br><span class=\"line\">echo -e <span class=\"string\">&quot;$&#123;green&#125; 1.安装FFmpeg (机器要安装FFmpeg才能正常推流) $&#123;font&#125;&quot;</span></span><br><span class=\"line\">echo -e <span class=\"string\">&quot;$&#123;green&#125; 2.开始无人值守循环推流 $&#123;font&#125;&quot;</span></span><br><span class=\"line\">echo -e <span class=\"string\">&quot;$&#123;green&#125; 3.停止推流 $&#123;font&#125;&quot;</span></span><br><span class=\"line\">start_menu()&#123;</span><br><span class=\"line\">    read -p <span class=\"string\">&quot;请输入数字(1-3),选择你要进行的操作:&quot;</span> num</span><br><span class=\"line\">    <span class=\"keyword\">case</span> <span class=\"string\">&quot;$num&quot;</span> <span class=\"keyword\">in</span></span><br><span class=\"line\">        <span class=\"number\">1</span>)</span><br><span class=\"line\">        ffmpeg_install</span><br><span class=\"line\">        ;;</span><br><span class=\"line\">        <span class=\"number\">2</span>)</span><br><span class=\"line\">        stream_start</span><br><span class=\"line\">        ;;</span><br><span class=\"line\">        <span class=\"number\">3</span>)</span><br><span class=\"line\">        stream_stop</span><br><span class=\"line\">        ;;</span><br><span class=\"line\">        *)</span><br><span class=\"line\">        echo -e <span class=\"string\">&quot;$&#123;red&#125; 请输入正确的数字 (1-3) $&#123;font&#125;&quot;</span></span><br><span class=\"line\">        ;;</span><br><span class=\"line\">    esac</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 运行开始菜单</span></span><br><span class=\"line\">start_menu</span><br></pre></td></tr></table></figure>\n<h2 id=\"运行脚本\"><a href=\"#运行脚本\" class=\"headerlink\" title=\"运行脚本\"></a><strong>运行脚本</strong></h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ bash ./直播循环推流脚本.sh</span><br><span class=\"line\">1：安装ffmpg</span><br><span class=\"line\">2：输入直播间的直播网址和直播密钥</span><br><span class=\"line\">3：停止推流</span><br></pre></td></tr></table></figure>\n<h2 id=\"detach-使脚本脱离窗口束缚\"><a href=\"#detach-使脚本脱离窗口束缚\" class=\"headerlink\" title=\"detach: 使脚本脱离窗口束缚\"></a><strong>detach: 使脚本脱离窗口束缚</strong></h2><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ screen -d 进程名称</span><br></pre></td></tr></table></figure>\n<h2 id=\"关闭进程\"><a href=\"#关闭进程\" class=\"headerlink\" title=\"关闭进程\"></a><strong>关闭进程</strong></h2><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ screen -X -S 进程名称 quit</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"cover":"/img/covers/9.jpg","cover_type":"img","excerpt":"","more":"<h1 id=\"7x24小时无人监守直播间-配置方法\"><a href=\"#7x24小时无人监守直播间-配置方法\" class=\"headerlink\" title=\"7x24小时无人监守直播间 配置方法\"></a>7x24小时无人监守直播间 配置方法</h1><h2 id=\"安装screen\"><a href=\"#安装screen\" class=\"headerlink\" title=\"安装screen\"></a><strong>安装screen</strong></h2><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ <span class=\"built_in\">mkdir</span> /home/lighthouse</span><br><span class=\"line\">$ <span class=\"built_in\">cd</span> /home/lighthouse</span><br><span class=\"line\">$ yum -y install screen</span><br></pre></td></tr></table></figure>\n<h2 id=\"新建窗口\"><a href=\"#新建窗口\" class=\"headerlink\" title=\"新建窗口\"></a><strong>新建窗口</strong></h2><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ screen -S game</span><br></pre></td></tr></table></figure>\n<h2 id=\"直播循环推流脚本\"><a href=\"#直播循环推流脚本\" class=\"headerlink\" title=\"直播循环推流脚本\"></a><strong>直播循环推流脚本</strong></h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#!/bin/bash</span></span><br><span class=\"line\">PATH=/<span class=\"built_in\">bin</span>:/sbin:/usr/<span class=\"built_in\">bin</span>:/usr/sbin:/usr/local/<span class=\"built_in\">bin</span>:/usr/local/sbin:~/<span class=\"built_in\">bin</span></span><br><span class=\"line\">export PATH</span><br><span class=\"line\"><span class=\"comment\">#=================================================================#</span></span><br><span class=\"line\"><span class=\"comment\">#   System Required: CentOS7 X86_64                               #</span></span><br><span class=\"line\"><span class=\"comment\">#   Description: FFmpeg Stream Media Server                       #</span></span><br><span class=\"line\"><span class=\"comment\">#   Author: LALA                                    #</span></span><br><span class=\"line\"><span class=\"comment\">#   Website: https://www.lala.im                                  #</span></span><br><span class=\"line\"><span class=\"comment\">#=================================================================#</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 颜色选择</span></span><br><span class=\"line\">red=<span class=\"string\">&#x27;\\033[0;31m&#x27;</span></span><br><span class=\"line\">green=<span class=\"string\">&#x27;\\033[0;32m&#x27;</span></span><br><span class=\"line\">yellow=<span class=\"string\">&#x27;\\033[0;33m&#x27;</span></span><br><span class=\"line\">font=<span class=\"string\">&quot;\\033[0m&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">ffmpeg_install()&#123;</span><br><span class=\"line\"><span class=\"comment\"># 安装FFMPEG</span></span><br><span class=\"line\">read -p <span class=\"string\">&quot;你的机器内是否已经安装过FFmpeg4.x?安装FFmpeg才能正常推流,是否现在安装FFmpeg?(yes/no):&quot;</span> Choose</span><br><span class=\"line\"><span class=\"keyword\">if</span> [ $Choose = <span class=\"string\">&quot;yes&quot;</span> ];then</span><br><span class=\"line\">\tyum -y install wget</span><br><span class=\"line\">\twget --no-check-certificate https://www.johnvansickle.com/ffmpeg/old-releases/ffmpeg-<span class=\"number\">4.0</span><span class=\"number\">.3</span>-64bit-static.tar.xz</span><br><span class=\"line\">\ttar -xJf ffmpeg-<span class=\"number\">4.0</span><span class=\"number\">.3</span>-64bit-static.tar.xz</span><br><span class=\"line\">\tcd ffmpeg-<span class=\"number\">4.0</span><span class=\"number\">.3</span>-64bit-static</span><br><span class=\"line\">\tmv ffmpeg /usr/<span class=\"built_in\">bin</span> &amp;&amp; mv ffprobe /usr/<span class=\"built_in\">bin</span> &amp;&amp; mv qt-faststart /usr/<span class=\"built_in\">bin</span> &amp;&amp; mv ffmpeg-10bit /usr/<span class=\"built_in\">bin</span></span><br><span class=\"line\">fi</span><br><span class=\"line\"><span class=\"keyword\">if</span> [ $Choose = <span class=\"string\">&quot;no&quot;</span> ]</span><br><span class=\"line\">then</span><br><span class=\"line\">    echo -e <span class=\"string\">&quot;$&#123;yellow&#125; 你选择不安装FFmpeg,请确定你的机器内已经自行安装过FFmpeg,否则程序无法正常工作! $&#123;font&#125;&quot;</span></span><br><span class=\"line\">    sleep <span class=\"number\">2</span></span><br><span class=\"line\">fi</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">stream_start()&#123;</span><br><span class=\"line\"><span class=\"comment\"># 定义推流地址和推流码</span></span><br><span class=\"line\">read -p <span class=\"string\">&quot;输入你的推流地址和推流码(rtmp协议):&quot;</span> rtmp</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 判断用户输入的地址是否合法</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> [[ $rtmp =~ <span class=\"string\">&quot;rtmp://&quot;</span> ]];then</span><br><span class=\"line\">\techo -e <span class=\"string\">&quot;$&#123;green&#125; 推流地址输入正确,程序将进行下一步操作. $&#123;font&#125;&quot;</span></span><br><span class=\"line\">  \tsleep <span class=\"number\">2</span></span><br><span class=\"line\">\t<span class=\"keyword\">else</span>  </span><br><span class=\"line\">  \techo -e <span class=\"string\">&quot;$&#123;red&#125; 你输入的地址不合法,请重新运行程序并输入! $&#123;font&#125;&quot;</span></span><br><span class=\"line\">  \texit <span class=\"number\">1</span></span><br><span class=\"line\">fi </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义视频存放目录</span></span><br><span class=\"line\">read -p <span class=\"string\">&quot;输入你的视频存放目录 (格式仅支持mp4,并且要绝对路径,例如/opt/video):&quot;</span> folder</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 判断是否需要添加水印</span></span><br><span class=\"line\">read -p <span class=\"string\">&quot;是否需要为视频添加水印?水印位置默认在右上方,需要较好CPU支持(yes/no):&quot;</span> watermark</span><br><span class=\"line\"><span class=\"keyword\">if</span> [ $watermark = <span class=\"string\">&quot;yes&quot;</span> ];then</span><br><span class=\"line\">\tread -p <span class=\"string\">&quot;输入你的水印图片存放绝对路径,例如/opt/image/watermark.jpg (格式支持jpg/png/bmp):&quot;</span> image</span><br><span class=\"line\">\techo -e <span class=\"string\">&quot;$&#123;yellow&#125; 添加水印完成,程序将开始推流. $&#123;font&#125;&quot;</span></span><br><span class=\"line\">\t<span class=\"comment\"># 循环</span></span><br><span class=\"line\">\t<span class=\"keyword\">while</span> true</span><br><span class=\"line\">\tdo</span><br><span class=\"line\">\t\tcd $folder</span><br><span class=\"line\">\t\t<span class=\"keyword\">for</span> video <span class=\"keyword\">in</span> $(ls *.mp4)</span><br><span class=\"line\">\t\tdo</span><br><span class=\"line\">\t\tffmpeg -re -i <span class=\"string\">&quot;$video&quot;</span> -i <span class=\"string\">&quot;$image&quot;</span> -filter_complex overlay=W-w-<span class=\"number\">5</span>:<span class=\"number\">5</span> -c:v libx264 -c:a aac -b:a 192k -strict -<span class=\"number\">2</span> -f flv $&#123;rtmp&#125;</span><br><span class=\"line\">\t\tdone</span><br><span class=\"line\">\tdone</span><br><span class=\"line\">fi</span><br><span class=\"line\"><span class=\"keyword\">if</span> [ $watermark = <span class=\"string\">&quot;no&quot;</span> ]</span><br><span class=\"line\">then</span><br><span class=\"line\">    echo -e <span class=\"string\">&quot;$&#123;yellow&#125; 你选择不添加水印,程序将开始推流. $&#123;font&#125;&quot;</span></span><br><span class=\"line\">    <span class=\"comment\"># 循环</span></span><br><span class=\"line\">\t<span class=\"keyword\">while</span> true</span><br><span class=\"line\">\tdo</span><br><span class=\"line\">\t\tcd $folder</span><br><span class=\"line\">\t\tvideo=$(find ./ -<span class=\"built_in\">type</span> f | shuf -n <span class=\"number\">1</span>)</span><br><span class=\"line\">\t\tffmpeg -re -i <span class=\"string\">&quot;$video&quot;</span> -preset ultrafast -vcodec libx264 -g <span class=\"number\">60</span> -b:v 1500k -c:a aac -b:a 92k -strict -<span class=\"number\">2</span> -f flv $&#123;rtmp&#125;</span><br><span class=\"line\"> \tdone</span><br><span class=\"line\">fi</span><br><span class=\"line\"> \t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 停止推流</span></span><br><span class=\"line\">stream_stop()&#123;</span><br><span class=\"line\">\tscreen -S stream -X quit</span><br><span class=\"line\">\tkillall ffmpeg</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 开始菜单设置</span></span><br><span class=\"line\">echo -e <span class=\"string\">&quot;$&#123;yellow&#125; CentOS7 X86_64 FFmpeg无人值守循环推流 For LALA.IM $&#123;font&#125;&quot;</span></span><br><span class=\"line\">echo -e <span class=\"string\">&quot;$&#123;red&#125; 请确定此脚本目前是在screen窗口内运行的! $&#123;font&#125;&quot;</span></span><br><span class=\"line\">echo -e <span class=\"string\">&quot;$&#123;green&#125; 1.安装FFmpeg (机器要安装FFmpeg才能正常推流) $&#123;font&#125;&quot;</span></span><br><span class=\"line\">echo -e <span class=\"string\">&quot;$&#123;green&#125; 2.开始无人值守循环推流 $&#123;font&#125;&quot;</span></span><br><span class=\"line\">echo -e <span class=\"string\">&quot;$&#123;green&#125; 3.停止推流 $&#123;font&#125;&quot;</span></span><br><span class=\"line\">start_menu()&#123;</span><br><span class=\"line\">    read -p <span class=\"string\">&quot;请输入数字(1-3),选择你要进行的操作:&quot;</span> num</span><br><span class=\"line\">    <span class=\"keyword\">case</span> <span class=\"string\">&quot;$num&quot;</span> <span class=\"keyword\">in</span></span><br><span class=\"line\">        <span class=\"number\">1</span>)</span><br><span class=\"line\">        ffmpeg_install</span><br><span class=\"line\">        ;;</span><br><span class=\"line\">        <span class=\"number\">2</span>)</span><br><span class=\"line\">        stream_start</span><br><span class=\"line\">        ;;</span><br><span class=\"line\">        <span class=\"number\">3</span>)</span><br><span class=\"line\">        stream_stop</span><br><span class=\"line\">        ;;</span><br><span class=\"line\">        *)</span><br><span class=\"line\">        echo -e <span class=\"string\">&quot;$&#123;red&#125; 请输入正确的数字 (1-3) $&#123;font&#125;&quot;</span></span><br><span class=\"line\">        ;;</span><br><span class=\"line\">    esac</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 运行开始菜单</span></span><br><span class=\"line\">start_menu</span><br></pre></td></tr></table></figure>\n<h2 id=\"运行脚本\"><a href=\"#运行脚本\" class=\"headerlink\" title=\"运行脚本\"></a><strong>运行脚本</strong></h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ bash ./直播循环推流脚本.sh</span><br><span class=\"line\">1：安装ffmpg</span><br><span class=\"line\">2：输入直播间的直播网址和直播密钥</span><br><span class=\"line\">3：停止推流</span><br></pre></td></tr></table></figure>\n<h2 id=\"detach-使脚本脱离窗口束缚\"><a href=\"#detach-使脚本脱离窗口束缚\" class=\"headerlink\" title=\"detach: 使脚本脱离窗口束缚\"></a><strong>detach: 使脚本脱离窗口束缚</strong></h2><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ screen -d 进程名称</span><br></pre></td></tr></table></figure>\n<h2 id=\"关闭进程\"><a href=\"#关闭进程\" class=\"headerlink\" title=\"关闭进程\"></a><strong>关闭进程</strong></h2><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ screen -X -S 进程名称 quit</span><br></pre></td></tr></table></figure>\n"},{"title":"DataWhale_《动手学深度学习-Pytorch版》","date":"2023-03-18T00:55:10.000Z","_content":"\n# DataWhale学习记录\n\n> **🔗相关学习链接**：\n>\n> * [D2l](https://zh-v2.d2l.ai/)\n> * [《动手学深度学习》组团学习报名](https://learning.datawhale.club/p/t_pc/course_pc_detail/camp_pro/course_2MfxkUb5a1yA2lWVgW0pzsu9B2U)\n> * [启智AI平台](https://openi.org.cn/)\n> * [和鲸社区](https://www.heywhale.com/home)\n> * [学习手册](https://datawhaler.feishu.cn/docx/FBx8da3YnoD7yxxwVOKckhpsnG)\n> *  [李沐老师的Tutorial](https://space.bilibili.com/1567748478/channel/seriesdetail?sid=358497)\n> * [方烨学姐分享的学习笔记](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/tree/main/notes)\n\n## 《动手学深度学习-Pytorch版》\n\n> 📰：这是我第一次来参加DataWhale的组团学习，让我很震惊的是，相比国内培训机构动辄几千上万的培训费用，这次学习是完全Freev的，太感动了。也希望有更多人能加入DataWhale。这是一个非常有意义的项目，可以让技术小白也能跟着一起学，大大降低了学习AI技术的门槛。一直有一个想法：“AI for Everyone.”我觉得创造AI的意义在于：AI会代替人完成重复且而乏味的工作，而人类本身可以投入到更有意义的工作中。AI终将服务于全人类，不应该成为大公司的专利。\n\n✨✨✨**下面是来自DataWhale成员分享，全都是大佬🙈**\n\n* **人工智能入门路线图**\n\n    > ![人工智能路线图](DataWhale学习记录/人工智能路线图.jpg)\n\n* **CV入门建议**\n\n    > ![CV入门](DataWhale学习记录/CV入门.jpg)\n\n* **科研经验**\n\n    > ![科研经验](DataWhale学习记录/科研经验.jpg)\n\n* **深度学习细分领域**\n\n    > ![深度学习细分领域](DataWhale学习记录/深度学习细分领域.jpg)\n\n* **深度学习小技巧** \n\n    > ![深度学习小技巧](DataWhale学习记录/深度学习小技巧.jpg)\n    > ![深度学习小技巧2](DataWhale学习记录/深度学习小技巧2.jpg)\n    > ![深度学习小技巧3](DataWhale学习记录/深度学习小技巧3.jpg)\n    > ![深度学习小技巧4](DataWhale学习记录/深度学习小技巧4.jpg)\n    > ![深度学习小技巧5](DataWhale学习记录/深度学习小技巧5.jpg)\n\n* **OpenI平台使用流程**：\n\n    https://openi.pcl.ac.cn/Datawhale/d2l\n\n    本repo致力于为学习者提供一个在OpenI启智上可运行的《[动手学深度学习(PyTorch)](https://github.com/d2l-ai/d2l-zh)》的教程和环境. 推荐阅读[详细教程](https://openi.pcl.ac.cn/Datawhale/d2l/src/branch/master/Tutorials.md)\n\n    1. fork本repo至自己名下\n    2. 点击云脑，选择调试环境\n    3. 推荐镜像：`dockerhub.pcl.ac.cn:5000/user-images/openi:d2l-learning`\n    4. 进入调试环境，在线运行相关代码\n","source":"_posts/学习记录/DataWhale/DataWhale学习记录.md","raw":"---\ntitle: DataWhale_《动手学深度学习-Pytorch版》\ncategories:\n  - 学习记录\n  - DataWhale\ntags:\n  - Deep Learning\n  - Pytorch\ndate: 2023-03-18 08:55:10\n---\n\n# DataWhale学习记录\n\n> **🔗相关学习链接**：\n>\n> * [D2l](https://zh-v2.d2l.ai/)\n> * [《动手学深度学习》组团学习报名](https://learning.datawhale.club/p/t_pc/course_pc_detail/camp_pro/course_2MfxkUb5a1yA2lWVgW0pzsu9B2U)\n> * [启智AI平台](https://openi.org.cn/)\n> * [和鲸社区](https://www.heywhale.com/home)\n> * [学习手册](https://datawhaler.feishu.cn/docx/FBx8da3YnoD7yxxwVOKckhpsnG)\n> *  [李沐老师的Tutorial](https://space.bilibili.com/1567748478/channel/seriesdetail?sid=358497)\n> * [方烨学姐分享的学习笔记](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/tree/main/notes)\n\n## 《动手学深度学习-Pytorch版》\n\n> 📰：这是我第一次来参加DataWhale的组团学习，让我很震惊的是，相比国内培训机构动辄几千上万的培训费用，这次学习是完全Freev的，太感动了。也希望有更多人能加入DataWhale。这是一个非常有意义的项目，可以让技术小白也能跟着一起学，大大降低了学习AI技术的门槛。一直有一个想法：“AI for Everyone.”我觉得创造AI的意义在于：AI会代替人完成重复且而乏味的工作，而人类本身可以投入到更有意义的工作中。AI终将服务于全人类，不应该成为大公司的专利。\n\n✨✨✨**下面是来自DataWhale成员分享，全都是大佬🙈**\n\n* **人工智能入门路线图**\n\n    > ![人工智能路线图](DataWhale学习记录/人工智能路线图.jpg)\n\n* **CV入门建议**\n\n    > ![CV入门](DataWhale学习记录/CV入门.jpg)\n\n* **科研经验**\n\n    > ![科研经验](DataWhale学习记录/科研经验.jpg)\n\n* **深度学习细分领域**\n\n    > ![深度学习细分领域](DataWhale学习记录/深度学习细分领域.jpg)\n\n* **深度学习小技巧** \n\n    > ![深度学习小技巧](DataWhale学习记录/深度学习小技巧.jpg)\n    > ![深度学习小技巧2](DataWhale学习记录/深度学习小技巧2.jpg)\n    > ![深度学习小技巧3](DataWhale学习记录/深度学习小技巧3.jpg)\n    > ![深度学习小技巧4](DataWhale学习记录/深度学习小技巧4.jpg)\n    > ![深度学习小技巧5](DataWhale学习记录/深度学习小技巧5.jpg)\n\n* **OpenI平台使用流程**：\n\n    https://openi.pcl.ac.cn/Datawhale/d2l\n\n    本repo致力于为学习者提供一个在OpenI启智上可运行的《[动手学深度学习(PyTorch)](https://github.com/d2l-ai/d2l-zh)》的教程和环境. 推荐阅读[详细教程](https://openi.pcl.ac.cn/Datawhale/d2l/src/branch/master/Tutorials.md)\n\n    1. fork本repo至自己名下\n    2. 点击云脑，选择调试环境\n    3. 推荐镜像：`dockerhub.pcl.ac.cn:5000/user-images/openi:d2l-learning`\n    4. 进入调试环境，在线运行相关代码\n","slug":"学习记录/DataWhale/DataWhale学习记录","published":1,"updated":"2023-03-22T08:28:31.949Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clfjkj0bz000n8gszedk7g7pw","content":"<h1 id=\"DataWhale学习记录\"><a href=\"#DataWhale学习记录\" class=\"headerlink\" title=\"DataWhale学习记录\"></a>DataWhale学习记录</h1><blockquote>\n<p><strong>🔗相关学习链接</strong>：</p>\n<ul>\n<li><a href=\"https://zh-v2.d2l.ai/\">D2l</a></li>\n<li><a href=\"https://learning.datawhale.club/p/t_pc/course_pc_detail/camp_pro/course_2MfxkUb5a1yA2lWVgW0pzsu9B2U\">《动手学深度学习》组团学习报名</a></li>\n<li><a href=\"https://openi.org.cn/\">启智AI平台</a></li>\n<li><a href=\"https://www.heywhale.com/home\">和鲸社区</a></li>\n<li><a href=\"https://datawhaler.feishu.cn/docx/FBx8da3YnoD7yxxwVOKckhpsnG\">学习手册</a></li>\n<li><a href=\"https://space.bilibili.com/1567748478/channel/seriesdetail?sid=358497\">李沐老师的Tutorial</a></li>\n<li><a href=\"https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/tree/main/notes\">方烨学姐分享的学习笔记</a></li>\n</ul>\n</blockquote>\n<h2 id=\"《动手学深度学习-Pytorch版》\"><a href=\"#《动手学深度学习-Pytorch版》\" class=\"headerlink\" title=\"《动手学深度学习-Pytorch版》\"></a>《动手学深度学习-Pytorch版》</h2><blockquote>\n<p>📰：这是我第一次来参加DataWhale的组团学习，让我很震惊的是，相比国内培训机构动辄几千上万的培训费用，这次学习是完全Freev的，太感动了。也希望有更多人能加入DataWhale。这是一个非常有意义的项目，可以让技术小白也能跟着一起学，大大降低了学习AI技术的门槛。一直有一个想法：“AI for Everyone.”我觉得创造AI的意义在于：AI会代替人完成重复且而乏味的工作，而人类本身可以投入到更有意义的工作中。AI终将服务于全人类，不应该成为大公司的专利。</p>\n</blockquote>\n<p>✨✨✨<strong>下面是来自DataWhale成员分享，全都是大佬🙈</strong></p>\n<ul>\n<li><p><strong>人工智能入门路线图</strong></p>\n<blockquote>\n<p><img src=\"/2023/03/18/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/DataWhale/DataWhale%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/人工智能路线图.jpg\" alt=\"人工智能路线图\"></p>\n</blockquote>\n</li>\n<li><p><strong>CV入门建议</strong></p>\n<blockquote>\n<p><img src=\"/2023/03/18/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/DataWhale/DataWhale%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/CV入门.jpg\" alt=\"CV入门\"></p>\n</blockquote>\n</li>\n<li><p><strong>科研经验</strong></p>\n<blockquote>\n<p><img src=\"/2023/03/18/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/DataWhale/DataWhale%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/科研经验.jpg\" alt=\"科研经验\"></p>\n</blockquote>\n</li>\n<li><p><strong>深度学习细分领域</strong></p>\n<blockquote>\n<p><img src=\"/2023/03/18/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/DataWhale/DataWhale%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/深度学习细分领域.jpg\" alt=\"深度学习细分领域\"></p>\n</blockquote>\n</li>\n<li><p><strong>深度学习小技巧</strong> </p>\n<blockquote>\n<p><img src=\"/2023/03/18/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/DataWhale/DataWhale%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/深度学习小技巧.jpg\" alt=\"深度学习小技巧\"><br><img src=\"/2023/03/18/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/DataWhale/DataWhale%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/深度学习小技巧2.jpg\" alt=\"深度学习小技巧2\"><br><img src=\"/2023/03/18/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/DataWhale/DataWhale%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/深度学习小技巧3.jpg\" alt=\"深度学习小技巧3\"><br><img src=\"/2023/03/18/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/DataWhale/DataWhale%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/深度学习小技巧4.jpg\" alt=\"深度学习小技巧4\"><br><img src=\"/2023/03/18/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/DataWhale/DataWhale%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/深度学习小技巧5.jpg\" alt=\"深度学习小技巧5\"></p>\n</blockquote>\n</li>\n<li><p><strong>OpenI平台使用流程</strong>：</p>\n<p>  <a href=\"https://openi.pcl.ac.cn/Datawhale/d2l\">https://openi.pcl.ac.cn/Datawhale/d2l</a></p>\n<p>  本repo致力于为学习者提供一个在OpenI启智上可运行的《<a href=\"https://github.com/d2l-ai/d2l-zh\">动手学深度学习(PyTorch)</a>》的教程和环境. 推荐阅读<a href=\"https://openi.pcl.ac.cn/Datawhale/d2l/src/branch/master/Tutorials.md\">详细教程</a></p>\n<ol>\n<li>fork本repo至自己名下</li>\n<li>点击云脑，选择调试环境</li>\n<li>推荐镜像：<code>dockerhub.pcl.ac.cn:5000/user-images/openi:d2l-learning</code></li>\n<li>进入调试环境，在线运行相关代码</li>\n</ol>\n</li>\n</ul>\n","site":{"data":{}},"cover":"/img/covers/2.jpg","cover_type":"img","excerpt":"","more":"<h1 id=\"DataWhale学习记录\"><a href=\"#DataWhale学习记录\" class=\"headerlink\" title=\"DataWhale学习记录\"></a>DataWhale学习记录</h1><blockquote>\n<p><strong>🔗相关学习链接</strong>：</p>\n<ul>\n<li><a href=\"https://zh-v2.d2l.ai/\">D2l</a></li>\n<li><a href=\"https://learning.datawhale.club/p/t_pc/course_pc_detail/camp_pro/course_2MfxkUb5a1yA2lWVgW0pzsu9B2U\">《动手学深度学习》组团学习报名</a></li>\n<li><a href=\"https://openi.org.cn/\">启智AI平台</a></li>\n<li><a href=\"https://www.heywhale.com/home\">和鲸社区</a></li>\n<li><a href=\"https://datawhaler.feishu.cn/docx/FBx8da3YnoD7yxxwVOKckhpsnG\">学习手册</a></li>\n<li><a href=\"https://space.bilibili.com/1567748478/channel/seriesdetail?sid=358497\">李沐老师的Tutorial</a></li>\n<li><a href=\"https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/tree/main/notes\">方烨学姐分享的学习笔记</a></li>\n</ul>\n</blockquote>\n<h2 id=\"《动手学深度学习-Pytorch版》\"><a href=\"#《动手学深度学习-Pytorch版》\" class=\"headerlink\" title=\"《动手学深度学习-Pytorch版》\"></a>《动手学深度学习-Pytorch版》</h2><blockquote>\n<p>📰：这是我第一次来参加DataWhale的组团学习，让我很震惊的是，相比国内培训机构动辄几千上万的培训费用，这次学习是完全Freev的，太感动了。也希望有更多人能加入DataWhale。这是一个非常有意义的项目，可以让技术小白也能跟着一起学，大大降低了学习AI技术的门槛。一直有一个想法：“AI for Everyone.”我觉得创造AI的意义在于：AI会代替人完成重复且而乏味的工作，而人类本身可以投入到更有意义的工作中。AI终将服务于全人类，不应该成为大公司的专利。</p>\n</blockquote>\n<p>✨✨✨<strong>下面是来自DataWhale成员分享，全都是大佬🙈</strong></p>\n<ul>\n<li><p><strong>人工智能入门路线图</strong></p>\n<blockquote>\n<p><img src=\"/2023/03/18/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/DataWhale/DataWhale%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/人工智能路线图.jpg\" alt=\"人工智能路线图\"></p>\n</blockquote>\n</li>\n<li><p><strong>CV入门建议</strong></p>\n<blockquote>\n<p><img src=\"/2023/03/18/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/DataWhale/DataWhale%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/CV入门.jpg\" alt=\"CV入门\"></p>\n</blockquote>\n</li>\n<li><p><strong>科研经验</strong></p>\n<blockquote>\n<p><img src=\"/2023/03/18/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/DataWhale/DataWhale%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/科研经验.jpg\" alt=\"科研经验\"></p>\n</blockquote>\n</li>\n<li><p><strong>深度学习细分领域</strong></p>\n<blockquote>\n<p><img src=\"/2023/03/18/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/DataWhale/DataWhale%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/深度学习细分领域.jpg\" alt=\"深度学习细分领域\"></p>\n</blockquote>\n</li>\n<li><p><strong>深度学习小技巧</strong> </p>\n<blockquote>\n<p><img src=\"/2023/03/18/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/DataWhale/DataWhale%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/深度学习小技巧.jpg\" alt=\"深度学习小技巧\"><br><img src=\"/2023/03/18/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/DataWhale/DataWhale%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/深度学习小技巧2.jpg\" alt=\"深度学习小技巧2\"><br><img src=\"/2023/03/18/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/DataWhale/DataWhale%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/深度学习小技巧3.jpg\" alt=\"深度学习小技巧3\"><br><img src=\"/2023/03/18/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/DataWhale/DataWhale%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/深度学习小技巧4.jpg\" alt=\"深度学习小技巧4\"><br><img src=\"/2023/03/18/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/DataWhale/DataWhale%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/深度学习小技巧5.jpg\" alt=\"深度学习小技巧5\"></p>\n</blockquote>\n</li>\n<li><p><strong>OpenI平台使用流程</strong>：</p>\n<p>  <a href=\"https://openi.pcl.ac.cn/Datawhale/d2l\">https://openi.pcl.ac.cn/Datawhale/d2l</a></p>\n<p>  本repo致力于为学习者提供一个在OpenI启智上可运行的《<a href=\"https://github.com/d2l-ai/d2l-zh\">动手学深度学习(PyTorch)</a>》的教程和环境. 推荐阅读<a href=\"https://openi.pcl.ac.cn/Datawhale/d2l/src/branch/master/Tutorials.md\">详细教程</a></p>\n<ol>\n<li>fork本repo至自己名下</li>\n<li>点击云脑，选择调试环境</li>\n<li>推荐镜像：<code>dockerhub.pcl.ac.cn:5000/user-images/openi:d2l-learning</code></li>\n<li>进入调试环境，在线运行相关代码</li>\n</ol>\n</li>\n</ul>\n"},{"title":"Windows下D2L_Pytorch环境配置(CPU版)","abbrlink":"2fc3e254","date":"2023-03-20T13:34:02.000Z","_content":"\n# Windows下D2L_Pytorch环境配置(CPU版)\n\n## 下载Miniconda\n\n* [官网下载](https://docs.conda.io/en/latest/miniconda.html) \n* [开源仓库下载](https://repo.anaconda.com/miniconda/Miniconda3-latest-Windows-x86_64.exe)\n\n## 安装Miniconda：\n\n![image-20230320210319260](Windows下D2L-Pytorch环境配置-CPU版/image-20230320210330101.png)\n\n## conda更换镜像源\n\n[上海交通大学 Linux 用户组 软件镜像服务 Conda 镜像源](https://mirrors.sjtug.sjtu.edu.cn/docs/anaconda) ：\n\n编辑`~/.condarc`\n\n复制下列代码，粘贴到`~/.condarc`文件中\n\n```txt\ndefault_channels:\n  - https://mirror.sjtu.edu.cn/anaconda/pkgs/r\n  - https://mirror.sjtu.edu.cn/anaconda/pkgs/main\ncustom_channels:\n  conda-forge: https://mirror.sjtu.edu.cn/anaconda/cloud/\n  pytorch: https://mirror.sjtu.edu.cn/anaconda/cloud/\nchannels:\n  - defaults\n```\n\n## 清除缓存\n\n**Anaconda Powershell Prompt**中输入：`conda clean -i`\n\n## PyPi更换镜像源\n\n[上海交通大学 Linux 用户组 软件镜像服务 PyPI 镜像源](https://mirrors.sjtug.sjtu.edu.cn/docs/pypi-packages)\n\n复制下列代码，粘贴到**Anaconda Powershell Prompt**中运行：\n\n```bash\npip config set global.index-url https://mirror.sjtu.edu.cn/pypi/web/simple\n```\n\n> 如需安装GPU版本Pytorch镜像源，请在**Anaconda Powershell Prompt**中运行：\n>\n> ```bash\n> pip install torch===1.7.1 torchvision===0.8.2 torchaudio===0.7.2 -f https://mirror.sjtu.edu.cn/pytorch-wheels/torch_stable.html\n> ```\n\n## Conda的基础操作\n\n```bash\n conda create -n d2l python=3.9 # conda 环境创建\n conda activate d2l # 激活 d2l 环境，不同环境包版本不同！\n conda deactivate # 退出该环境\n conda remove -n d2l --all # 删除整个环境\n conda list  # 查看当前环境在已安装的包\n```\n\n## 安装课程第三方库\n\n```bash\npip install d2l torch # 必装库\npip install d2l torch torchvision rise # 所有库\n```\n\n**安装清单**： \n\n* （必装）\n\n    > d2l # 课程代码片段 \n>\n    > torch # 深度学习框架 PyTorch \n\n* （可选）\n\n    >  torchvision # PyTorch CV工具包 \n    >\n    >  rise # Notebook Slides 插件\n\n**🔗很有用的网站**\n\n  * [阿里巴巴开源镜像站-OPSX镜像站-阿里云开发者社区 (aliyun.com)](https://developer.aliyun.com/mirror/)\n  * [校园网联合镜像站 (cernet.edu.cn)](https://mirrors.cernet.edu.cn/list)\n  * [Git 下载地址 (git-scm.com)](https://git-scm.com/download/)\n  * [Github下载加速](https://ghproxy.com/)\n  * [计算机教育中缺失的一课 ](https://missing-semester-cn.github.io/)\n","source":"_posts/踩坑日记/环境配置/Windows下D2L-Pytorch环境配置-CPU版.md","raw":"---\ntitle: Windows下D2L_Pytorch环境配置(CPU版)\ncategories:\n  - 踩坑日记\n  - 环境配置\ntags:\n  - DataWhale\n  - Deep Learning\n  - Pytorch\nabbrlink: 2fc3e254\ndate: 2023-03-20 21:34:02\n---\n\n# Windows下D2L_Pytorch环境配置(CPU版)\n\n## 下载Miniconda\n\n* [官网下载](https://docs.conda.io/en/latest/miniconda.html) \n* [开源仓库下载](https://repo.anaconda.com/miniconda/Miniconda3-latest-Windows-x86_64.exe)\n\n## 安装Miniconda：\n\n![image-20230320210319260](Windows下D2L-Pytorch环境配置-CPU版/image-20230320210330101.png)\n\n## conda更换镜像源\n\n[上海交通大学 Linux 用户组 软件镜像服务 Conda 镜像源](https://mirrors.sjtug.sjtu.edu.cn/docs/anaconda) ：\n\n编辑`~/.condarc`\n\n复制下列代码，粘贴到`~/.condarc`文件中\n\n```txt\ndefault_channels:\n  - https://mirror.sjtu.edu.cn/anaconda/pkgs/r\n  - https://mirror.sjtu.edu.cn/anaconda/pkgs/main\ncustom_channels:\n  conda-forge: https://mirror.sjtu.edu.cn/anaconda/cloud/\n  pytorch: https://mirror.sjtu.edu.cn/anaconda/cloud/\nchannels:\n  - defaults\n```\n\n## 清除缓存\n\n**Anaconda Powershell Prompt**中输入：`conda clean -i`\n\n## PyPi更换镜像源\n\n[上海交通大学 Linux 用户组 软件镜像服务 PyPI 镜像源](https://mirrors.sjtug.sjtu.edu.cn/docs/pypi-packages)\n\n复制下列代码，粘贴到**Anaconda Powershell Prompt**中运行：\n\n```bash\npip config set global.index-url https://mirror.sjtu.edu.cn/pypi/web/simple\n```\n\n> 如需安装GPU版本Pytorch镜像源，请在**Anaconda Powershell Prompt**中运行：\n>\n> ```bash\n> pip install torch===1.7.1 torchvision===0.8.2 torchaudio===0.7.2 -f https://mirror.sjtu.edu.cn/pytorch-wheels/torch_stable.html\n> ```\n\n## Conda的基础操作\n\n```bash\n conda create -n d2l python=3.9 # conda 环境创建\n conda activate d2l # 激活 d2l 环境，不同环境包版本不同！\n conda deactivate # 退出该环境\n conda remove -n d2l --all # 删除整个环境\n conda list  # 查看当前环境在已安装的包\n```\n\n## 安装课程第三方库\n\n```bash\npip install d2l torch # 必装库\npip install d2l torch torchvision rise # 所有库\n```\n\n**安装清单**： \n\n* （必装）\n\n    > d2l # 课程代码片段 \n>\n    > torch # 深度学习框架 PyTorch \n\n* （可选）\n\n    >  torchvision # PyTorch CV工具包 \n    >\n    >  rise # Notebook Slides 插件\n\n**🔗很有用的网站**\n\n  * [阿里巴巴开源镜像站-OPSX镜像站-阿里云开发者社区 (aliyun.com)](https://developer.aliyun.com/mirror/)\n  * [校园网联合镜像站 (cernet.edu.cn)](https://mirrors.cernet.edu.cn/list)\n  * [Git 下载地址 (git-scm.com)](https://git-scm.com/download/)\n  * [Github下载加速](https://ghproxy.com/)\n  * [计算机教育中缺失的一课 ](https://missing-semester-cn.github.io/)\n","slug":"踩坑日记/环境配置/Windows下D2L-Pytorch环境配置-CPU版","published":1,"updated":"2023-03-22T08:28:51.649Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clfjkj0c1000p8gsz0fb37e8i","content":"<h1 id=\"Windows下D2L-Pytorch环境配置-CPU版\"><a href=\"#Windows下D2L-Pytorch环境配置-CPU版\" class=\"headerlink\" title=\"Windows下D2L_Pytorch环境配置(CPU版)\"></a>Windows下D2L_Pytorch环境配置(CPU版)</h1><h2 id=\"下载Miniconda\"><a href=\"#下载Miniconda\" class=\"headerlink\" title=\"下载Miniconda\"></a>下载Miniconda</h2><ul>\n<li><a href=\"https://docs.conda.io/en/latest/miniconda.html\">官网下载</a> </li>\n<li><a href=\"https://repo.anaconda.com/miniconda/Miniconda3-latest-Windows-x86_64.exe\">开源仓库下载</a></li>\n</ul>\n<h2 id=\"安装Miniconda：\"><a href=\"#安装Miniconda：\" class=\"headerlink\" title=\"安装Miniconda：\"></a>安装Miniconda：</h2><p><img src=\"/2023/03/20/%E8%B8%A9%E5%9D%91%E6%97%A5%E8%AE%B0/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/Windows%E4%B8%8BD2L-Pytorch%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-CPU%E7%89%88/image-20230320210330101.png\" alt=\"image-20230320210319260\"></p>\n<h2 id=\"conda更换镜像源\"><a href=\"#conda更换镜像源\" class=\"headerlink\" title=\"conda更换镜像源\"></a>conda更换镜像源</h2><p><a href=\"https://mirrors.sjtug.sjtu.edu.cn/docs/anaconda\">上海交通大学 Linux 用户组 软件镜像服务 Conda 镜像源</a> ：</p>\n<p>编辑<code>~/.condarc</code></p>\n<p>复制下列代码，粘贴到<code>~/.condarc</code>文件中</p>\n<figure class=\"highlight txt\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">default_channels:</span><br><span class=\"line\">  - https://mirror.sjtu.edu.cn/anaconda/pkgs/r</span><br><span class=\"line\">  - https://mirror.sjtu.edu.cn/anaconda/pkgs/main</span><br><span class=\"line\">custom_channels:</span><br><span class=\"line\">  conda-forge: https://mirror.sjtu.edu.cn/anaconda/cloud/</span><br><span class=\"line\">  pytorch: https://mirror.sjtu.edu.cn/anaconda/cloud/</span><br><span class=\"line\">channels:</span><br><span class=\"line\">  - defaults</span><br></pre></td></tr></table></figure>\n<h2 id=\"清除缓存\"><a href=\"#清除缓存\" class=\"headerlink\" title=\"清除缓存\"></a>清除缓存</h2><p><strong>Anaconda Powershell Prompt</strong>中输入：<code>conda clean -i</code></p>\n<h2 id=\"PyPi更换镜像源\"><a href=\"#PyPi更换镜像源\" class=\"headerlink\" title=\"PyPi更换镜像源\"></a>PyPi更换镜像源</h2><p><a href=\"https://mirrors.sjtug.sjtu.edu.cn/docs/pypi-packages\">上海交通大学 Linux 用户组 软件镜像服务 PyPI 镜像源</a></p>\n<p>复制下列代码，粘贴到<strong>Anaconda Powershell Prompt</strong>中运行：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip config <span class=\"built_in\">set</span> global.index-url https://mirror.sjtu.edu.cn/pypi/web/simple</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>如需安装GPU版本Pytorch镜像源，请在<strong>Anaconda Powershell Prompt</strong>中运行：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install torch===1.7.1 torchvision===0.8.2 torchaudio===0.7.2 -f https://mirror.sjtu.edu.cn/pytorch-wheels/torch_stable.html</span><br></pre></td></tr></table></figure>\n</blockquote>\n<h2 id=\"Conda的基础操作\"><a href=\"#Conda的基础操作\" class=\"headerlink\" title=\"Conda的基础操作\"></a>Conda的基础操作</h2><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda create -n d2l python=3.9 <span class=\"comment\"># conda 环境创建</span></span><br><span class=\"line\">conda activate d2l <span class=\"comment\"># 激活 d2l 环境，不同环境包版本不同！</span></span><br><span class=\"line\">conda deactivate <span class=\"comment\"># 退出该环境</span></span><br><span class=\"line\">conda remove -n d2l --all <span class=\"comment\"># 删除整个环境</span></span><br><span class=\"line\">conda list  <span class=\"comment\"># 查看当前环境在已安装的包</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"安装课程第三方库\"><a href=\"#安装课程第三方库\" class=\"headerlink\" title=\"安装课程第三方库\"></a>安装课程第三方库</h2><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install d2l torch <span class=\"comment\"># 必装库</span></span><br><span class=\"line\">pip install d2l torch torchvision rise <span class=\"comment\"># 所有库</span></span><br></pre></td></tr></table></figure>\n<p><strong>安装清单</strong>： </p>\n<ul>\n<li><p>（必装）</p>\n<blockquote>\n<p>d2l # 课程代码片段 </p>\n<p>torch # 深度学习框架 PyTorch </p>\n</blockquote>\n</li>\n<li><p>（可选）</p>\n<blockquote>\n<p> torchvision # PyTorch CV工具包 </p>\n<p> rise # Notebook Slides 插件</p>\n</blockquote>\n</li>\n</ul>\n<p><strong>🔗很有用的网站</strong></p>\n<ul>\n<li><a href=\"https://developer.aliyun.com/mirror/\">阿里巴巴开源镜像站-OPSX镜像站-阿里云开发者社区 (aliyun.com)</a></li>\n<li><a href=\"https://mirrors.cernet.edu.cn/list\">校园网联合镜像站 (cernet.edu.cn)</a></li>\n<li><a href=\"https://git-scm.com/download/\">Git 下载地址 (git-scm.com)</a></li>\n<li><a href=\"https://ghproxy.com/\">Github下载加速</a></li>\n<li><a href=\"https://missing-semester-cn.github.io/\">计算机教育中缺失的一课 </a></li>\n</ul>\n","site":{"data":{}},"cover":"/img/covers/1.jpg","cover_type":"img","excerpt":"","more":"<h1 id=\"Windows下D2L-Pytorch环境配置-CPU版\"><a href=\"#Windows下D2L-Pytorch环境配置-CPU版\" class=\"headerlink\" title=\"Windows下D2L_Pytorch环境配置(CPU版)\"></a>Windows下D2L_Pytorch环境配置(CPU版)</h1><h2 id=\"下载Miniconda\"><a href=\"#下载Miniconda\" class=\"headerlink\" title=\"下载Miniconda\"></a>下载Miniconda</h2><ul>\n<li><a href=\"https://docs.conda.io/en/latest/miniconda.html\">官网下载</a> </li>\n<li><a href=\"https://repo.anaconda.com/miniconda/Miniconda3-latest-Windows-x86_64.exe\">开源仓库下载</a></li>\n</ul>\n<h2 id=\"安装Miniconda：\"><a href=\"#安装Miniconda：\" class=\"headerlink\" title=\"安装Miniconda：\"></a>安装Miniconda：</h2><p><img src=\"/2023/03/20/%E8%B8%A9%E5%9D%91%E6%97%A5%E8%AE%B0/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/Windows%E4%B8%8BD2L-Pytorch%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-CPU%E7%89%88/image-20230320210330101.png\" alt=\"image-20230320210319260\"></p>\n<h2 id=\"conda更换镜像源\"><a href=\"#conda更换镜像源\" class=\"headerlink\" title=\"conda更换镜像源\"></a>conda更换镜像源</h2><p><a href=\"https://mirrors.sjtug.sjtu.edu.cn/docs/anaconda\">上海交通大学 Linux 用户组 软件镜像服务 Conda 镜像源</a> ：</p>\n<p>编辑<code>~/.condarc</code></p>\n<p>复制下列代码，粘贴到<code>~/.condarc</code>文件中</p>\n<figure class=\"highlight txt\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">default_channels:</span><br><span class=\"line\">  - https://mirror.sjtu.edu.cn/anaconda/pkgs/r</span><br><span class=\"line\">  - https://mirror.sjtu.edu.cn/anaconda/pkgs/main</span><br><span class=\"line\">custom_channels:</span><br><span class=\"line\">  conda-forge: https://mirror.sjtu.edu.cn/anaconda/cloud/</span><br><span class=\"line\">  pytorch: https://mirror.sjtu.edu.cn/anaconda/cloud/</span><br><span class=\"line\">channels:</span><br><span class=\"line\">  - defaults</span><br></pre></td></tr></table></figure>\n<h2 id=\"清除缓存\"><a href=\"#清除缓存\" class=\"headerlink\" title=\"清除缓存\"></a>清除缓存</h2><p><strong>Anaconda Powershell Prompt</strong>中输入：<code>conda clean -i</code></p>\n<h2 id=\"PyPi更换镜像源\"><a href=\"#PyPi更换镜像源\" class=\"headerlink\" title=\"PyPi更换镜像源\"></a>PyPi更换镜像源</h2><p><a href=\"https://mirrors.sjtug.sjtu.edu.cn/docs/pypi-packages\">上海交通大学 Linux 用户组 软件镜像服务 PyPI 镜像源</a></p>\n<p>复制下列代码，粘贴到<strong>Anaconda Powershell Prompt</strong>中运行：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip config <span class=\"built_in\">set</span> global.index-url https://mirror.sjtu.edu.cn/pypi/web/simple</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>如需安装GPU版本Pytorch镜像源，请在<strong>Anaconda Powershell Prompt</strong>中运行：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install torch===1.7.1 torchvision===0.8.2 torchaudio===0.7.2 -f https://mirror.sjtu.edu.cn/pytorch-wheels/torch_stable.html</span><br></pre></td></tr></table></figure>\n</blockquote>\n<h2 id=\"Conda的基础操作\"><a href=\"#Conda的基础操作\" class=\"headerlink\" title=\"Conda的基础操作\"></a>Conda的基础操作</h2><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda create -n d2l python=3.9 <span class=\"comment\"># conda 环境创建</span></span><br><span class=\"line\">conda activate d2l <span class=\"comment\"># 激活 d2l 环境，不同环境包版本不同！</span></span><br><span class=\"line\">conda deactivate <span class=\"comment\"># 退出该环境</span></span><br><span class=\"line\">conda remove -n d2l --all <span class=\"comment\"># 删除整个环境</span></span><br><span class=\"line\">conda list  <span class=\"comment\"># 查看当前环境在已安装的包</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"安装课程第三方库\"><a href=\"#安装课程第三方库\" class=\"headerlink\" title=\"安装课程第三方库\"></a>安装课程第三方库</h2><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install d2l torch <span class=\"comment\"># 必装库</span></span><br><span class=\"line\">pip install d2l torch torchvision rise <span class=\"comment\"># 所有库</span></span><br></pre></td></tr></table></figure>\n<p><strong>安装清单</strong>： </p>\n<ul>\n<li><p>（必装）</p>\n<blockquote>\n<p>d2l # 课程代码片段 </p>\n<p>torch # 深度学习框架 PyTorch </p>\n</blockquote>\n</li>\n<li><p>（可选）</p>\n<blockquote>\n<p> torchvision # PyTorch CV工具包 </p>\n<p> rise # Notebook Slides 插件</p>\n</blockquote>\n</li>\n</ul>\n<p><strong>🔗很有用的网站</strong></p>\n<ul>\n<li><a href=\"https://developer.aliyun.com/mirror/\">阿里巴巴开源镜像站-OPSX镜像站-阿里云开发者社区 (aliyun.com)</a></li>\n<li><a href=\"https://mirrors.cernet.edu.cn/list\">校园网联合镜像站 (cernet.edu.cn)</a></li>\n<li><a href=\"https://git-scm.com/download/\">Git 下载地址 (git-scm.com)</a></li>\n<li><a href=\"https://ghproxy.com/\">Github下载加速</a></li>\n<li><a href=\"https://missing-semester-cn.github.io/\">计算机教育中缺失的一课 </a></li>\n</ul>\n"},{"title":"Hello World","comments":1,"abbrlink":"4a17b156","date":"2023-01-01T13:02:30.000Z","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","source":"_posts/踩坑日记/Tips/hello-world.md","raw":"---\ntitle: Hello World\ncategories:\n  - 踩坑日记\n  - Tips\ncomments: true\nabbrlink: 4a17b156\ndate: 2023-01-01 21:02:30\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","slug":"踩坑日记/Tips/hello-world","published":1,"updated":"2023-03-21T11:34:15.919Z","layout":"post","photos":[],"link":"","_id":"clfjkj0c3000u8gsz3a8mbgs4","content":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n","site":{"data":{}},"cover":"/img/covers/9.jpg","cover_type":"img","excerpt":"","more":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n"},{"title":"Git-Bash秒变Linux","abbrlink":"e1b07774","date":"2023-03-21T03:11:01.000Z","_content":"\n*引用：[博客园-何其有静](https://www.cnblogs.com/heqiyoujing/p/10023084.html)*\n\n> 自从用上git后，不爽git bush很久了，今天心血来潮给他换件衣服。\n\n* ### 编辑配置文件\n\n    ```bash\n    vim ~/.bash_profile\n    ```\n\n* ###  添加配置代码\n\n    进入insert模式，添加：\n\n    ```bash\n    export PS1=\"\\\\u@windows:\\w\\$ \"\n    ```\n\n* ### 保存并退出\n\n    按住Esc，连续按两次大写的Z（！这个真的学到了），即可退出vim编辑页面。即可和linux下一样如果还能看到PC名称，则将上一步的代码，改为：\n\n    ```bash\n    export PS1=\"\\\\u@windows:\\w\\$(__git_ps1 '(%s)')\\$ \"\n    ```\n\n* ### 话不多说，上图\n\n    ![image-20230320230913167](Git-Bash秒变Linux/image-20230320230913167-16793685665802.png)\n\n* ### BUT\n\n    上述操作确实能解决Git用户title的一些问题，但也会出现新的问题，其中主要的问题在于：使用Git时，无法直观地查看当前分支名称。解决办法也很简单，在Git Bash中输入：\n\n    ```bash\n    bash\n    ```\n\n    即可变回原来的样式：\n\n    ![image-20230321164332935](Git-Bash秒变Linux/image-20230321164332935-16793882362101.png)\n\n    这样以后，不适用`git`时，就和`Linux`一样；使用git时只需`bash`一下即可。\n\n","source":"_posts/踩坑日记/Tips/Git-Bash秒变Linux.md","raw":"---\ntitle: Git-Bash秒变Linux\ncategories:\n  - 踩坑日记\n  - Tips\ntags:\n  - Linux\n  - Git Bush\nabbrlink: e1b07774\ndate: 2023-03-21 11:11:01\n---\n\n*引用：[博客园-何其有静](https://www.cnblogs.com/heqiyoujing/p/10023084.html)*\n\n> 自从用上git后，不爽git bush很久了，今天心血来潮给他换件衣服。\n\n* ### 编辑配置文件\n\n    ```bash\n    vim ~/.bash_profile\n    ```\n\n* ###  添加配置代码\n\n    进入insert模式，添加：\n\n    ```bash\n    export PS1=\"\\\\u@windows:\\w\\$ \"\n    ```\n\n* ### 保存并退出\n\n    按住Esc，连续按两次大写的Z（！这个真的学到了），即可退出vim编辑页面。即可和linux下一样如果还能看到PC名称，则将上一步的代码，改为：\n\n    ```bash\n    export PS1=\"\\\\u@windows:\\w\\$(__git_ps1 '(%s)')\\$ \"\n    ```\n\n* ### 话不多说，上图\n\n    ![image-20230320230913167](Git-Bash秒变Linux/image-20230320230913167-16793685665802.png)\n\n* ### BUT\n\n    上述操作确实能解决Git用户title的一些问题，但也会出现新的问题，其中主要的问题在于：使用Git时，无法直观地查看当前分支名称。解决办法也很简单，在Git Bash中输入：\n\n    ```bash\n    bash\n    ```\n\n    即可变回原来的样式：\n\n    ![image-20230321164332935](Git-Bash秒变Linux/image-20230321164332935-16793882362101.png)\n\n    这样以后，不适用`git`时，就和`Linux`一样；使用git时只需`bash`一下即可。\n\n","slug":"踩坑日记/Tips/Git-Bash秒变Linux","published":1,"updated":"2023-03-21T11:13:47.620Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clfjkj0c4000x8gsz552begtg","content":"<p><em>引用：<a href=\"https://www.cnblogs.com/heqiyoujing/p/10023084.html\">博客园-何其有静</a></em></p>\n<blockquote>\n<p>自从用上git后，不爽git bush很久了，今天心血来潮给他换件衣服。</p>\n</blockquote>\n<ul>\n<li><h3 id=\"编辑配置文件\"><a href=\"#编辑配置文件\" class=\"headerlink\" title=\"编辑配置文件\"></a>编辑配置文件</h3>  <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vim ~/.bash_profile</span><br></pre></td></tr></table></figure>\n</li>\n<li><h3 id=\"添加配置代码\"><a href=\"#添加配置代码\" class=\"headerlink\" title=\"添加配置代码\"></a>添加配置代码</h3><p>  进入insert模式，添加：</p>\n  <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">export</span> PS1=<span class=\"string\">&quot;\\\\u@windows:\\w\\$ &quot;</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><h3 id=\"保存并退出\"><a href=\"#保存并退出\" class=\"headerlink\" title=\"保存并退出\"></a>保存并退出</h3><p>  按住Esc，连续按两次大写的Z（！这个真的学到了），即可退出vim编辑页面。即可和linux下一样如果还能看到PC名称，则将上一步的代码，改为：</p>\n  <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">export</span> PS1=<span class=\"string\">&quot;\\\\u@windows:\\w\\$(__git_ps1 &#x27;(%s)&#x27;)\\$ &quot;</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><h3 id=\"话不多说，上图\"><a href=\"#话不多说，上图\" class=\"headerlink\" title=\"话不多说，上图\"></a>话不多说，上图</h3><p>  <img src=\"/2023/03/21/%E8%B8%A9%E5%9D%91%E6%97%A5%E8%AE%B0/Tips/Git-Bash%E7%A7%92%E5%8F%98Linux/image-20230320230913167-16793685665802.png\" alt=\"image-20230320230913167\"></p>\n</li>\n<li><h3 id=\"BUT\"><a href=\"#BUT\" class=\"headerlink\" title=\"BUT\"></a>BUT</h3><p>  上述操作确实能解决Git用户title的一些问题，但也会出现新的问题，其中主要的问题在于：使用Git时，无法直观地查看当前分支名称。解决办法也很简单，在Git Bash中输入：</p>\n  <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bash</span><br></pre></td></tr></table></figure>\n<p>  即可变回原来的样式：</p>\n<p>  <img src=\"/2023/03/21/%E8%B8%A9%E5%9D%91%E6%97%A5%E8%AE%B0/Tips/Git-Bash%E7%A7%92%E5%8F%98Linux/image-20230321164332935-16793882362101.png\" alt=\"image-20230321164332935\"></p>\n<p>  这样以后，不适用<code>git</code>时，就和<code>Linux</code>一样；使用git时只需<code>bash</code>一下即可。</p>\n</li>\n</ul>\n","site":{"data":{}},"cover":"/img/covers/8.jpg","cover_type":"img","excerpt":"","more":"<p><em>引用：<a href=\"https://www.cnblogs.com/heqiyoujing/p/10023084.html\">博客园-何其有静</a></em></p>\n<blockquote>\n<p>自从用上git后，不爽git bush很久了，今天心血来潮给他换件衣服。</p>\n</blockquote>\n<ul>\n<li><h3 id=\"编辑配置文件\"><a href=\"#编辑配置文件\" class=\"headerlink\" title=\"编辑配置文件\"></a>编辑配置文件</h3>  <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vim ~/.bash_profile</span><br></pre></td></tr></table></figure>\n</li>\n<li><h3 id=\"添加配置代码\"><a href=\"#添加配置代码\" class=\"headerlink\" title=\"添加配置代码\"></a>添加配置代码</h3><p>  进入insert模式，添加：</p>\n  <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">export</span> PS1=<span class=\"string\">&quot;\\\\u@windows:\\w\\$ &quot;</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><h3 id=\"保存并退出\"><a href=\"#保存并退出\" class=\"headerlink\" title=\"保存并退出\"></a>保存并退出</h3><p>  按住Esc，连续按两次大写的Z（！这个真的学到了），即可退出vim编辑页面。即可和linux下一样如果还能看到PC名称，则将上一步的代码，改为：</p>\n  <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">export</span> PS1=<span class=\"string\">&quot;\\\\u@windows:\\w\\$(__git_ps1 &#x27;(%s)&#x27;)\\$ &quot;</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><h3 id=\"话不多说，上图\"><a href=\"#话不多说，上图\" class=\"headerlink\" title=\"话不多说，上图\"></a>话不多说，上图</h3><p>  <img src=\"/2023/03/21/%E8%B8%A9%E5%9D%91%E6%97%A5%E8%AE%B0/Tips/Git-Bash%E7%A7%92%E5%8F%98Linux/image-20230320230913167-16793685665802.png\" alt=\"image-20230320230913167\"></p>\n</li>\n<li><h3 id=\"BUT\"><a href=\"#BUT\" class=\"headerlink\" title=\"BUT\"></a>BUT</h3><p>  上述操作确实能解决Git用户title的一些问题，但也会出现新的问题，其中主要的问题在于：使用Git时，无法直观地查看当前分支名称。解决办法也很简单，在Git Bash中输入：</p>\n  <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bash</span><br></pre></td></tr></table></figure>\n<p>  即可变回原来的样式：</p>\n<p>  <img src=\"/2023/03/21/%E8%B8%A9%E5%9D%91%E6%97%A5%E8%AE%B0/Tips/Git-Bash%E7%A7%92%E5%8F%98Linux/image-20230321164332935-16793882362101.png\" alt=\"image-20230321164332935\"></p>\n<p>  这样以后，不适用<code>git</code>时，就和<code>Linux</code>一样；使用git时只需<code>bash</code>一下即可。</p>\n</li>\n</ul>\n"},{"title":"Pandas数据处理","_content":"\n### 数据清洗\n\n","source":"_drafts/Pandas数据处理.md","raw":"---\ntitle: Pandas数据处理\ntags: \n  - Python\n  - Pandas\ncategories: \n  - 学习记录\n  - Python\n---\n\n### 数据清洗\n\n","slug":"Pandas数据处理","published":0,"date":"2023-03-31T07:10:45.655Z","updated":"2023-03-31T06:47:33.018Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clfw9fner0000v8szcmvn4kgy","content":"<h3 id=\"数据清洗\"><a href=\"#数据清洗\" class=\"headerlink\" title=\"数据清洗\"></a>数据清洗</h3>","site":{"data":{}},"cover":"/img/covers/10.jpg","cover_type":"img","excerpt":"","more":"<h3 id=\"数据清洗\"><a href=\"#数据清洗\" class=\"headerlink\" title=\"数据清洗\"></a>数据清洗</h3>"},{"_content":"# WSL迁移、更换wsl的默认位置\n\n> 下列命令均在`powershell`中执行：\n\n## 1. 关闭wsl\n\n```powershell\nwsl --shutdown\n```\n\n## 2. 导出WSL分发版\n\n我的分发版本是：ubuntu；迁移的地址为 F:\\WSL\\images\\my_ubuntu.tar\n\n```powershell\nwsl --export Ubuntu F:\\WSL\\images\\my_ubuntu.tar\n```\n\n## 3. 卸载当前分发版\n\n```powershell\nwsl --unregister Ubuntu\n```\n\n## 4. 导入tar文件\n\n```powershell\n wsl --import Ubuntu F:\\WSL\\my_ubuntu F:\\WSL\\images\\my_ubuntu.tar\n```\n\n## 5. 设置默认用户\n\nyour_name是你安装wsl时使用的用户名\n\n```powershell\nubuntu config --default-user your_username\n```\n\n","source":"_drafts/WSL位置迁移.md","raw":"# WSL迁移、更换wsl的默认位置\n\n> 下列命令均在`powershell`中执行：\n\n## 1. 关闭wsl\n\n```powershell\nwsl --shutdown\n```\n\n## 2. 导出WSL分发版\n\n我的分发版本是：ubuntu；迁移的地址为 F:\\WSL\\images\\my_ubuntu.tar\n\n```powershell\nwsl --export Ubuntu F:\\WSL\\images\\my_ubuntu.tar\n```\n\n## 3. 卸载当前分发版\n\n```powershell\nwsl --unregister Ubuntu\n```\n\n## 4. 导入tar文件\n\n```powershell\n wsl --import Ubuntu F:\\WSL\\my_ubuntu F:\\WSL\\images\\my_ubuntu.tar\n```\n\n## 5. 设置默认用户\n\nyour_name是你安装wsl时使用的用户名\n\n```powershell\nubuntu config --default-user your_username\n```\n\n","slug":"WSL位置迁移","published":0,"date":"2023-03-31T07:20:37.105Z","updated":"2023-03-31T07:20:37.115Z","title":"","comments":1,"layout":"post","photos":[],"link":"","_id":"clfw9fnev0001v8sz7ayw3z6p","content":"<h1 id=\"WSL迁移、更换wsl的默认位置\"><a href=\"#WSL迁移、更换wsl的默认位置\" class=\"headerlink\" title=\"WSL迁移、更换wsl的默认位置\"></a>WSL迁移、更换wsl的默认位置</h1><blockquote>\n<p>下列命令均在<code>powershell</code>中执行：</p>\n</blockquote>\n<h2 id=\"1-关闭wsl\"><a href=\"#1-关闭wsl\" class=\"headerlink\" title=\"1. 关闭wsl\"></a>1. 关闭wsl</h2><figure class=\"highlight powershell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wsl <span class=\"literal\">--shutdown</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"2-导出WSL分发版\"><a href=\"#2-导出WSL分发版\" class=\"headerlink\" title=\"2. 导出WSL分发版\"></a>2. 导出WSL分发版</h2><p>我的分发版本是：ubuntu；迁移的地址为 F:\\WSL\\images\\my_ubuntu.tar</p>\n<figure class=\"highlight powershell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wsl <span class=\"literal\">--export</span> Ubuntu F:\\WSL\\images\\my_ubuntu.tar</span><br></pre></td></tr></table></figure>\n<h2 id=\"3-卸载当前分发版\"><a href=\"#3-卸载当前分发版\" class=\"headerlink\" title=\"3. 卸载当前分发版\"></a>3. 卸载当前分发版</h2><figure class=\"highlight powershell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wsl <span class=\"literal\">--unregister</span> Ubuntu</span><br></pre></td></tr></table></figure>\n<h2 id=\"4-导入tar文件\"><a href=\"#4-导入tar文件\" class=\"headerlink\" title=\"4. 导入tar文件\"></a>4. 导入tar文件</h2><figure class=\"highlight powershell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wsl <span class=\"literal\">--import</span> Ubuntu F:\\WSL\\my_ubuntu F:\\WSL\\images\\my_ubuntu.tar</span><br></pre></td></tr></table></figure>\n<h2 id=\"5-设置默认用户\"><a href=\"#5-设置默认用户\" class=\"headerlink\" title=\"5. 设置默认用户\"></a>5. 设置默认用户</h2><p>your_name是你安装wsl时使用的用户名</p>\n<figure class=\"highlight powershell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ubuntu config <span class=\"literal\">--default-user</span> your_username</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"cover":"/img/covers/3.jpg","cover_type":"img","excerpt":"","more":"<h1 id=\"WSL迁移、更换wsl的默认位置\"><a href=\"#WSL迁移、更换wsl的默认位置\" class=\"headerlink\" title=\"WSL迁移、更换wsl的默认位置\"></a>WSL迁移、更换wsl的默认位置</h1><blockquote>\n<p>下列命令均在<code>powershell</code>中执行：</p>\n</blockquote>\n<h2 id=\"1-关闭wsl\"><a href=\"#1-关闭wsl\" class=\"headerlink\" title=\"1. 关闭wsl\"></a>1. 关闭wsl</h2><figure class=\"highlight powershell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wsl <span class=\"literal\">--shutdown</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"2-导出WSL分发版\"><a href=\"#2-导出WSL分发版\" class=\"headerlink\" title=\"2. 导出WSL分发版\"></a>2. 导出WSL分发版</h2><p>我的分发版本是：ubuntu；迁移的地址为 F:\\WSL\\images\\my_ubuntu.tar</p>\n<figure class=\"highlight powershell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wsl <span class=\"literal\">--export</span> Ubuntu F:\\WSL\\images\\my_ubuntu.tar</span><br></pre></td></tr></table></figure>\n<h2 id=\"3-卸载当前分发版\"><a href=\"#3-卸载当前分发版\" class=\"headerlink\" title=\"3. 卸载当前分发版\"></a>3. 卸载当前分发版</h2><figure class=\"highlight powershell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wsl <span class=\"literal\">--unregister</span> Ubuntu</span><br></pre></td></tr></table></figure>\n<h2 id=\"4-导入tar文件\"><a href=\"#4-导入tar文件\" class=\"headerlink\" title=\"4. 导入tar文件\"></a>4. 导入tar文件</h2><figure class=\"highlight powershell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wsl <span class=\"literal\">--import</span> Ubuntu F:\\WSL\\my_ubuntu F:\\WSL\\images\\my_ubuntu.tar</span><br></pre></td></tr></table></figure>\n<h2 id=\"5-设置默认用户\"><a href=\"#5-设置默认用户\" class=\"headerlink\" title=\"5. 设置默认用户\"></a>5. 设置默认用户</h2><p>your_name是你安装wsl时使用的用户名</p>\n<figure class=\"highlight powershell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ubuntu config <span class=\"literal\">--default-user</span> your_username</span><br></pre></td></tr></table></figure>\n"},{"_content":"# WSL更换清华源\n\n### 备份`sources.list`\n\n```bash\nsudo cp  /etc/apt/sources.list /etc/apt/sources.list.bak\n```\n\n### 打开`sources.list`\n\n```bash\nsudo vim /etc/apt/sources.list\n```\n\n### 更换`sources.list`内容\n\n打开这个网址：\n\n[ubuntu | 镜像站使用帮助 | 清华大学开源软件镜像站 | Tsinghua Open Source Mirror (zhihu.com)](https://link.zhihu.com/?target=https%3A//mirrors.tuna.tsinghua.edu.cn/help/ubuntu/)\n\n在里面选择符合你自己的Ubuntu版本，这里以22.04为例：\n\n```sources.list\n# 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-updates main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-updates main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-backports main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-backports main restricted universe multiverse\n\n# deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-security main restricted universe multiverse\n# # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-security main restricted universe multiverse\n\ndeb http://security.ubuntu.com/ubuntu/ jammy-security main restricted universe multiverse\n# deb-src http://security.ubuntu.com/ubuntu/ jammy-security main restricted universe multiverse\n\n# 预发布软件源，不建议启用\n# deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-proposed main restricted universe multiverse\n# # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-proposed main restricted universe multiverse\n```\n\n### 更新包\n\n```bash\nsudo apt update\n```","source":"_drafts/WSL更换清华源.md","raw":"# WSL更换清华源\n\n### 备份`sources.list`\n\n```bash\nsudo cp  /etc/apt/sources.list /etc/apt/sources.list.bak\n```\n\n### 打开`sources.list`\n\n```bash\nsudo vim /etc/apt/sources.list\n```\n\n### 更换`sources.list`内容\n\n打开这个网址：\n\n[ubuntu | 镜像站使用帮助 | 清华大学开源软件镜像站 | Tsinghua Open Source Mirror (zhihu.com)](https://link.zhihu.com/?target=https%3A//mirrors.tuna.tsinghua.edu.cn/help/ubuntu/)\n\n在里面选择符合你自己的Ubuntu版本，这里以22.04为例：\n\n```sources.list\n# 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-updates main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-updates main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-backports main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-backports main restricted universe multiverse\n\n# deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-security main restricted universe multiverse\n# # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-security main restricted universe multiverse\n\ndeb http://security.ubuntu.com/ubuntu/ jammy-security main restricted universe multiverse\n# deb-src http://security.ubuntu.com/ubuntu/ jammy-security main restricted universe multiverse\n\n# 预发布软件源，不建议启用\n# deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-proposed main restricted universe multiverse\n# # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-proposed main restricted universe multiverse\n```\n\n### 更新包\n\n```bash\nsudo apt update\n```","slug":"WSL更换清华源","published":0,"date":"2023-03-31T07:20:37.155Z","updated":"2023-03-31T07:20:37.155Z","title":"","comments":1,"layout":"post","photos":[],"link":"","_id":"clfw9fnf50009v8szenzj23py","content":"<h1 id=\"WSL更换清华源\"><a href=\"#WSL更换清华源\" class=\"headerlink\" title=\"WSL更换清华源\"></a>WSL更换清华源</h1><h3 id=\"备份sources-list\"><a href=\"#备份sources-list\" class=\"headerlink\" title=\"备份sources.list\"></a>备份<code>sources.list</code></h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo <span class=\"built_in\">cp</span>  /etc/apt/sources.list /etc/apt/sources.list.bak</span><br></pre></td></tr></table></figure>\n<h3 id=\"打开sources-list\"><a href=\"#打开sources-list\" class=\"headerlink\" title=\"打开sources.list\"></a>打开<code>sources.list</code></h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo vim /etc/apt/sources.list</span><br></pre></td></tr></table></figure>\n<h3 id=\"更换sources-list内容\"><a href=\"#更换sources-list内容\" class=\"headerlink\" title=\"更换sources.list内容\"></a>更换<code>sources.list</code>内容</h3><p>打开这个网址：</p>\n<p><a href=\"https://link.zhihu.com/?target=https%3A//mirrors.tuna.tsinghua.edu.cn/help/ubuntu/\">ubuntu | 镜像站使用帮助 | 清华大学开源软件镜像站 | Tsinghua Open Source Mirror (zhihu.com)</a></p>\n<p>在里面选择符合你自己的Ubuntu版本，这里以22.04为例：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释</span><br><span class=\"line\">deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy main restricted universe multiverse</span><br><span class=\"line\"># deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy main restricted universe multiverse</span><br><span class=\"line\">deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-updates main restricted universe multiverse</span><br><span class=\"line\"># deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-updates main restricted universe multiverse</span><br><span class=\"line\">deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-backports main restricted universe multiverse</span><br><span class=\"line\"># deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-backports main restricted universe multiverse</span><br><span class=\"line\"></span><br><span class=\"line\"># deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-security main restricted universe multiverse</span><br><span class=\"line\"># # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-security main restricted universe multiverse</span><br><span class=\"line\"></span><br><span class=\"line\">deb http://security.ubuntu.com/ubuntu/ jammy-security main restricted universe multiverse</span><br><span class=\"line\"># deb-src http://security.ubuntu.com/ubuntu/ jammy-security main restricted universe multiverse</span><br><span class=\"line\"></span><br><span class=\"line\"># 预发布软件源，不建议启用</span><br><span class=\"line\"># deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-proposed main restricted universe multiverse</span><br><span class=\"line\"># # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-proposed main restricted universe multiverse</span><br></pre></td></tr></table></figure>\n<h3 id=\"更新包\"><a href=\"#更新包\" class=\"headerlink\" title=\"更新包\"></a>更新包</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt update</span><br></pre></td></tr></table></figure>","site":{"data":{}},"cover":"/img/covers/9.jpg","cover_type":"img","excerpt":"","more":"<h1 id=\"WSL更换清华源\"><a href=\"#WSL更换清华源\" class=\"headerlink\" title=\"WSL更换清华源\"></a>WSL更换清华源</h1><h3 id=\"备份sources-list\"><a href=\"#备份sources-list\" class=\"headerlink\" title=\"备份sources.list\"></a>备份<code>sources.list</code></h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo <span class=\"built_in\">cp</span>  /etc/apt/sources.list /etc/apt/sources.list.bak</span><br></pre></td></tr></table></figure>\n<h3 id=\"打开sources-list\"><a href=\"#打开sources-list\" class=\"headerlink\" title=\"打开sources.list\"></a>打开<code>sources.list</code></h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo vim /etc/apt/sources.list</span><br></pre></td></tr></table></figure>\n<h3 id=\"更换sources-list内容\"><a href=\"#更换sources-list内容\" class=\"headerlink\" title=\"更换sources.list内容\"></a>更换<code>sources.list</code>内容</h3><p>打开这个网址：</p>\n<p><a href=\"https://link.zhihu.com/?target=https%3A//mirrors.tuna.tsinghua.edu.cn/help/ubuntu/\">ubuntu | 镜像站使用帮助 | 清华大学开源软件镜像站 | Tsinghua Open Source Mirror (zhihu.com)</a></p>\n<p>在里面选择符合你自己的Ubuntu版本，这里以22.04为例：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释</span><br><span class=\"line\">deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy main restricted universe multiverse</span><br><span class=\"line\"># deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy main restricted universe multiverse</span><br><span class=\"line\">deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-updates main restricted universe multiverse</span><br><span class=\"line\"># deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-updates main restricted universe multiverse</span><br><span class=\"line\">deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-backports main restricted universe multiverse</span><br><span class=\"line\"># deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-backports main restricted universe multiverse</span><br><span class=\"line\"></span><br><span class=\"line\"># deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-security main restricted universe multiverse</span><br><span class=\"line\"># # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-security main restricted universe multiverse</span><br><span class=\"line\"></span><br><span class=\"line\">deb http://security.ubuntu.com/ubuntu/ jammy-security main restricted universe multiverse</span><br><span class=\"line\"># deb-src http://security.ubuntu.com/ubuntu/ jammy-security main restricted universe multiverse</span><br><span class=\"line\"></span><br><span class=\"line\"># 预发布软件源，不建议启用</span><br><span class=\"line\"># deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-proposed main restricted universe multiverse</span><br><span class=\"line\"># # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-proposed main restricted universe multiverse</span><br></pre></td></tr></table></figure>\n<h3 id=\"更新包\"><a href=\"#更新包\" class=\"headerlink\" title=\"更新包\"></a>更新包</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt update</span><br></pre></td></tr></table></figure>"},{"_content":"# WSL配置`zsh`\n\n## zsh\n\n### **安装**：\n\n```bash\nsudo apt-get install zsh\n```\n\n### **查看可以用的**`shell`\n\n```bash\ncat /etc/shells\n```\n\n### **更改你的默认** `Shell`\n\n```bash\nchsh -s /bin/zsh\n```\n\n## oh-my-zsh\n\n### **安装**：\n\n```bash\n# 用curl\nsh -c \"$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\n```\n\n或者\n\n```bash\n# 用wget\nsh -c \"$(wget https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh -O -)\"\n```\n\n### **修改主题**\n\n内置主题已经放在 `～/.oh-my-zsh/themes` 目录下\n\n想要修改使用 vim 编辑 `.zshrc`，键入以下内容并保存：\n\n```bash\nZSH_THEME=\" \"\n```\n\n### 安装插件\n\n#### zsh-autosuggestions\n\n* 把插件下载到本地的 `~/.oh-my-zsh/custom/plugins` 目录：\n\n```bash\ngit clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions\n```\n\n* 在 `.zshrc` 中，把 `zsh-autosuggestions` 加入插件列表：\n\n```bash\nplugins=(\n    # other plugins...\n    zsh-autosuggestions  # 插件之间使用空格隔开\n)\n```\n\n* 开启新的 Shell 或执行 `souce ~/.zshrc`，就可以开始体验插件。\n\n#### zsh-syntax-highlighting\n\n* 把插件下载到本地的 `~/.oh-my-zsh/custom/plugins` 目录:\n\n```bash\ngit clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting \n```\n\n* 在 `.zshrc` 中，把 `zsh-syntax-highlighting` 加入插件列表：\n\n```bash\nplugins=(\n    # other plugins...\n    zsh-autosuggestions\n    zsh-syntax-highlighting\n)\n```\n\n* 开启新的 Shell 或执行 `souce ~/.zshrc`，就可以开始体验插件了。\n\n#### z\n\n* 由于 oh-my-zsh 内置了 z 插件，所以只需要在 `.zshrc` 中，把 z 加入插件列表：\n\n```bash\nplugins=(\n     # other plugins...\n     zsh-autosuggestions\n     zsh-syntax-highlighting\n     z\n)\n```\n\n* 开启新的 Shell 或执行 `souce ~/.zshrc`，就可以开始体验插件了。\n\n### 设置 alias\n\n类似于`cd ~/projects/alicode/blog`命令这种：\n\n* 在 `.zshrc` 中键入：\n\n```bash\nalias cdblog=\"cd ~/projects/alicode/blog\" \n```\n\n* 开启新的 Shell 或 `souce ~/.zshrc`，以使配置生效。生效后就可以使用 `cdblog` 进行跳转了","source":"_drafts/WSL配置zsh.md","raw":"# WSL配置`zsh`\n\n## zsh\n\n### **安装**：\n\n```bash\nsudo apt-get install zsh\n```\n\n### **查看可以用的**`shell`\n\n```bash\ncat /etc/shells\n```\n\n### **更改你的默认** `Shell`\n\n```bash\nchsh -s /bin/zsh\n```\n\n## oh-my-zsh\n\n### **安装**：\n\n```bash\n# 用curl\nsh -c \"$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\n```\n\n或者\n\n```bash\n# 用wget\nsh -c \"$(wget https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh -O -)\"\n```\n\n### **修改主题**\n\n内置主题已经放在 `～/.oh-my-zsh/themes` 目录下\n\n想要修改使用 vim 编辑 `.zshrc`，键入以下内容并保存：\n\n```bash\nZSH_THEME=\" \"\n```\n\n### 安装插件\n\n#### zsh-autosuggestions\n\n* 把插件下载到本地的 `~/.oh-my-zsh/custom/plugins` 目录：\n\n```bash\ngit clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions\n```\n\n* 在 `.zshrc` 中，把 `zsh-autosuggestions` 加入插件列表：\n\n```bash\nplugins=(\n    # other plugins...\n    zsh-autosuggestions  # 插件之间使用空格隔开\n)\n```\n\n* 开启新的 Shell 或执行 `souce ~/.zshrc`，就可以开始体验插件。\n\n#### zsh-syntax-highlighting\n\n* 把插件下载到本地的 `~/.oh-my-zsh/custom/plugins` 目录:\n\n```bash\ngit clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting \n```\n\n* 在 `.zshrc` 中，把 `zsh-syntax-highlighting` 加入插件列表：\n\n```bash\nplugins=(\n    # other plugins...\n    zsh-autosuggestions\n    zsh-syntax-highlighting\n)\n```\n\n* 开启新的 Shell 或执行 `souce ~/.zshrc`，就可以开始体验插件了。\n\n#### z\n\n* 由于 oh-my-zsh 内置了 z 插件，所以只需要在 `.zshrc` 中，把 z 加入插件列表：\n\n```bash\nplugins=(\n     # other plugins...\n     zsh-autosuggestions\n     zsh-syntax-highlighting\n     z\n)\n```\n\n* 开启新的 Shell 或执行 `souce ~/.zshrc`，就可以开始体验插件了。\n\n### 设置 alias\n\n类似于`cd ~/projects/alicode/blog`命令这种：\n\n* 在 `.zshrc` 中键入：\n\n```bash\nalias cdblog=\"cd ~/projects/alicode/blog\" \n```\n\n* 开启新的 Shell 或 `souce ~/.zshrc`，以使配置生效。生效后就可以使用 `cdblog` 进行跳转了","slug":"WSL配置zsh","published":0,"date":"2023-03-31T07:20:37.155Z","updated":"2023-03-31T07:20:37.155Z","title":"","comments":1,"layout":"post","photos":[],"link":"","_id":"clfw9fnf6000av8szb8bc26nm","content":"<h1 id=\"WSL配置zsh\"><a href=\"#WSL配置zsh\" class=\"headerlink\" title=\"WSL配置zsh\"></a>WSL配置<code>zsh</code></h1><h2 id=\"zsh\"><a href=\"#zsh\" class=\"headerlink\" title=\"zsh\"></a>zsh</h2><h3 id=\"安装：\"><a href=\"#安装：\" class=\"headerlink\" title=\"安装：\"></a><strong>安装</strong>：</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get install zsh</span><br></pre></td></tr></table></figure>\n<h3 id=\"查看可以用的shell\"><a href=\"#查看可以用的shell\" class=\"headerlink\" title=\"查看可以用的shell\"></a><strong>查看可以用的</strong><code>shell</code></h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">cat</span> /etc/shells</span><br></pre></td></tr></table></figure>\n<h3 id=\"更改你的默认-Shell\"><a href=\"#更改你的默认-Shell\" class=\"headerlink\" title=\"更改你的默认 Shell\"></a><strong>更改你的默认</strong> <code>Shell</code></h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">chsh -s /bin/zsh</span><br></pre></td></tr></table></figure>\n<h2 id=\"oh-my-zsh\"><a href=\"#oh-my-zsh\" class=\"headerlink\" title=\"oh-my-zsh\"></a>oh-my-zsh</h2><h3 id=\"安装：-1\"><a href=\"#安装：-1\" class=\"headerlink\" title=\"安装：\"></a><strong>安装</strong>：</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 用curl</span></span><br><span class=\"line\">sh -c <span class=\"string\">&quot;<span class=\"subst\">$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)</span>&quot;</span></span><br></pre></td></tr></table></figure>\n<p>或者</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 用wget</span></span><br><span class=\"line\">sh -c <span class=\"string\">&quot;<span class=\"subst\">$(wget https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh -O -)</span>&quot;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"修改主题\"><a href=\"#修改主题\" class=\"headerlink\" title=\"修改主题\"></a><strong>修改主题</strong></h3><p>内置主题已经放在 <code>～/.oh-my-zsh/themes</code> 目录下</p>\n<p>想要修改使用 vim 编辑 <code>.zshrc</code>，键入以下内容并保存：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ZSH_THEME=<span class=\"string\">&quot; &quot;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"安装插件\"><a href=\"#安装插件\" class=\"headerlink\" title=\"安装插件\"></a>安装插件</h3><h4 id=\"zsh-autosuggestions\"><a href=\"#zsh-autosuggestions\" class=\"headerlink\" title=\"zsh-autosuggestions\"></a>zsh-autosuggestions</h4><ul>\n<li>把插件下载到本地的 <code>~/.oh-my-zsh/custom/plugins</code> 目录：</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git <span class=\"built_in\">clone</span> https://github.com/zsh-users/zsh-autosuggestions <span class=\"variable\">$&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;</span>/plugins/zsh-autosuggestions</span><br></pre></td></tr></table></figure>\n<ul>\n<li>在 <code>.zshrc</code> 中，把 <code>zsh-autosuggestions</code> 加入插件列表：</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plugins=(</span><br><span class=\"line\">    <span class=\"comment\"># other plugins...</span></span><br><span class=\"line\">    zsh-autosuggestions  <span class=\"comment\"># 插件之间使用空格隔开</span></span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n<ul>\n<li>开启新的 Shell 或执行 <code>souce ~/.zshrc</code>，就可以开始体验插件。</li>\n</ul>\n<h4 id=\"zsh-syntax-highlighting\"><a href=\"#zsh-syntax-highlighting\" class=\"headerlink\" title=\"zsh-syntax-highlighting\"></a>zsh-syntax-highlighting</h4><ul>\n<li>把插件下载到本地的 <code>~/.oh-my-zsh/custom/plugins</code> 目录:</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git <span class=\"built_in\">clone</span> https://github.com/zsh-users/zsh-syntax-highlighting.git <span class=\"variable\">$&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;</span>/plugins/zsh-syntax-highlighting </span><br></pre></td></tr></table></figure>\n<ul>\n<li>在 <code>.zshrc</code> 中，把 <code>zsh-syntax-highlighting</code> 加入插件列表：</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plugins=(</span><br><span class=\"line\">    <span class=\"comment\"># other plugins...</span></span><br><span class=\"line\">    zsh-autosuggestions</span><br><span class=\"line\">    zsh-syntax-highlighting</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n<ul>\n<li>开启新的 Shell 或执行 <code>souce ~/.zshrc</code>，就可以开始体验插件了。</li>\n</ul>\n<h4 id=\"z\"><a href=\"#z\" class=\"headerlink\" title=\"z\"></a>z</h4><ul>\n<li>由于 oh-my-zsh 内置了 z 插件，所以只需要在 <code>.zshrc</code> 中，把 z 加入插件列表：</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plugins=(</span><br><span class=\"line\">     <span class=\"comment\"># other plugins...</span></span><br><span class=\"line\">     zsh-autosuggestions</span><br><span class=\"line\">     zsh-syntax-highlighting</span><br><span class=\"line\">     z</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n<ul>\n<li>开启新的 Shell 或执行 <code>souce ~/.zshrc</code>，就可以开始体验插件了。</li>\n</ul>\n<h3 id=\"设置-alias\"><a href=\"#设置-alias\" class=\"headerlink\" title=\"设置 alias\"></a>设置 alias</h3><p>类似于<code>cd ~/projects/alicode/blog</code>命令这种：</p>\n<ul>\n<li>在 <code>.zshrc</code> 中键入：</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">alias</span> cdblog=<span class=\"string\">&quot;cd ~/projects/alicode/blog&quot;</span> </span><br></pre></td></tr></table></figure>\n<ul>\n<li>开启新的 Shell 或 <code>souce ~/.zshrc</code>，以使配置生效。生效后就可以使用 <code>cdblog</code> 进行跳转了</li>\n</ul>\n","site":{"data":{}},"cover":"/img/covers/9.jpg","cover_type":"img","excerpt":"","more":"<h1 id=\"WSL配置zsh\"><a href=\"#WSL配置zsh\" class=\"headerlink\" title=\"WSL配置zsh\"></a>WSL配置<code>zsh</code></h1><h2 id=\"zsh\"><a href=\"#zsh\" class=\"headerlink\" title=\"zsh\"></a>zsh</h2><h3 id=\"安装：\"><a href=\"#安装：\" class=\"headerlink\" title=\"安装：\"></a><strong>安装</strong>：</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get install zsh</span><br></pre></td></tr></table></figure>\n<h3 id=\"查看可以用的shell\"><a href=\"#查看可以用的shell\" class=\"headerlink\" title=\"查看可以用的shell\"></a><strong>查看可以用的</strong><code>shell</code></h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">cat</span> /etc/shells</span><br></pre></td></tr></table></figure>\n<h3 id=\"更改你的默认-Shell\"><a href=\"#更改你的默认-Shell\" class=\"headerlink\" title=\"更改你的默认 Shell\"></a><strong>更改你的默认</strong> <code>Shell</code></h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">chsh -s /bin/zsh</span><br></pre></td></tr></table></figure>\n<h2 id=\"oh-my-zsh\"><a href=\"#oh-my-zsh\" class=\"headerlink\" title=\"oh-my-zsh\"></a>oh-my-zsh</h2><h3 id=\"安装：-1\"><a href=\"#安装：-1\" class=\"headerlink\" title=\"安装：\"></a><strong>安装</strong>：</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 用curl</span></span><br><span class=\"line\">sh -c <span class=\"string\">&quot;<span class=\"subst\">$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)</span>&quot;</span></span><br></pre></td></tr></table></figure>\n<p>或者</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 用wget</span></span><br><span class=\"line\">sh -c <span class=\"string\">&quot;<span class=\"subst\">$(wget https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh -O -)</span>&quot;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"修改主题\"><a href=\"#修改主题\" class=\"headerlink\" title=\"修改主题\"></a><strong>修改主题</strong></h3><p>内置主题已经放在 <code>～/.oh-my-zsh/themes</code> 目录下</p>\n<p>想要修改使用 vim 编辑 <code>.zshrc</code>，键入以下内容并保存：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ZSH_THEME=<span class=\"string\">&quot; &quot;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"安装插件\"><a href=\"#安装插件\" class=\"headerlink\" title=\"安装插件\"></a>安装插件</h3><h4 id=\"zsh-autosuggestions\"><a href=\"#zsh-autosuggestions\" class=\"headerlink\" title=\"zsh-autosuggestions\"></a>zsh-autosuggestions</h4><ul>\n<li>把插件下载到本地的 <code>~/.oh-my-zsh/custom/plugins</code> 目录：</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git <span class=\"built_in\">clone</span> https://github.com/zsh-users/zsh-autosuggestions <span class=\"variable\">$&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;</span>/plugins/zsh-autosuggestions</span><br></pre></td></tr></table></figure>\n<ul>\n<li>在 <code>.zshrc</code> 中，把 <code>zsh-autosuggestions</code> 加入插件列表：</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plugins=(</span><br><span class=\"line\">    <span class=\"comment\"># other plugins...</span></span><br><span class=\"line\">    zsh-autosuggestions  <span class=\"comment\"># 插件之间使用空格隔开</span></span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n<ul>\n<li>开启新的 Shell 或执行 <code>souce ~/.zshrc</code>，就可以开始体验插件。</li>\n</ul>\n<h4 id=\"zsh-syntax-highlighting\"><a href=\"#zsh-syntax-highlighting\" class=\"headerlink\" title=\"zsh-syntax-highlighting\"></a>zsh-syntax-highlighting</h4><ul>\n<li>把插件下载到本地的 <code>~/.oh-my-zsh/custom/plugins</code> 目录:</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git <span class=\"built_in\">clone</span> https://github.com/zsh-users/zsh-syntax-highlighting.git <span class=\"variable\">$&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;</span>/plugins/zsh-syntax-highlighting </span><br></pre></td></tr></table></figure>\n<ul>\n<li>在 <code>.zshrc</code> 中，把 <code>zsh-syntax-highlighting</code> 加入插件列表：</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plugins=(</span><br><span class=\"line\">    <span class=\"comment\"># other plugins...</span></span><br><span class=\"line\">    zsh-autosuggestions</span><br><span class=\"line\">    zsh-syntax-highlighting</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n<ul>\n<li>开启新的 Shell 或执行 <code>souce ~/.zshrc</code>，就可以开始体验插件了。</li>\n</ul>\n<h4 id=\"z\"><a href=\"#z\" class=\"headerlink\" title=\"z\"></a>z</h4><ul>\n<li>由于 oh-my-zsh 内置了 z 插件，所以只需要在 <code>.zshrc</code> 中，把 z 加入插件列表：</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plugins=(</span><br><span class=\"line\">     <span class=\"comment\"># other plugins...</span></span><br><span class=\"line\">     zsh-autosuggestions</span><br><span class=\"line\">     zsh-syntax-highlighting</span><br><span class=\"line\">     z</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n<ul>\n<li>开启新的 Shell 或执行 <code>souce ~/.zshrc</code>，就可以开始体验插件了。</li>\n</ul>\n<h3 id=\"设置-alias\"><a href=\"#设置-alias\" class=\"headerlink\" title=\"设置 alias\"></a>设置 alias</h3><p>类似于<code>cd ~/projects/alicode/blog</code>命令这种：</p>\n<ul>\n<li>在 <code>.zshrc</code> 中键入：</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">alias</span> cdblog=<span class=\"string\">&quot;cd ~/projects/alicode/blog&quot;</span> </span><br></pre></td></tr></table></figure>\n<ul>\n<li>开启新的 Shell 或 <code>souce ~/.zshrc</code>，以使配置生效。生效后就可以使用 <code>cdblog</code> 进行跳转了</li>\n</ul>\n"}],"PostAsset":[{"_id":"source/_posts/学习记录/考研/计算机网络/image-20230205135027022.png","slug":"image-20230205135027022.png","post":"clfjkj0bm000a8gsz8cfgf6f9","modified":0,"renderable":0},{"_id":"source/_posts/学习记录/考研/计算机网络/image-20230205152246881.png","slug":"image-20230205152246881.png","post":"clfjkj0bm000a8gsz8cfgf6f9","modified":0,"renderable":0},{"_id":"source/_posts/学习记录/考研/微机原理及接口技术/image-20230308223135359.png","slug":"image-20230308223135359.png","post":"clfjkj0br000d8gsz1xpr35nt","modified":0,"renderable":0},{"_id":"source/_posts/学习记录/考研/微机原理及接口技术/image-20230308232215011-16782889368582.png","slug":"image-20230308232215011-16782889368582.png","post":"clfjkj0br000d8gsz1xpr35nt","modified":0,"renderable":0},{"_id":"source/_posts/学习记录/考研/微机原理及接口技术/image-20230308232215011.png","slug":"image-20230308232215011.png","post":"clfjkj0br000d8gsz1xpr35nt","modified":0,"renderable":0},{"_id":"source/_posts/学习记录/考研/微机原理及接口技术/微机原理及接口技术.jpg","slug":"微机原理及接口技术.jpg","post":"clfjkj0br000d8gsz1xpr35nt","modified":0,"renderable":0},{"_id":"source/_posts/学习记录/DataWhale/D2L_ch02_预备知识/output_7_0-16794727634302.svg","slug":"output_7_0-16794727634302.svg","post":"clfjkj0bt000e8gsz69jyfuie","modified":0,"renderable":0},{"_id":"source/_posts/学习记录/DataWhale/D2L_ch02_预备知识/output_7_0.svg","slug":"output_7_0.svg","post":"clfjkj0bt000e8gsz69jyfuie","modified":0,"renderable":0},{"_id":"source/_posts/学习记录/DataWhale/D2L_Ch01_深度学习的相关概念/image-20230319170611582.png","slug":"image-20230319170611582.png","post":"clfjkj0bw000h8gszc81rdz2k","modified":0,"renderable":0},{"_id":"source/_posts/学习记录/DataWhale/D2L_Ch01_深度学习的相关概念/image-20230319174247282.png","slug":"image-20230319174247282.png","post":"clfjkj0bw000h8gszc81rdz2k","modified":0,"renderable":0},{"_id":"source/_posts/学习记录/DataWhale/D2L_Ch01_深度学习的相关概念/image-20230319174557369.png","slug":"image-20230319174557369.png","post":"clfjkj0bw000h8gszc81rdz2k","modified":0,"renderable":0},{"_id":"source/_posts/学习记录/DataWhale/D2L_Ch01_深度学习的相关概念/image-20230319192839845.png","slug":"image-20230319192839845.png","post":"clfjkj0bw000h8gszc81rdz2k","modified":0,"renderable":0},{"_id":"source/_posts/学习记录/DataWhale/D2L_Ch01_深度学习的相关概念/image-20230319212918986.png","slug":"image-20230319212918986.png","post":"clfjkj0bw000h8gszc81rdz2k","modified":0,"renderable":0},{"_id":"source/_posts/学习记录/DataWhale/DataWhale学习记录/CV入门-16791573687564.jpg","slug":"CV入门-16791573687564.jpg","post":"clfjkj0bz000n8gszedk7g7pw","modified":0,"renderable":0},{"_id":"source/_posts/学习记录/DataWhale/DataWhale学习记录/CV入门.jpg","slug":"CV入门.jpg","post":"clfjkj0bz000n8gszedk7g7pw","modified":0,"renderable":0},{"_id":"source/_posts/学习记录/DataWhale/DataWhale学习记录/image-20230318220521147.png","slug":"image-20230318220521147.png","post":"clfjkj0bz000n8gszedk7g7pw","modified":0,"renderable":0},{"_id":"source/_posts/学习记录/DataWhale/DataWhale学习记录/人工智能路线图-16791573386491.jpg","slug":"人工智能路线图-16791573386491.jpg","post":"clfjkj0bz000n8gszedk7g7pw","modified":0,"renderable":0},{"_id":"source/_posts/学习记录/DataWhale/DataWhale学习记录/人工智能路线图-16791581610352.jpg","slug":"人工智能路线图-16791581610352.jpg","post":"clfjkj0bz000n8gszedk7g7pw","modified":0,"renderable":0},{"_id":"source/_posts/学习记录/DataWhale/DataWhale学习记录/人工智能路线图.jpg","slug":"人工智能路线图.jpg","post":"clfjkj0bz000n8gszedk7g7pw","modified":0,"renderable":0},{"_id":"source/_posts/学习记录/DataWhale/DataWhale学习记录/深度学习小技巧-167915739504012.jpg","slug":"深度学习小技巧-167915739504012.jpg","post":"clfjkj0bz000n8gszedk7g7pw","modified":0,"renderable":0},{"_id":"source/_posts/学习记录/DataWhale/DataWhale学习记录/深度学习小技巧.jpg","slug":"深度学习小技巧.jpg","post":"clfjkj0bz000n8gszedk7g7pw","modified":0,"renderable":0},{"_id":"source/_posts/学习记录/DataWhale/DataWhale学习记录/深度学习小技巧2.jpg","slug":"深度学习小技巧2.jpg","post":"clfjkj0bz000n8gszedk7g7pw","modified":0,"renderable":0},{"_id":"source/_posts/学习记录/DataWhale/DataWhale学习记录/深度学习小技巧3-167915739841715.jpg","slug":"深度学习小技巧3-167915739841715.jpg","post":"clfjkj0bz000n8gszedk7g7pw","modified":0,"renderable":0},{"_id":"source/_posts/学习记录/DataWhale/DataWhale学习记录/深度学习小技巧3.jpg","slug":"深度学习小技巧3.jpg","post":"clfjkj0bz000n8gszedk7g7pw","modified":0,"renderable":0},{"_id":"source/_posts/学习记录/DataWhale/DataWhale学习记录/深度学习小技巧4.jpg","slug":"深度学习小技巧4.jpg","post":"clfjkj0bz000n8gszedk7g7pw","modified":0,"renderable":0},{"_id":"source/_posts/学习记录/DataWhale/DataWhale学习记录/深度学习小技巧5-167915740039318.jpg","slug":"深度学习小技巧5-167915740039318.jpg","post":"clfjkj0bz000n8gszedk7g7pw","modified":0,"renderable":0},{"_id":"source/_posts/学习记录/DataWhale/DataWhale学习记录/深度学习小技巧5.jpg","slug":"深度学习小技巧5.jpg","post":"clfjkj0bz000n8gszedk7g7pw","modified":0,"renderable":0},{"_id":"source/_posts/学习记录/DataWhale/DataWhale学习记录/深度学习细分领域-167915739206110.jpg","slug":"深度学习细分领域-167915739206110.jpg","post":"clfjkj0bz000n8gszedk7g7pw","modified":0,"renderable":0},{"_id":"source/_posts/学习记录/DataWhale/DataWhale学习记录/深度学习细分领域.jpg","slug":"深度学习细分领域.jpg","post":"clfjkj0bz000n8gszedk7g7pw","modified":0,"renderable":0},{"_id":"source/_posts/学习记录/DataWhale/DataWhale学习记录/科研经验-16791573889377.jpg","slug":"科研经验-16791573889377.jpg","post":"clfjkj0bz000n8gszedk7g7pw","modified":0,"renderable":0},{"_id":"source/_posts/学习记录/DataWhale/DataWhale学习记录/科研经验.jpg","slug":"科研经验.jpg","post":"clfjkj0bz000n8gszedk7g7pw","modified":0,"renderable":0},{"_id":"source/_posts/踩坑日记/环境配置/Windows下D2L-Pytorch环境配置-CPU版/image-20230320210330101.png","slug":"image-20230320210330101.png","post":"clfjkj0c1000p8gsz0fb37e8i","modified":0,"renderable":0},{"_id":"source/_posts/踩坑日记/Tips/Git-Bash秒变Linux/image-20230320230913167-16793685665802.png","slug":"image-20230320230913167-16793685665802.png","post":"clfjkj0c4000x8gsz552begtg","modified":0,"renderable":0},{"_id":"source/_posts/踩坑日记/Tips/Git-Bash秒变Linux/image-20230320230913167.png","slug":"image-20230320230913167.png","post":"clfjkj0c4000x8gsz552begtg","modified":0,"renderable":0},{"_id":"source/_posts/踩坑日记/Tips/Git-Bash秒变Linux/image-20230321164332935-16793882362101.png","slug":"image-20230321164332935-16793882362101.png","post":"clfjkj0c4000x8gsz552begtg","modified":0,"renderable":0},{"_id":"source/_posts/踩坑日记/Tips/Git-Bash秒变Linux/image-20230321164332935.png","slug":"image-20230321164332935.png","post":"clfjkj0c4000x8gsz552begtg","modified":0,"renderable":0}],"PostCategory":[{"post_id":"clfjkj0bw000h8gszc81rdz2k","category_id":"clfjkj0bj00068gsz2a84angh","_id":"clfjkj0c2000r8gszdt799tr4"},{"post_id":"clfjkj0bw000h8gszc81rdz2k","category_id":"clfjkj0bu000f8gszdmx0a8g6","_id":"clfjkj0c4000w8gszgge9d388"},{"post_id":"clfjkj0bz000n8gszedk7g7pw","category_id":"clfjkj0bj00068gsz2a84angh","_id":"clfjkj0c800158gszf0chhomq"},{"post_id":"clfjkj0bz000n8gszedk7g7pw","category_id":"clfjkj0bu000f8gszdmx0a8g6","_id":"clfjkj0ca00198gszdzos02ho"},{"post_id":"clfjkj0bm000a8gsz8cfgf6f9","category_id":"clfjkj0bj00068gsz2a84angh","_id":"clfjkj0ca001a8gsz8jbw3dfc"},{"post_id":"clfjkj0bm000a8gsz8cfgf6f9","category_id":"clfjkj0by000l8gsz2tw8678c","_id":"clfjkj0cb001d8gsz46o2ebq8"},{"post_id":"clfjkj0br000d8gsz1xpr35nt","category_id":"clfjkj0bj00068gsz2a84angh","_id":"clfjkj0cb001e8gszcdsy374p"},{"post_id":"clfjkj0br000d8gsz1xpr35nt","category_id":"clfjkj0by000l8gsz2tw8678c","_id":"clfjkj0cc001g8gszeevb7zua"},{"post_id":"clfjkj0bt000e8gsz69jyfuie","category_id":"clfjkj0bj00068gsz2a84angh","_id":"clfjkj0cd001p8gsz7mi7332u"},{"post_id":"clfjkj0bt000e8gsz69jyfuie","category_id":"clfjkj0bu000f8gszdmx0a8g6","_id":"clfjkj0ce001s8gsz1fns5qu7"},{"post_id":"clfjkj0bx000j8gsz0pzga2oz","category_id":"clfjkj0ca001b8gsz52d6ginw","_id":"clfjkj0cg00238gsz3ffmbn07"},{"post_id":"clfjkj0bx000j8gsz0pzga2oz","category_id":"clfjkj0ce001v8gsz6tug7uv0","_id":"clfjkj0cg00258gszdqhu36k2"},{"post_id":"clfjkj0c1000p8gsz0fb37e8i","category_id":"clfjkj0ca001b8gsz52d6ginw","_id":"clfjkj0cg00288gsze20vh885"},{"post_id":"clfjkj0c1000p8gsz0fb37e8i","category_id":"clfjkj0cf001z8gsze5gi7v3u","_id":"clfjkj0ch002a8gsz2lpo4sxn"},{"post_id":"clfjkj0c3000u8gsz3a8mbgs4","category_id":"clfjkj0ca001b8gsz52d6ginw","_id":"clfjkj0ch002b8gsz2gk2dbul"},{"post_id":"clfjkj0c3000u8gsz3a8mbgs4","category_id":"clfjkj0ce001v8gsz6tug7uv0","_id":"clfjkj0ch002c8gsza1708am8"},{"post_id":"clfjkj0c4000x8gsz552begtg","category_id":"clfjkj0ca001b8gsz52d6ginw","_id":"clfjkj0ch002d8gszfn4h0sbq"},{"post_id":"clfjkj0c4000x8gsz552begtg","category_id":"clfjkj0ce001v8gsz6tug7uv0","_id":"clfjkj0ch002e8gsz4h1qc7r8"},{"post_id":"clfw9fner0000v8szcmvn4kgy","category_id":"clfjkj0bj00068gsz2a84angh","_id":"clfw9fnf30005v8sz4u302toj"},{"post_id":"clfw9fner0000v8szcmvn4kgy","category_id":"clfw9fnf10003v8szcs4q21qt","_id":"clfw9fnf40007v8sz2w5jdlc7"}],"PostTag":[{"post_id":"clfjkj0bt000e8gsz69jyfuie","tag_id":"clfjkj0bj00078gszanxf1k7l","_id":"clfjkj0c0000o8gszg8ohg5wu"},{"post_id":"clfjkj0bt000e8gsz69jyfuie","tag_id":"clfjkj0bn000c8gsz275r83ef","_id":"clfjkj0c2000q8gszfu2vemz0"},{"post_id":"clfjkj0bw000h8gszc81rdz2k","tag_id":"clfjkj0bn000c8gsz275r83ef","_id":"clfjkj0c4000v8gszb9te53yx"},{"post_id":"clfjkj0bw000h8gszc81rdz2k","tag_id":"clfjkj0bj00078gszanxf1k7l","_id":"clfjkj0c5000y8gsz3m7lbvmp"},{"post_id":"clfjkj0bm000a8gsz8cfgf6f9","tag_id":"clfjkj0c2000s8gszee8q9zin","_id":"clfjkj0c900188gsz074y6ux3"},{"post_id":"clfjkj0br000d8gsz1xpr35nt","tag_id":"clfjkj0c600118gsz1a3val8h","_id":"clfjkj0cc001i8gsz2k1ec4g2"},{"post_id":"clfjkj0br000d8gsz1xpr35nt","tag_id":"clfjkj0c900178gsz1nji1vot","_id":"clfjkj0cd001l8gszf24zeh20"},{"post_id":"clfjkj0br000d8gsz1xpr35nt","tag_id":"clfjkj0cb001c8gszd6kkg0fl","_id":"clfjkj0cd001o8gsz74qs1t83"},{"post_id":"clfjkj0bx000j8gsz0pzga2oz","tag_id":"clfjkj0cc001h8gsz65fw2211","_id":"clfjkj0ce001r8gszfnft85gh"},{"post_id":"clfjkj0bz000n8gszedk7g7pw","tag_id":"clfjkj0bn000c8gsz275r83ef","_id":"clfjkj0ce001u8gsz8a6c2p1d"},{"post_id":"clfjkj0bz000n8gszedk7g7pw","tag_id":"clfjkj0cd001n8gszccw22yyt","_id":"clfjkj0cf001w8gszbnzcd9di"},{"post_id":"clfjkj0c1000p8gsz0fb37e8i","tag_id":"clfjkj0bj00078gszanxf1k7l","_id":"clfjkj0cf001y8gsz0ur00alw"},{"post_id":"clfjkj0c1000p8gsz0fb37e8i","tag_id":"clfjkj0bn000c8gsz275r83ef","_id":"clfjkj0cf00208gszetexefm6"},{"post_id":"clfjkj0c1000p8gsz0fb37e8i","tag_id":"clfjkj0cd001n8gszccw22yyt","_id":"clfjkj0cg00228gszafp567k7"},{"post_id":"clfjkj0c4000x8gsz552begtg","tag_id":"clfjkj0cf001x8gsz6sfy2m0c","_id":"clfjkj0cg00268gsz246f32zp"},{"post_id":"clfjkj0c4000x8gsz552begtg","tag_id":"clfjkj0cf00218gsz1gzsfeig","_id":"clfjkj0ch00298gszd6xn6a5v"},{"post_id":"clfw9fner0000v8szcmvn4kgy","tag_id":"clfw9fnex0002v8sza87r5pj6","_id":"clfw9fnf30006v8sz5br61ubi"},{"post_id":"clfw9fner0000v8szcmvn4kgy","tag_id":"clfw9fnf20004v8szdvv9dewv","_id":"clfw9fnf40008v8szd6v7hi67"}],"Tag":[{"name":"DataWhale","_id":"clfjkj0bj00078gszanxf1k7l"},{"name":"Deep Learning","_id":"clfjkj0bn000c8gsz275r83ef"},{"name":"计算机网络","_id":"clfjkj0c2000s8gszee8q9zin"},{"name":"微机原理及接口技术","_id":"clfjkj0c600118gsz1a3val8h"},{"name":"自动化","_id":"clfjkj0c900178gsz1nji1vot"},{"name":"控制科学与工程","_id":"clfjkj0cb001c8gszd6kkg0fl"},{"name":"bash脚本","_id":"clfjkj0cc001h8gsz65fw2211"},{"name":"Pytorch","_id":"clfjkj0cd001n8gszccw22yyt"},{"name":"Linux","_id":"clfjkj0cf001x8gsz6sfy2m0c"},{"name":"Git Bush","_id":"clfjkj0cf00218gsz1gzsfeig"},{"name":"Python","_id":"clfw9fnex0002v8sza87r5pj6"},{"name":"Pandas","_id":"clfw9fnf20004v8szdvv9dewv"}]}}